# Annex 1: Artificial Neural Networks

In this annex part you will find all the intuition and theory of Artificial Neural Networks, which are at the heart of the Deep Q-Learning model we build in Part 2 - Minimizing the Costs. Here is the plan of attack to study Artificial Neural Networks:

\begin{enumerate}
    \item The Neuron
    \item The Activation Function
    \item How do Neural Networks work?
    \item How do Neural Networks learn?
    \item Forward-Propagation and Back-Propagation
    \item Gradient Descent
    \item Batch Gradient Descent and Stochastic Gradient Descent
\end{enumerate}

\subsection{The Neuron}

The neuron is the basic building block of Artificial Neural Networks. In the images below are actual real life neurons which have been smeared onto a glass, colored a little bit and observed through a microscope:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_1.png}
		\end{center}
\end{figure}

As we can see, they have the structure of a body with lots of different branches coming out of them. But the question is: How can we recreate that in a machine? Indeed, we really need to recreate it in a machine since the whole purpose of Deep Learning is to mimic how the human brain works, in the hope that by doing so we are going to create something amazing: a powerful infrastructure for machines to be able to learn.

\

Why do we hope for that? Because the human brain just happens to be one of the most powerful learning tools on the planet. So we just hope that if we recreate it then we will have something as awesome as that. So our challenge right now, that is our very first step to creating artificial neural networks, is to recreate a neuron.

\

So how do we do it? Well, first of all let's have a closer look at what a neuron actually is. The image below was first created by a Spanish neuroscientist and Chagga Ramon Yi Kajal in 1899:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.16]{ANN_2.png}
		\end{center}
\end{figure}

This neuroscientist dyed in neurons in actual brain tissue, and looked at them under a microscope. While he was looking at them he actually drew what he saw, which is exactly what we see on the above image. Today technology has advanced quite a lot allowing us to see neurons much closer in more detail so that we can actually draw what it looks like diagrammatically.

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.16]{ANN_3.png}
		\end{center}
\end{figure}

Above is a neuron. This neuron exchanges signals between its neighbour neurons. The dendrites are the receivers of the signal and the axon is the transmitter of the signal. Here is an image of how it all works conceptually:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.16]{ANN_4.png}
		\end{center}
\end{figure}

We can see that the dendrites of the neuron are connected to the axons of other neurons above it. Then the signal travels down its axon and passes on to the dendrites of the next neuron. That is how they are connected and how a neuron works. Thus now is the time to move from neuroscience to technology.

\

Here is how a neuron is represented inside an Artificial Neural Network:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.175]{ANN_5.png}
		\end{center}
\end{figure}

Just as a human neuron, it gets some input signals and it has an output signal. The blue arrow connecting the input signals to the neuron, and the neuron to the output signal, are like the synapses in the human neuron. But here in the machine neuron, what are exactly going to be these input and output signals? Well, the input signals are going to be the scaled independent variables composing the states of the environment, which remember in the case study are the temperature of the server, the number of users and the rate of data transmission, and the output signal will be the output values, which in the Deep Q-Learning model are always the Q-Values. Hence we get the general representation of a neuron for machines:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.15]{ANN_7.png}
		\end{center}
\end{figure}

And now to finish with the neuron, let's add the last missing elements in this representation, but also the most important ones: the weights. Each synapse (blue arrow) will be attributed a weight. The larger is the weight, the stronger the signal will be through the synapse. And what is fundamental to understand is that, these weights, will be what the machine will update and update over time to improve the predictions. Let's represent them on the previous graphic, to make sure we all visualize them well:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.15]{ANN_8.png}
		\end{center}
\end{figure}

\subsection{The Activation Function}

The Activation Function is the function $\phi$ operating inside the neuron, that will take as inputs the linear sum of the input values multiplied by their associated weights, and that will return the output value:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_9.png}
		\end{center}
\end{figure}

\

\

such that:

\

\begin{equation*}
    y = \phi\left( \sum_{i=1}^m w_i x_i \right)
\end{equation*}

\

Now what exactly is going to be the function $\phi$?

\

There can be many of them, but let's give the four most used ones, including of course the one we used in Part 2 - Minimizing Costs:

\

\begin{enumerate}
    \item The Threshold Activation Function
    \item The Sigmoid Activation Function
    \item The Rectifier Activation Function
    \item The Hyperbolic Tangent Activation Function
\end{enumerate}

\

Let's have a look at them one by one.

\newpage

\subsubsection{The Threshold Activation Function}

\

The Threshold Activation Function is simply defined by the following:

\

\begin{equation*}
    \phi(x) =
    \begin{cases}
        1 \textrm{ if } x \ge 0 \\
        0 \textrm{ if } x < 0
    \end{cases}
\end{equation*}

\

so that we get the following curve:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_10.png}
		\end{center}
\end{figure}

\

\

This means that the signal passing through the neuron will be discontinuous, and will only be activated if:

\

\begin{equation*}
    \sum_{i=1}^m w_i x_i \ge 0
\end{equation*}

\

Now let's have a look at the next activation function: the Sigmoid Activation function.

\

The Sigmoid Activation Function is the most effective and widely used one in Artificial Neural Networks, but mostly inside the last hidden layer (if we are dealing with a Deep Neural Network composed of several hidden layers) passing the signal towards the output layer.

\newpage

\subsubsection{The Sigmoid Activation Function}

\

The Sigmoid Activation Function is defined by the following:

\

\begin{equation*}
\phi(x) = \frac{1}{1+e^{-x}}
\end{equation*}

\

so that we get the following curve:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_11.png}
		\end{center}
\end{figure}

\

\

This means that the signal passing through the neuron will be continuous and will always be activated. And the higher is $\sum_{i=1}^m w_i x_i$, the stronger will be that signal.

\

Now let's have a look at another widely used activation function: the Rectifier Activation function.

\

You will find it in most of the \underline{DEEP} Neural Networks, but mostly inside the hidden layers, as opposed to the sigmoid function which is rather used for the output layer.

\newpage

\subsubsection{The Rectifier Activation Function}

\

The Threshold Activation Function is simply defined by the following:

\

\begin{equation*}
    \phi(x) = \max(x,0)
\end{equation*}

\

so that we get the following curve:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_12.png}
		\end{center}
\end{figure}

\

\

This means that the signal passing through the neuron will be continuous, and will only be activated if:

\

\begin{equation*}
    \sum_{i=1}^m w_i x_i \ge 0
\end{equation*}

\

And the higher is $\sum_{i=1}^m w_i x_i$ above 0, the stronger will be that signal.

\

Now let's have a look at the next activation function: the Hyperbolic Tangent Activation function.

\

The Hyperbolic Tangent activation function is less widely used, though it can sometimes be a more relevant choice in some Artificial Neural Networks, especially when the inputs are standardized.

\newpage

\subsubsection{The Hyperbolic Tangent Activation Function}

\

The Hyperbolic Tangent Activation Function is defined by the following:

\

\begin{equation*}
    \phi(x) = \frac{1-e^{-2x}}{1+e^{-2x}}
\end{equation*}

\

so that we get the following curve:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_13.png}
		\end{center}
\end{figure}

\

\

This means that the signal passing through the neuron will be continuous and will always be activated. The higher $\sum_{i=1}^m w_i x_i$ is above 0, the stronger will be that signal. The lower $\sum_{i=1}^m w_i x_i$ is below 0, the weaker will be that signal.

\newpage

So that raises the question: which activation function should we choose, or, more frequently asked, how do we know which one to choose?

\

Good news, the answer is simple, and let's give it inside a small blueprint.

\

That actually depends on what is returned as the dependent variable. If it is a binary outcome 0 or 1, then a good choice would be the threshold activation function. If what you want to be returned is the probability that the dependent variable is 1, then an excellent choice is the sigmoid activation function, since its sigmoid curve is a perfect fit to model probabilities.

\

Here is this small blueprint highlighted in this slide:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_14.png}
		\end{center}
\end{figure}

\

\

But then when should I use the other two activation functions, i.e. the Rectifier activation function and the Hyperbolic Tangent activation function?

\

Easy again, the Rectifier and the Hyperbolic Tangent activation functions should be used within the hidden layers of a Deep Neural Network (with more than one hidden layer), except the last hidden layer leading to the output layer for which it is recommended to use a Sigmoid Activation function.

\newpage

Let's recap this again inside the following slide:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_15.png}
		\end{center}
\end{figure}

\

\

And lastly, how to choose between the Rectifier activation function and the Hyperbolic Tangent activation function in the hidden layers? Still easy, you should consider using the Rectifier activation function when the inputs are normalized (scaled between 0 and 1), and the Hyperbolic Tangent activation function when the inputs are standardized (scaled between -1 and +1):

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_16.png}
		\end{center}
\end{figure}

\

\

Now let's move on to the next section to explain, how Neural Networks work.

\newpage

\subsection{How do Neural Networks work?}

To explain this, let's consider the problem of predicting Real Estate prices. We have some independent variables which we are going to use to predict the price of houses and apartments. For simplicity purpose, and to be able to represent everything in a graph, let's say that our independent variables (our predictors) are the following:

\begin{enumerate}
    \item Area (squared feet)
    \item Number of Bedrooms
    \item Distance to city (Miles)
    \item Age
\end{enumerate}

Then, our dependent variable is of course the apartment price to predict.

\

Each of the independent variables is attributed a weight, in such a way that the higher is the weight, the more effect the independent variable will have on the dependent variable, that is, the stronger predictor it will be of the dependent variable. Hence, as soon as new inputs enter the Neural Network, the signals are forward-propagated from each of the inputs, reaching the neurons of the hidden layer. Then inside each neuron of the hidden layer, the activation function is applied, so that the lower is the weight of the input, the more the activation function will block the signal coming from that input, and the higher is the weight of that input, the more the activation will let that signal go through. And finally, all the signals coming from the hidden neurons, more or less blocked by the activation functions, are forward propagated to the output layer, to return the final outcome, that is the price prediction.

\

Let's represent this in the following graphic:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_17.png}
		\end{center}
\end{figure}

\newpage

\subsection{How do Neural Networks learn?}

Simply put, Neural Networks learn by updating, over many iterations, the weights of all the inputs and hidden neurons (when having several hidden layers), towards always the same goal to reduce the loss error between the predictions and the real values.

\

Indeed, in order for Neural Networks to learn, we need the actual values, which are also called the targets. In our example above about Real Estate Pricing, the actual values are the real prices of the houses and apartments in sales. These real prices depend on the independent variables listed above (area, number of bedrooms, distance to city, and age), and the Neural Network will learn to make better predictions of these prices, by running the following process:

\

\begin{enumerate}
    \item The Neural Network first forward propagates the signals coming from the input independent variables $x_1$, $x_2$, $x_3$ and $x_4$.
    
    \
    
    \item Then it gets the predicted price $\hat{y}$ in the output layer.
    
    \
    
    \item Then it computes the loss error $C$ between the predicted price $\hat{y}$ (prediction) and the actual price $y$ (target):
    
    \
    
    \begin{equation*}
        C = \frac{1}{2} (\hat{y} - y)^2
    \end{equation*}
    
    \
    
    \item Then this loss error is back-propagated inside the Neural Network, from right to left in our representation.
    
    \
    
    \item Then on each of the neurons the Neural Network runs a technique called Gradient Descent (which we will discuss in the next section), to update the weights into the direction of loss reduction, that is, into new weights which reduce the loss error $C$.
    
    \
    
    \item Then this whole process is repeated many times, with each time new inputs and new targets, until we get the desired performance (early stopping) or the last iteration (number of iterations chosen in the implementation).
\end{enumerate}

\

Let's represent the two main phases, forward-propagation and back-propagation, of this whole process in the two following separate graphics (see next page):

\newpage

\subsection{Forward-Propagation and Back-Propagation}

\

\textbf{Phase 1: Forward-Propagation:}

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_18.png}
		\end{center}
\end{figure}

\textbf{Phase 2: Back-Propagation:}

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_19.png}
		\end{center}
\end{figure}

\newpage

\subsection{Gradient Descent}

\subsubsection{Gradient Descent Introduction}

When people talk about Machine Learning or Deep Learning they mostly talk about algorithms that are used. But the real questions are, why are those algorithms considered to be Machine Learning or Deep Learning algorithms and others are not? What is the underlining technique that connects them?

\

The answer to the first question is pretty intuitive: those algorithms are considered to learn their parameters by themselves. This property was not very common before and most algorithms were hand tuned by engineers to achieve a required specification/goal.

\

But then Gradient Descent came on stage and most algorithms that did not work before suddenly made sense and started to optimize themselves.

\

So is Gradient Descent magic? Well to someone it is, but for us it is a mathematical algorithm that is used to optimize a model that has its internal parameters (weights). Or, to be more technical, let’s see what Wikipedia says about it:

\begin{center}
    “Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function.”
\end{center}

That is a correct but mouthful definition, and for someone that is just starting, it is the scary one as well! Let’s break it down:

\

\textbf{Algorithm} - In simple words, it is a blueprint on how to solve a problem. Everyday example of an algorithm would be a cooking recipe.

\

\textbf{Iterative} - This means that it uses some kind of loop (for programmers, for-loops or while-loops) to perform steps. Each step uses previously calculated values as an input for the current step. Now, one question rises, “What is our initial value?”. We will answer on this a bit later through examples.

\

\textbf{Optimization} - It tries to find the best solutions according to some criteria leading to several alternative solutions, but only one is consider the best.

\

\textbf{First-order} - Gradient Descent is using first derivative of a criterion function (cost, loss) in order to find what is a better solution to the given problem.

\

Hence, when we put everything together in simple words, we get the following:

\

Gradient descent is a blueprint on how to find the best solution to a problem where more than one solution is acceptable. It uses a goal to determine how far we are from finding the best solution.

\

Until this point we have everything clarified except the cost function.

\

The cost is the indicator that we follow during the optimization process. Based on that indicator we can tell how far we are from the optimum of a function. One good example of the Cost is the Mean-Squared Error, which we have seen earlier in this book:

\begin{equation*}
    \textrm{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)
\end{equation*}

where:

\begin{equation*}
    \begin{cases}
        \textrm{$\hat{y}_i$ is the model prediction} \\
        \textrm{$y_i$ is the target (the actual value)} \\
        \textrm{$n$ is the number of samples in a dataset}
    \end{cases}
\end{equation*}

Every algorithm that uses Gradient Descent as an optimization technique has parameters (weights) which are changing during the optimization process. When we say that we are looking for the minimum of the loss function, we actually mean we are looking for the values of the weights for which the loss has the lowest possible value.

\

Accordingly, to answer our second question from the beginning, the technique that connects all the Machine Learning algorithms from Linear Regression to the most complicated neural networks is indeed, Gradient Descent.

\

\subsubsection{Gradient Descent Intuition}

\

As we have seen, Gradient Descent is an optimization technique that helps us find the minimum of a cost function. Now let’s visualize it in the most intuitive way, like the following ball in a bowl (with little math sprinkle on top of it):

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_20.png}
		\end{center}
\end{figure}

Imagine this is a cross section of a bowl, inside which we drop a small red ball and let it find its way down to the bottom of the bowl. After some time it will stop rolling since it has found the sweet spot for it, at the bottom of the bowl.

\

You can think about the Gradient Descent in the same way. It starts somewhere in the bowl (initial values of parameters) and tries to find the bottom of the bowl, or in other words, the minimum of a cost function.

\

Let’s go through the example that is shown on the image above. The initial values of the parameters have set our ball at the position shown. Based on that we get some predictions, which we compare to our target values. The difference between these two sets will be our loss for the current set of parameters.

\

Then we calculate the first derivative of the cost function, with respect to the parameters. This is where the name \textbf{Gradient} comes from. Here, this first derivative gives us the slope of the tangent to the curve where the ball is. If the slope is negative, like on the picture above, we take the next step to the right side. If the slope is positive we take the next step to the left side.

\

The name \textbf{Descent} thus comes from the fact that we always take the next step that points downhill, as represented in the following graphic:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.15]{ANN_21.png}
		\end{center}
\end{figure}

Now at this position our ball has a positive slope, so we have to take the next step to the left.

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.15]{ANN_22.png}
		\end{center}
\end{figure}

Eventually, by repeating the same steps, the ball will end up at the bottom of the bowl:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_23.png}
		\end{center}
\end{figure}

And that's it! This is how Gradient Descent operates in one dimension (one parameter). Now you might ask: "Great, but how does this scale?" We saw an example of one dimensional optimization, what about two or even 3 dimensions?

\

Excellent question. Gradient Descent guarantees that this approach scales on as many dimensions as needed, provided the cost function is convex. In fact, if the cost function is convex, Gradient Descent will find the absolute minimum of the cost function. Below is an example in 2 Dimensions:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.15]{ANN_24.png}
		\end{center}
\end{figure}

However if the cost function is not convex, it will only find a local minimum. Below is an example in 3 Dimensions:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.15]{ANN_25.png}
		\end{center}
\end{figure}

Now that we understand what Gradient Descent is all about, time to study the most advanced and most effective versions of it:

\begin{enumerate}
    \item Batch Gradient Descent
    \item Mini-Batch Gradient Descent
    \item Stochastic Gradient Descent
\end{enumerate}

\subsection{Batch Gradient Descent}

"Gradient Descent", "Batch Gradient Descent", "Mini Batch Gradient Descent", "Stochastic Gradient Descent".. There are so many terms and someone who is just starting may found him/her very confused.

\

The main difference across all of these versions of Gradient Descent is in the way we feed our data to a model, and how often we update our parameters (weights) to move our small red ball. Let's start by explaining Batch Gradient Descent.

\

Batch Gradient Descent is exactly what we did in Part 2 - Minimizing Costs, where remember we had a batch of inputs feeding the neural network, forward-propagating them to obtain in the end a batch of predictions, which themselves are compared to a batch of targets. The global loss error between the predictions and the targets of the two batches is then computed as the sum of the loss errors between each prediction and its associated target. That global loss is back-propagated into the neural network, where gradient descent or stochastic gradient descent is performed to update all the weights, according to how much they were responsible for that global loss error.

\

In the next page below is an example of Batch Gradient Descent. The problem to solve is about predicting the score (from 0 to 100 \%) students get at an exam, based on the time spent to study, and the time spent to sleep:

\newpage

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_26.png}
		\end{center}
\end{figure}

An important thing to note on this graphic above is that these are not multiple neural networks, but a single one represented by separate weight updates. And again, as we can notice in this example of Batch Gradient Descent, we feed all of our data to the model at once. This will produce collective updates of the weights and fast optimization of the network. However, there is the bad side of this as well. There is once again the possibility to get stuck at a local minimum, as we can see in the next graphic below:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_30.png}
		\end{center}
\end{figure}

\newpage

The reason why this happens was explained a bit earlier: it is because the cost function in the graphic above is not convex. And this type of optimization (simple Gradient Descent) requires the cost function to be convex. If that is not the case we can find ourselves stuck in a local minimum and never find the global minimum having the optimal parameters. On the other hand, below is an example of a convex cost function, the same one as we saw earlier:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_23.png}
		\end{center}
\end{figure}

In simple terms, a function is convex if it has only one global minimum. And the graph of a convex function has the bowl shape.

\

However, in most of the problems, including the business problems, the cost function will not be convex (as in this same graphic example in 3D below), thus not allowing simple Gradient Descent to perform well. This is where Stochastic Gradient Descent comes into play.

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.13]{ANN_25.png}
		\end{center}
\end{figure}

\subsection{Stochastic Gradient Descent}

Stochastic Gradient Descent (SGD) comes to save the day. It indeed provides better results overall, preventing the algorithm to get stuck in a local minimum. However, as its name suggests, it is stochastic, or in other words, random. Because of this property, no matter how many times you run the algorithm, the process will always be slightly different. And that, regardless of the initialization.

\

Stochastic Gradient Descent does not run on the whole dataset at once but instead input by input. Hence, the process goes like this:

\begin{enumerate}
    \item Input a single observation
    \item Get the single prediction
    \item Compute the loss error between the prediction and the target
    \item Back-Propagate the loss error into the neural network
    \item Update the weights with Gradient Descent
    \item Repeat 1. to 5. through the whole dataset
\end{enumerate}

Let's represent the three first iterations on the three first single inputs for this same example given earlier about predicting the scores at an exam:

\

\textbf{First Input Row of Observation:}

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_27.png}
		\end{center}
\end{figure}

\newpage

\textbf{Second Input Row of Observation:}

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_28.png}
		\end{center}
\end{figure}

\textbf{Third Input Row of Observation:}

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_29.png}
		\end{center}
\end{figure}

Each of the three graphics above is an example of one weights update ran by Stochastic Gradient Descent. As we can see, each time we only input a single row of observation from our dataset to the neural network, then we update the weights accordingly and proceed to the next input row of observation.

\

At first glance, Stochastic Gradient Descent seems slower, because we input each row separately. But in reality, it is much faster because of the fact that we don’t have to load the whole dataset in the memory, nor to wait for whole dataset to pass through the model updating the weights.

\

To finish this section, let's recap on the difference between Batch Gradient Descent and Stochastic Gradient Descent, with the following graphic:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{ANN_31.png}
		\end{center}
\end{figure}

\subsection{Mini-Batch Gradient Descent}

Mini-Batch Gradient Descent is using the best from both worlds to combine Batch Gradient Descent with Stochastic Gradient Descent. This is done by feeding the neural network with batches of data instead of feeding single input rows of observations one by one or the whole dataset at once.

\

This approach is faster than classical Stochastic Gradient Descent and prevents from getting stuck in the local minimum. This also helps when people don’t have enough computing resources to load the whole dataset in the memory, or enough processing power to get full benefit of Stochastic Gradient Descent.

\subsection{Optimizers}

The optimizer is exactly the tool that will update the weights of the Neural Network through Stochastic Gradient Descent. Up to this point we have only mentioned the Adam optimizer (see Part 2 - Minimizing Costs), which is the most common optimizer used for the Deep Learning and Deep Reinforcement Learning models. Nevertheless there are a lot more optimizers which have their own benefits and applications.

\

Let’s go through the most famous and widely used Gradient Descent optimizers.

\newpage

\subsubsection{Momentum Optimizer}

\

The classical Stochastic Gradient Descent has very big oscillations, which leaves room for improvement. The Momentum optimizer handles these big oscillations by adding fractions of the directions calculated in the previous step to the current step. This amplifies the speed of the current direction update. In the graphic just below we can see and compare the Classical SGD and the Momentum SGD in action:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.75]{ANN_32.png}
		\end{center}
\end{figure}

\

\

The benefits of the Momentum Optimizer are the following:

\

\begin{itemize}
    \item Faster Convergence
    \item Less Oscillations
\end{itemize}

\

But the Momentum Optimizer also has drawbacks, which are the following:

\

\begin{itemize}
    \item Tendency to overshoot the global minimum of the cost function because of the momentum.
    \item Less frequent in Deep Learning libraries, which thus requires the know-how of its implementation.
\end{itemize}

\newpage

\subsubsection{The Nesterov Accelerated Gradient Optimizer}

Yuri Nesterov solved the momentum the overshot minimum problem by reversing the calculation order in the update formula:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.6]{ANN_33.png}
		\end{center}
\end{figure}

\subsubsection{The AdaGrad (Adaptive Gradients) Optimizer}

The idea of adapting our updates according to the slope of the error function, coming from the Nesterov optimizer, is taken and applied in the AdaGrad Optimizer while optimizing the learning rate as well.

\

Hence, in the AdaGrad optimizer we have the same principle, not only applied on gradients but also on the learning rate.

\

Here are the benefits of this optimizer:

\begin{itemize}
    \item The AdaGrad optimizer allows to make big updates for infrequent parameters.
    \item And it allows to make small updates for frequent parameters.
\end{itemize}

And here are the drawbacks:

\begin{itemize}
    \item The Learning Rate is always decreasing, this could lead to very small, if any updates.
    \item Less frequent in Deep Learning libraries, which thus requires the know-how of its implementation.
\end{itemize}

\subsubsection{The AdaDelta Optimizer}

The AdaDelta Optimizer was invented to fix that decreasing learning rate issue of the AdaGrad Optimizer (first drawback). No need to get in further details, we just needed to introduce the AdaDelta optimizer and its particularity in order to understand the strength of the most widely used and most effective optimizer: the Adam Optimizer.

\subsubsection{The Adam (Adaptive Moment Estimation) Optimizer}

The Adam Optimizer is an improvement over the AdaDelta Optimizer. The idea behind it is to store in a memory the momentum changes, as we calculate the learning rate for each parameter separately.

\

Now remember the benefits of the Adam Optimizer, which are to be considered whenever building a Neural Network.

\begin{itemize}
    \item It is one of the most powerful optimizers.
    \item It is always pre-implemented in the Deep Learning libraries (Keras, TensorFlow, PyTorch). You will not miss it.
\end{itemize}

Of course, this is the one we use when building the Artificial Brain of our AI in Part 2 - Minimizing the Costs. Let's again provide the code that builds this artificial brain and notice, at the last line of code, the simplicity of selecting the Adam Optimizer:

\

\begin{lstlisting}
# Artificial Intelligence for Business - Case Study 2
# Building the Brain

# Importing the libraries
from keras.layers import Input, Dense
from keras.models import Model
from keras.optimizers import Adam

# BUILDING THE BRAIN

class Brain(object):
    
    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD
    
    def __init__(self, learning_rate = 0.001, number_actions = 5):
        self.learning_rate = learning_rate
        
        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE
        states = Input(shape = (3,))
        
        # BUILDING THE FULLY CONNECTED HIDDEN LAYERS
        x = Dense(units = 64, activation = 'sigmoid')(states)
        y = Dense(units = 32, activation = 'sigmoid')(x)
        
        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER
        q_values = Dense(units = number_actions, activation = 'softmax')(y)
        
        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT
        self.model = Model(inputs = states, outputs = q_values)
        
        # COMPILING THE MODEL WITH AN MSE LOSS FUNCTION AND THE ADAM OPTIMIZER
        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))
\end{lstlisting}

\newpage

# Annex 2: Three Extra AI Models

As a Bonus, in this section we provide three extra AI models, closer to the State of the Art. However, these AI models are not necessarily adapted to solve business problems, but more to solve specific tasks like playing games or training a virtual robot to walk. We are going to study three powerful models, including two in the Deep Reinforcement Learning branch of AI, and one in the Policy Gradient branch:

\begin{enumerate}
    \item Deep Convolutional Q-Learning (Deep RL)
    \item A3C (Deep RL)
    \item Augmented Random Search (Policy Gradient)
\end{enumerate}

\subsection{Deep Convolutional Q-Learning}

In the previous section, our inputs were vectors encoded values defining the states of the environment. But since an encoded vector doesn't preserve the spatial structure of an image, this is not the best form to describe a state. The spatial structure is indeed important because it gives us more information to predict the next state, and predicting the next state is of course essential for our AI to know what is the right next move. Therefore we need to preserve the spatial structure and to do that, our inputs must be 3D images (2D for the array of pixels plus one additional dimension for the colors). In that case, the inputs are simply the images of the screen itself, exactly like what a human sees when playing the game. Following this analogy, the AI acts like a human: it observes the input images of the screen when playing the game, the input images go into a convolutional neural network (the brain for a human) which will detect the state in each image. However, this convolutional neural network doesn't contain pooling layers, because they would loose the location of the objects inside the image, and of course the AI need to keep track of the objects. Therefore we only keep the convolutional layers, and then by flattening them into a 1-dimensional vector, we get the input of our previous Deep Q-Learning network. Then the same process is being ran.

\

Therefore in summary, Deep Convolutional Q-Learning is the same as Deep Q-Learning, with the only difference that the inputs are now images, and a Convolutional Neural Network is added at the beginning of the fully-connected Deep Q-Learning network to detect the states (or simply the objects) of the images.

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.3]{DCQL}
			\caption{Deep Convolutional Q-Learning}
		\end{center}
\end{figure}

\newpage

\subsection{Asynchronous Actor-Critic Agents (A3C)}

\subsubsection{A3C Intuition}

So far, the action played at each time has been the output of one neural network, as if only one agent was deciding the strategy to play the game. This will no longer be the case with A3C. This time, we are going to have several agents, each one interacting with its own copy of the environment. Let's say there are $n$ agents $A_1$, $A_2$,..., $A_n$.

\

Each agent is sharing two networks: the actor and the critic. The critic evaluates the present states, while the actor evaluates the possible values in the present state. The actor is used to make decisions. At each epoch time of training for on agent, it takes the last version of the shared networks and uses the actor during n steps in order to make a decision. Over the n steps, it collects all the observed new states, the values of these new states, the rewards, etc... After the n steps, the agent uses the collected observations in order to update the shared models. The times of epoch, and therefore the times of updates of the shared network by the agent are not synchronous, hence the name.

\

That way, if an unlucky agent starts to be stuck into a suboptimal but attractive policy, it will reach out that state -- because other agents also updated the shared policy before the agent got stuck -- and will continue effective exploration.

\

In order to explain the update rules of the actor and the critic, let us see the networks as functions that depend on vectors of parameters $\theta$ (for the actor) and $\theta_v$ (for the critic).

\subsubsection{The whole A3C Process}

The official A3C algorithm is the one of the Google DeepMind paper, "Asynchronous Methods for Deep Reinforcement Learning" (https://arxiv.org/pdf/1602.01783.pdf). In this paper you will find it in the following S3 algorithm:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.4]{S3}
			\caption{A3C algorithm (https://arxiv.org/pdf/1602.01783.pdf)}
		\end{center}
\end{figure}

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.4]{A3C}
			\caption{A3C}
		\end{center}
\end{figure}

\

In this figure above we can clearly see the three As of the A3C:

\begin{itemize}

\item[$\bullet$] \textbf{Asynchronous}: There are several agents, each one having their own copy of the environment, and all asynchronised (playing the game at different times).

\item[$\bullet$] \textbf{Advantage}: The advantage is the difference between the prediction of the actor, $Q(s,a)$, and the prediction of the critic, $V(s)$:

\begin{equation*}
A = Q(s,a) - V(s)
\end{equation*}

\item[$\bullet$] \textbf{Actor-Critic}: Of course we can see the actor and the critic, that therefore generate two different losses: the policy loss and the value loss. The policy loss is the loss related to the predictions of the actor. The value loss is the loss related to the predictions of the critic. Over many epochs of the training, these two losses will be back-propagated into the Neural Network, then reduced with an optimizer through stochastic gradient descent.

\end{itemize}

\newpage

\subsection{Augmented Random Search}

\subsubsection{Problem to solve}

We want to build and train an AI that walks or runs across a field. The field is a flat ground that looks like this:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.15]{Half_Cheetah.png}
		\end{center}
\end{figure}

On this same field you can see a Half-Cheetah. This will be one of the agents we will train to walk on this field. Both the field and the agent form what we call an environment, which belongs to PyBullet, the official Python Interface for the Bullet Physics SDK specialized for Robotics Simulation and Reinforcement Learning, built and developed by Erwin Coumans. For more info click \href{https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit}{here}. You can also check the GitHub page on this \href{https://github.com/bulletphysics/bullet3}{link}.

\subsubsection{AI Solution}

The solution to our problem is a very recent AI model called \textbf{ARS}, or \textbf{Augmented Random Search}. The related research paper was released by Horia Mania, Aurelia Guy and Benjamin Recht on March 20, 2018. You can find the full research paper \href{https://arxiv.org/pdf/1803.07055.pdf}{here}.

\

ARS is based on a specific branch of Artificial Intelligence called Evolution Strategies. The difference is that ES uses parallelized deep neural networks of several layers, while ARS uses a simple linear policy, which is a Perceptron (a shallow neural network of one layer composed of several neurones). ARS is also slightly similar to \textbf{PPO} - \textbf{Proximal Policy Optimization}, in the sense that ARS aims to optimize a policy (a function of the states returning the actions to play) that performs the best actions allowing the AI to walk. However the technique is different. If you are curious about \textbf{PPO}, you can check out the research paper \href{https://arxiv.org/pdf/1707.06347.pdf}{here}.

\

Now let's deep dive into the ARS.

\

The whole picture is pretty simple. We have a policy, that takes as inputs the states of the environment, and returns as outputs the actions to play in order to walk and run across a field. Now before we start explaining the algorithm, let's describe in more details the inputs, the outputs and the policy.

\

\

\textbf{The inputs.}

\

The input is a vector encoding the states of the environment. What does it mean? First let's explain what exactly is a state of the environment. A state is the exact situation happening at a specific time $t$, e.g:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.5]{Cheetah_in_the_air.png}
		\end{center}
\end{figure}

We can see the cheetah in the air, back legs up, front legs bent, about to land on the ground. All this is encoded into a vector. How? By simply gathering enough values that can describe what is happening here. So the encoded vector will contain the coordinates of the angular points of the cheetah, as well as the angles of rotation around the rotors, and more values like the velocity. Therefore at each time $t$, a vector of same format is encoding what is happening exactly in the environment. This encoded vector is what we call the input state of the environment, and will be the input of our policy that we will try to optimize.

\

\textbf{The outputs.}

\

The output, returned by our policy, is a group of actions played by the agent. More precisely, these actions are the different muscles impulsions of the agent. For example, one of the actions will be the intensity of the muscle pushing the back leg at the level of the foot. What is important to understand here is more the fact that the policy is returning a \underline{group of actions} as opposed to a single action. Indeed a common practice in Reinforcement Learning is to return one discreet action at each time $t$. Here, not only we return a group of actions, but each of these actions is continuous. Indeed, in order for an agent to walk on a field, it has to move all the parts of its body at each time $t$, as opposed to only one leg for example. And the actions are continuous because the impulsions of the muscles are measured by continuous metrics. Hence, the output is also a vector of several continuous values, just like the input state.

\textbf{The policy.}

\

Between the inputs and the outputs we have a policy, which is nothing else than a function, taking as inputs the input states, and returning as outputs the actions to play, i.e. the muscle impulsions. This policy will be linear, since indeed it will be a perceptron, which is a simple neural network of one layer and several neurones:

\

\

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=1]{Perceptron.png}
		\end{center}
\end{figure}

\

\

The Hidden layer in the middle contains the different neurones of the perceptron. To each couple of (input value, output value) is attributed a weight. Therefore in total we have number\_of\_inputs $\times$ number\_of\_outputs weights. All these weights are gathered in a matrix, which is nothing else than the matrix of our linear policy. In this matrix, the rows correspond to the output values (the actions) and the columns correspond to the input values (of the states). Therefore this matrix of weights, called $\Theta$, is composed of $n = \textrm{number\_of\_outputs}$ rows and $m = \textrm{number\_of\_inputs columns}$:

\

\begin{equation*}
\boldsymbol{\Theta}
=
\begin{pmatrix}
(\textrm{input 1, output 1}) & (\textrm{input 2, output 1}) & \cdots & (\textrm{input m, output 1}) \\
(\textrm{input 1, output 2}) & (\textrm{input 2, output 2}) & \cdots & (\textrm{input m, output 2}) \\
\vdots & \vdots & \ddots & \vdots \\
(\textrm{input 1, output n}) & (\textrm{input 2, output n}) & \cdots & (\textrm{input m, output n})
\end{pmatrix}
=
\begin{pmatrix}
\theta_{1,1} & \theta_{2,1} & \cdots & \theta_{m,1} \\
\theta_{1,2} & \theta_{2,2} & \cdots & \theta_{m,2} \\
\vdots & \vdots & \ddots & \vdots \\
\theta_{1,n} & \theta_{2,n} & \cdots & \theta_{m,n}
\end{pmatrix}
\end{equation*}

\newpage

\textbf{The ARS algorithm.}

\

\textbf{Initialization.}

\

At the very beginning, all the weights $\theta_{i,j}$ of our linear policy are initialized to zero:

\begin{equation*}
\forall i,j \in \{1,n\}\times\{1,m\}, \theta_{i,j} = 0
\end{equation*}

\

\textbf{Applying perturbations to the weights.}

\

Then, we are going to apply some very little perturbations to each of these weights, by adding some very small values $\delta_{i,j}$ to each of the $\theta_{i,j}$ in our matrix of weights:

\begin{equation*}
\begin{pmatrix}
\theta_{1,1} & \theta_{2,1} & \cdots & \theta_{m,1} \\
\theta_{1,2} & \theta_{2,2} & \cdots & \theta_{m,2} \\
\vdots & \vdots & \ddots & \vdots \\
\theta_{1,n} & \theta_{2,n} & \cdots & \theta_{m,n}
\end{pmatrix}
\longrightarrow
\begin{pmatrix}
\theta_{1,1} + \delta_{1,1} & \theta_{2,1} + \delta_{2,1} & \cdots & \theta_{m,1} + \delta_{m,1} \\
\theta_{1,2} + \delta_{1,2} & \theta_{2,2} + \delta_{2,2} & \cdots & \theta_{m,2} + \delta_{m,1} \\
\vdots & \vdots & \ddots & \vdots \\
\theta_{1,n} + \delta_{1,n} & \theta_{2,n} + \delta_{2,n} & \cdots & \theta_{m,n} + \delta_{m,n}
\end{pmatrix}
\end{equation*}

\

We will call this: "applying some perturbations in one \underline{positive direction}" $+\Delta_k$, where $\Delta_k$ is the following matrix of perturbations:

\begin{equation*}
\Delta_k
=
\begin{pmatrix}
\delta_{1,1} & \delta_{2,1} & \cdots & \delta_{m,1} \\
\delta_{1,2} & \delta_{2,2} & \cdots & \delta_{m,2} \\
\vdots & \vdots & \ddots & \vdots \\
\delta_{1,n} & \delta_{2,n} & \cdots & \delta_{m,n}
\end{pmatrix}
\end{equation*}

\

"Positive" comes from the fact that we are \underline{adding} the small values $\delta_{i,j}$ to our weights $\theta_{i,j}$. These little perturbations $\delta_{i,j}$ are sampled from a Gaussian distribution $\mathcal{N}(0,\sigma)$ (the standard deviation $\sigma$ is what we call "noise" in the ARS model.

\

And each time we do this, we are also going to apply the exact same perturbations $\delta_{i,j}$ to our weights $\theta_{i,j}$, but in the opposite direction $-\Delta_k$, by simply this time subtracting the exact same $\delta_{i,j}$:

\begin{equation*}
\begin{pmatrix}
\theta_{1,1} & \theta_{2,1} & \cdots & \theta_{m,1} \\
\theta_{1,2} & \theta_{2,2} & \cdots & \theta_{m,2} \\
\vdots & \vdots & \ddots & \vdots \\
\theta_{1,n} & \theta_{2,n} & \cdots & \theta_{m,n}
\end{pmatrix}
\longrightarrow
\begin{pmatrix}
\theta_{1,1} - \delta_{1,1} & \theta_{2,1} - \delta_{2,1} & \cdots & \theta_{m,1} - \delta_{m,1} \\
\theta_{1,2} - \delta_{1,2} & \theta_{2,2} - \delta_{2,2} & \cdots & \theta_{m,2} - \delta_{m,1} \\
\vdots & \vdots & \ddots & \vdots \\
\theta_{1,n} - \delta_{1,n} & \theta_{2,n} - \delta_{2,n} & \cdots & \theta_{m,n} - \delta_{m,n}
\end{pmatrix}
\end{equation*}

\

We will call this: "applying some perturbations in the \underline{negative direction}" $-\Delta_k$.

\

Therefore in conclusion, we sample one specific matrix of perturbations $\Delta_k$ with some values $\delta_{i,j}$ close to zero and we update the weights of our matrix $\Theta$ in the positive direction $+ \Delta_k$ and the negative direction $- \Delta_k$:

\begin{align*}
\textbf{Positive Direction: } & \Theta \rightarrow \Theta + \Delta_k \\
\textbf{Negative Direction: } & \Theta \rightarrow \Theta - \Delta_k
\end{align*}

\

And in fact, over each full episode, we are going to apply these positive and negative perturbations for many different directions $\Delta_1$, $\Delta_2$, $\Delta_3$, etc. We will do this for 16 different directions:

\begin{align*}
\textbf{Positive Directions: } & \Theta \rightarrow \Theta + \Delta_1, \ \Theta \rightarrow \Theta + \Delta_2, \ ... \ , \ \Theta \rightarrow \Theta + \Delta_{16} \\
\textbf{Negative Directions: } & \Theta \rightarrow \Theta - \Delta_1, \ \Theta \rightarrow \Theta - \Delta_2, \ ... \ , \ \Theta \rightarrow \Theta - \Delta_{16}
\end{align*}

\

Now it is time to ask: why are we doing this?

\

The reason is actually simple and intuitive to understand. We want to update the weights in these different directions to find the ones that will increase the most the total reward over the episodes. We want to figure out which updates of the weights will lead to the highest rewards. Indeed, increasing the total reward accumulated over the episode is our ultimate goal, since the higher is the reward, the better the agent will have the ability to walk.

\

Now another question, less obvious: Why, for each direction, do we want to take the positive and the negative one?

\

That is because, once we figure out the directions that increase the most the rewards (by simply getting the accumulated reward over the full episode for each direction and then sorting them by the highest obtained), we will do one step of gradient descent to update the weights in these best directions. However we don't have any reward function of the weights, so we couldn't apply gradient descent directly. Indeed, in order to apply gradient descent we would need to have a reward function of the weights, $r(\Theta)$, differentiate it with respect to the weights:

\begin{equation*}
\frac{\partial r(\Theta)}{\partial \Theta}
\end{equation*}

and then do this one step of gradient descent to update the weights:

\begin{equation*}
\Theta(\textrm{new}) := \Theta(\textrm{old}) + \frac{\partial r(\Theta)}{\partial \Theta} d \Theta
\end{equation*}

\

But we cannot do that because we don't have an explicit expression of the reward with respect to the weights. So instead of computing directly this gradient, we will approximate it. And that's where the combo of positive \& negative directions comes into play, with the method of finite differences.

\

\textbf{Approximated Gradient Descent with the Method of Finite Differences.}

\

So now we understand that we have to do one step of gradient descent to update the weights in the directions that increase the most the reward, and that to do this one step we have no choice but to approximate the gradient of the rewards with respect to the weights. More specifically, we have to approximate:

\begin{equation*}
\frac{\partial r(\Theta)}{\partial \Theta} d \Theta
\end{equation*}

\

Well with what we have done before applying the perturbations in the positive and negative directions, we will be able to approximate this easily. Since the value of each perturbation $\delta$ is a very small number close to zero, then the difference between the reward $r_{+}$ we get when applying the perturbation in the positive direction ($\Theta \rightarrow \Theta + \Delta$) and the reward $r_{-}$ we get when applying the perturbation in the negative (or opposite) direction ($\Theta \rightarrow \Theta - \Delta$) is approximately equal to that gradient:

\begin{equation*}
r_{+} - r_{-} \approx \frac{\partial r(\Theta)}{\partial \Theta}
\end{equation*}

so that we get the following approximation:

\begin{equation*}
(r_{+} - r_{-}) \Delta \approx \frac{\partial r(\Theta)}{\partial \Theta} d \Theta
\end{equation*}

\

This approximation is the result of the method of finite differences and allows us to do this one step of approximated gradient descent.

\

Then we choose a number of best directions we want to keep as the ones leading to the highest rewards and we do this one step of approximated gradient descent on all these best directions. How do we know the top directions that increase the most the rewards? Well let's say we want to keep 16 best directions, we simply apply the positive and negative perturbations for each of all our directions over one full episode, we store the couple of rewards $(r_{+}, r_{-})$ we get for each of these directions, and eventually we keep the 16 highest maximums of $r_{+}$ and $r_{-}$. These 16 highest rewards correspond to our 16 best directions.

\

Then eventually we average our approximated gradients over those 16 best directions to update the whole matrix of weights $\Theta$:

\begin{equation}
\Theta(\textrm{new}) = \Theta(\textrm{old}) + \frac{1}{16}\sum_{k=1}^{16} [r_{+}(\textrm{$k^{th}$ best direction}) - r_{-}(\textrm{$k^{th}$ best direction})] \Delta_{\textrm{$k^{th}$ best direction}}
\end{equation}

\

Right after this update, the step of gradient descent is applied to the whole matrix of weights $\Theta$, so that the weights of our policy are updated into the top directions that increase the most the accumulated reward.

\

\textbf{Training loop.}

\

Eventually, we repeat this whole process (apart from the initialization of the weights to zero) for a certain number of steps (e.g. 1000 steps).

\

We can improve the performance of the ARS with the two following action items:

\

\begin{enumerate}
    \item Normalizing the states
    \
    \item Scaling by the standard deviation of the reward
    \
    \item Tuning the learning rate
\end{enumerate}

\

Let's have a look at each of these improvement solutions in the next page.

\newpage

\textbf{Normalizing the states.}

\

In the \href{https://arxiv.org/pdf/1803.07055.pdf}{research paper}, we have the options between \textbf{V1} and \textbf{V2} (see page 6). \textbf{V1} is the algorithm above without normalizing the input states, and \textbf{V2} is the ARS with normalized input states.

\

Normalizing the states clearly improves the performance.

\

\textbf{Scaling by the standard deviation of the reward.}

\

We can scale by dividing the previous sum in equation (1) by the standard deviation $\sigma_r$ of the reward, so that we get:

\begin{equation}
\Theta(\textrm{new}) = \Theta(\textrm{old}) + \frac{1}{16 \sigma_r} \sum_{k=1}^{16} [r_{+}(\textrm{$k^{th}$ best direction}) - r_{-}(\textrm{$k^{th}$ best direction})] \Delta_{\textrm{$k^{th}$ best direction}}
\end{equation}

\

\textbf{Tuning the learning rate.}

\

For tuning purposes we can add a learning rate factor in equation (2) (denoted by $\alpha$ in the paper):

\begin{equation*}
\Theta(\textrm{new}) = \Theta(\textrm{old}) + \frac{\alpha}{16 \sigma_r} \sum_{k=1}^{16} [r_{+}(\textrm{$k^{th}$ best direction}) - r_{-}(\textrm{$k^{th}$ best direction})] \Delta_{\textrm{$k^{th}$ best direction}}
\end{equation*}

\newpage

# Annex 3: Questions and Answers

\subsection{Q\&As on Part 1 - Optimizing Processes}

\textbf{What are the Plan and the Policy?}

Simply put, the plan is the process of creating the environment of the input states, and the policy is the function that takes the input states defined by the plan as inputs and returns the actions to play as outputs. So in the case study, the whole process we do of defining the warehouse environment is our plan and the policy is our AI. Take a look at the following links as they might help give you some additional context:
\begin{enumerate}
    \item \href{http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter\%209.pdf}{Link 1}
    \item \href{https://www.quora.com/In-artificial-intelligence-which-is-better-policies-or-plans-and-why}{Link 2}
\end{enumerate}

\

\textbf{Who determines the discount factor in the Bellman Equation? and How?}

It is determined by the AI developer through experimenting. You try first with 1 (no discount), then you decrease a bit and observe if you get better results. And by repeating this you find an optimal value.

\

More questions and their answer will be added here, as soon as relevant questions are asked within the course.

\newpage

\subsection{Q\&As on Part 2 - Minimizing Costs}

\textbf{In Deep Reinforcement Learning, when to use Argmax vs Softmax?}

You would use Argmax for not too complex problems (like business problems), and Softmax for complex problems like playing games or making a robot to walk. Indeed, for the complex problems you need to do some exploration vs exploitation, and that's exactly what Softmax allows you to do. However, Business Problems are not too complex so you don't need to do much exploration, and therefore an Argmax method is way sufficient.

\

\textbf{Is there any specific reason for choosing two layers with 64 and 32 neurons for the brain's architecture? Should we then pay attention to overfitting?}

What we should do is start with some classical architectures that we find on papers (ImageNet, ResNet, Inception, MobileNets, etc.). Then we try, we see if we get good results, and if that's the case we can stop there. For our DNN, we simply took a classic architecture, with 2 fully connected layers of 64 and 32 neurons, which turned out to work very well for our case study. Then we do prevent overfitting in the course by applying two different techniques. These allow us to improve the model, and thus, improve the score. These two techniques are:
\begin{enumerate}
    \item Early Stopping
    \item Dropout
\end{enumerate}

\

More questions and their answer will be added here, as soon as relevant questions are asked within the course.

\newpage

\subsection{Q\&As on Part 3 - Maximizing Revenues}

\textbf{Could you please explain in greater details what a distribution is? What are on the x-axis and y-axis?}

Let us assume that we have an experiment to select the strategy to be deployed on the customer 100 times (that is with 100 separate customers one after the other), and we compute the frequency of strategy selection. Then we repeat it again for another 100 times. And again for another 100 times. Hence we obtain many frequencies.  If we repeat such frequency computations many times, for example 500 times, we can plot a histogram of these frequencies. By the Central Limit Theorem it will be bell-shaped, and its mean will be the mean of all frequencies we obtained over the experiment. Therefore in conclusion, on the x-axis we have the different possible values of these frequencies, and on the y-axis we have the number of times we obtained each frequency over the experiment.

\

\textbf{In the first Intuition Lecture, could you please explain why D5 (in orange) is the best distribution? Why is it not D3 (in pink)?}

In this situation, 0 is loss, and 1 is gain, or win. The D5 is the best because it is skewed so we will have average outcomes close to 1, meaning that there we have more wins, or gains. And actually all casino machines nowadays are carefully programmed to have distribution like D1 or D3. But it is a good concrete example.

\

\textbf{On the graphic below, why is the yellow mark the best choice, and not the green mark?}

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{Beta_Distribution_Slide.png}
		\end{center}
\end{figure}

The yellow mark is the best choice because it is the furthest from the origin on the x-axis, which therefore means that it has the highest estimated return.

\newpage

\textbf{I don't understand how and why Thompson Sampling can accept delayed feedback. Please explain.}

When doing Thompson Sampling, we can still perform updates in our algorithm (like making new guesses for the distributions with existing data, sampling from the guessed distribution, etc) while we are waiting for the results of an experiment in the real world. This would not hinder our algorithm from working. This is why it can accept delayed feedback.

\

\textbf{What are further examples of Thompson Sampling applications?}

The most classic example is Conversion Rate Optimization. You have several ads od a same product and you want to know which one has the highest CTR. So you are going to do the same thing as we did with the strategies, except that this time the arms will be the ads. \\
Another potential application of Multi-armed bandits (MAB) can be the online testing of algorithms. \\
For example, let's suppose you are running an e-commerce website and you have at your disposal several Machine Learning algorithms to provide recommendations to users (of whatever the website is selling), but you don't know which algorithm leads to the best recommendations. \\
You could consider your problem as a MAB problem and define each Machine Learning algorithm as an "arm": at each round when one user requests a recommendation, one arm (i.e. one of the algorithms) will be selected to make the recommendations, and you will receive a reward. In this case, you could define your reward in various ways, a simple example is "1" if the user clicks/buys an item and "0" otherwise.
Eventually your bandit algorithm will converge and end up always choosing the algorithm which is the most efficient at providing recommendations. This is a good way to find the most suitable model in an online problem. \\
Another example coming to my mind is finding the best clinical treatment for patients: each possible treatment could be considered as an "arm", and a simple way to define the reward would be a number between 0 (the treatment has no effect at all) and 1 (the patient is cured perfectly). \\
In this case, the goal is to find as quickly as possible the best treatment while minimizing the cumulative regret (which is equivalent to say you want to avoid as much as possible selecting "bad" or even sub-optimal treatments during the process).

\

\textbf{Where can I find some great resource on the Beta distribution?}

The best is the following:
\href{https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution}{Beta Distribution}

\

\textbf{I am curious as to how one would apply Thompson Sampling proactively when running this theoretical strategy campaign. Would you iterate the program over each round?}

First, a data engineer makes a pipeline for reading data from the website and reacting to it in real time. Then a web visit triggers a response to recompute the parameters and to choose a strategy for the next time.

\

More questions and their answer will be added here, as soon as relevant questions are asked within the course.
