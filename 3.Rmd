# Parte 3 - Maximización de Beneficios Revenues

Congratulations for smashing the first and second case studies! Now let’s move on to a very different kind of Artificial Intelligence, that has tremendous efficiency for businesses, and that you must absolutely know about.

## Caso Práctico: Maximizing Revenue of an Online Retail Business

### Problem to solve

Imagine an Online Retail Business that has million of customers. These customers are only people buying some products on the website from time to time, getting them delivered at home. The business is doing good, but the board of executives has decided to take some action plan to maximize revenue even more. This plan consists of offering to the customers the option to subscribe to a premium plan, which will give them some benefits like reduced prices, special deals, etc. This premium plan is offered at a yearly price of \$ 100, and the goal of this online retail business is of course to get the maximum customers to subscribe to this premium plan. Let's do some quick maths to give us some motivation for building an AI to maximize the revenues of this business. Let's say that this online retail business has 100 million customers. Now consider two conversion strategies trying to convert the customers to the premium plan: a bad one, with a conversion rate of 1\%, and a good one, with a conversion rate of 11\%. If the business deploys the bad strategy, it will make in one year a total extra revenue coming from the premium plan subscription of: $100,000,000 \times 0.01 \times 100 = \$ 100,000,000$. On the other hand, if the business deploys the good strategy, it will make in one year a total extra revenue coming from the premium plan subscription of $100,000,000 \times 0.11 \times 100 = \$ 1,100,000,000$. Thus, by figuring out the good strategy to deploy, the business maximized its extra revenues by making 1 Billion extra dollars.

\

In this previous Utopian example, we only had two strategies, and besides we knew their conversion rates. However in our Caso Práctico we will be facing 9 different strategies, and our AI will have no idea of which is the best one, and absolutely no prior information on any of their conversion rates. However we will make the assumption that each of these 9 strategies does have a fixed conversion rate. These strategies were carefully and smartly elaborated by the marketing team, and each of them has the same goal: convert the maximum clients to the premium plan. However, these 9 strategies are all different. They have different forms, different packages, different ads, and different special deals to convince and persuade the clients to subscribe to the premium plan. Of course, the marketing team has no idea of which of these 9 strategies is the best one. But they want to figure it out as soon as possible, and by saving the maximum costs, which one has the highest conversion rate, because they know how finding and deploying that best strategy can significantly maximize the revenues. Also, the marketing experts choose not to send an email to their 100 million customers, because that would be costly and they would risk spamming too many customers. Instead they will subtly look for that best strategy through online learning. What is online learning? It will consist of deploying a strategy each time a customer browses the online retail business website to hang out inside or buy some products. Then as the customer navigates the website, he or she will suddenly get a pop-up ad, suggesting him or her to subscribe to the premium plan. And for each customer browsing the website, only one of the 9 strategies will be deployed. Then the user will choose, or not, to take action and subscribe to the premium plan. If the customer subscribes, it is a success, otherwise, it is a failure. The more customers we do this with, the more feed-backs we get, and the better we could get an idea of what is the best strategy. But of course, we will not figure that out manually, visually, or with some simple maths. Instead we want to implement the smartest algorithm that will figure out what is the best strategy in the least amount of time. And that's for the same two reasons: first because deploying each strategy has a cost (e.g. coming from the pop-up ad), and second because the company wants to annoy the least customers with their ad.

\newpage

Let's sum up the differences in features of these 9 strategies simply this way:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.165]{Strategies_Slide.png}
		\end{center}
\end{figure}

\textbf{Simulation.}

\

In order to simulate this Caso Práctico, we will assume these strategies have the following conversion rates:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.165]{Simulation_Slide.png}
		\end{center}
\end{figure}

However, please make sure to understand that in a real life situation we would have no idea of what would be these conversion rates. We only know them here for simulation purposes, just so that we can check in the end that our AI manages to figure out the best strategy, which according to the table above, is strategy number 7 (highest conversion rate).

\subsubsection{Environment to define}

Online Learning is a special branch of Artificial Intelligence, where there is not much need of defining the states and actions. Here, a state would simply be a specific customer onto whom we deploy a strategy, and the action would simply be the strategy selected. But you will see further in the AI algorithm that we don't have the states as inputs and the actions as outputs like in our two previous case studies, because this time we are not doing Q-Learning or Deep Q-Learning. Here we are doing online learning. However we do have to define the rewards, since again we will have to make a rewards matrix, where each row corresponds to a user being deployed a strategy, and each column corresponds to one of the 9 strategies. Therefore, since we will actually run this online learning experiment on 10,000 customers, this rewards matrix will have 10,000 rows and 9 columns. Then, each cell will get either a 0 if the customer doesn't subscribe to the premium plan after being approached by the selected strategy, and a 1 if the customer does subscribe after being approached by the selected strategy. And the values in the cell are exactly, the rewards.

\

Now a very important thing to understand is that the rewards matrix is only here for the simulation, and in real life we would have no such thing as a rewards matrix. We will just simulate 10,000 customers successively being approached by one of the 9 strategies, and thanks to the rewards matrix we will simulate the decision of the customer to subscribe yes or no to the premium plan. If the cell corresponding to a specific customer and a specific selected strategy has a 1, that will simulate a conversion by the customer to the premium plan, and if the cell has a 0, that will simulate a rejection. Here is below as an example the first rows of a simulated rewards matrix:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.49]{Rewards_Matrix.png}
		\end{center}
\end{figure}

According to this simulation, all given in the above rewards matrix:

\begin{enumerate}
    \item The first customer (row of index 0) would not subscribe to the premium plan after being approached by any strategy.
    \item The second customer (row of index 1) would subscribe to the premium plan after being approached by strategy 5 or strategy 7 only.
    \item The third customer (row of index 2) would not subscribe to the premium plan after being approached by any strategy.
\end{enumerate}

Thompson Sampling will collect the feed-backs of whether or not each of these customers subscribe to the premium plan one after the other, and thanks to its powerful algorithm, will quickly figure out the strategy with the highest conversion rate, that is the best one to be deployed on the millions of customers, thus maximizing the company's income from this new revenue stream.

\subsection{AI Solution}

The AI solution that will figure out the best strategy is called "Thompson Sampling". It is by far the best model for that kind of problems in this Online Learning branch of Artificial Intelligence. To recap, each time a new customer connects to the online retail business website, that's a new round $n$ and we select one of our 9 strategies to attempt a conversion (subscription to the premium plan). The goal is to select the best strategy at each round, over many rounds. Here is how Thompson Sampling will do that:

\

\textbf{For each round $n$, repeat, over 1000 rounds, the following three steps:}

\begin{enumerate}

    \item \textbf{Step 1.} For each strategy $i$, we take a random draw from the following distribution:
    \begin{equation*}
        \theta_i(n) \sim \beta(N_i^1(n)+1,N_i^0(n)+1)
    \end{equation*}
    where:
    \begin{equation*}
        \begin{cases}
            \textrm{$N_i^1(n)$ is the number of times the strategy $i$ has received a 1 reward up to round $n$,} \\
            \textrm{$N_i^0(n)$ is the number of times the strategy $i$ has received a 0 reward up to round $n$.}
        \end{cases}
    \end{equation*}
    
    \item \textbf{Step 2.} We select the strategy $s(n)$ that has the highest $\theta_i(n)$:
    \begin{equation*}
        s(n) = \underset{i\in\{1,...,9\}}{\textrm{argmax}}(\theta_i(n))
    \end{equation*}
    
    \item \textbf{Step 3.} We update $N_{s(n)}^1(n)$ and $N_{s(n)}^0(n)$ according to the following conditions:
    \begin{itemize}
        \item If the strategy selected $s(n)$ received a 1 reward:
        \begin{equation*}
            N_{s(n)}^1(n) := N_{s(n)}^1(n) + 1
        \end{equation*}
        \item If the strategy selected $s(n)$ received a 0 reward:
        \begin{equation*}
            N_{s(n)}^0(n) := N_{s(n)}^0(n) + 1
        \end{equation*}
    \end{itemize}

\end{enumerate}

\textbf{Intuition.} Each strategy has its own beta distribution. Over the rounds, the beta distribution of the strategy with the highest conversion rate will be progressively shifted to the right, and the beta distributions of the strategies with lower conversion rates will be progressively shifted to the left (Steps 1 and 3). Therefore, because of Step 2, the strategy with the highest conversion rate will be more and more selected. Below is a graph displaying three beta distributions of three strategies, that will help you visualize this:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.1]{Beta_Distribution_Slide.png}
		\end{center}
\end{figure}

\newpage

\subsection{Implementation}

Let's provide the whole implementation of Thompson Sampling for this specific Caso Práctico, following the same simulation given above.

\

While implementing Thompson Sampling, we will also implement the Random Selection algorithm, which will simply select a random strategy at each round. This will be our benchmark to evaluate the performance of our Thompson Sampling model. Of course, Thompson Sampling and the Random Selection algorithm will be competing on the same simulation, that is on the same rewards matrix. And in the end, after the whole simulation is done, we will assess the performance of Thompson Sampling by computing the relative return, defined by the following formula:

\begin{equation*}
    \textrm{Relative Return} = \frac{\textrm{(Total Reward of Thompson Sampling)} - (\textrm{Total Reward of Random Selection})}{\textrm{Total Reward of Random Selection}} \times 100
\end{equation*}

We will also plot the histogram of selected ads, just to check that the strategy with the highest conversion rate (Strategy 7) was the one selected the most.

\

So if you are ready, here we go:

\

First, we import the required libraries and we set the parameters ($N = 10000$ customers and $d = 9$ strategies):

\

\begin{lstlisting}
# Artificial Intelligence for Business
# Maximizing the Revenues of an Online Retail Business with Thompson Sampling

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import random

# Setting the parameters
N = 10000
d = 9
\end{lstlisting}

\

Then, we create the simulation, by building the rewards matrix of 10000 rows corresponding to the customers, and 9 columns corresponding to the strategies. At each round and for each strategy, we draw a random number between 0 and 1, and if this random number is lower than the conversion rate of the strategy, the reward will be 1. Otherwise, it will be 0. That way we simulate the conversion rates listed above for our 9 strategies:

\

\begin{lstlisting}
# Creating the simulation
# conversion_rates = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
conversion_rates = [0.05,0.13,0.09,0.16,0.11,0.04,0.20,0.08,0.01]
X = np.array(np.zeros([N,d]))
for i in range(N):
    for j in range(d):
        if np.random.rand() <= conversion_rates[j]:
            X[i,j] = 1
\end{lstlisting}

\

Then, we will loop over the 10000 rows (or rounds) of this rewards matrix, and at each round we will get two separate strategy selections: one from the Random Selection algorithm, and one from Thompson Sampling. We keep track of the strategies selected by each of these two algorithms, and we compute the total reward accumulated over the rounds by each of them. Thompson Sampling is implemented following exactly the Steps 1, 2 and 3 provided above:

\

\begin{lstlisting}
# Implementing a Random Strategy and Thompson Sampling
strategies_selected_rs = []
strategies_selected_ts = []
total_reward_rs = 0
total_reward_ts = 0
numbers_of_rewards_1 = [0] * d
numbers_of_rewards_0 = [0] * d
for n in range(0, N):
    # Random Strategy
    strategy_rs = random.randrange(d)
    strategies_selected_rs.append(strategy_rs)
    reward_rs = X[n, strategy_rs]
    total_reward_rs = total_reward_rs + reward_rs
    # Thompson Sampling
    strategy_ts = 0
    max_random = 0
    for i in range(0, d):
        random_beta = random.betavariate(numbers_of_rewards_1[i] + 1,
                                         numbers_of_rewards_0[i] + 1)
        if random_beta > max_random:
            max_random = random_beta
            strategy_ts = i
    reward_ts = X[n, strategy_ts]
    if reward_ts == 1:
        numbers_of_rewards_1[strategy_ts] = numbers_of_rewards_1[strategy_ts] + 1
    else:
        numbers_of_rewards_0[strategy_ts] = numbers_of_rewards_0[strategy_ts] + 1
    strategies_selected_ts.append(strategy_ts)
    total_reward_ts = total_reward_ts + reward_ts
\end{lstlisting}

\

Then we compute the final score, which is the relative return of Thompson Sampling with respect to our benchmark that is the Random Selection:

\

\begin{lstlisting}
# Computing the Relative Return
relative_return = (total_reward_ts - total_reward_rs) / total_reward_rs * 100
print("Relative Return: {:.0f} %".format(relative_return))
\end{lstlisting}

And buckle up, by executing this code we obtain a final relative return, of...:

\begin{equation*}
    \textrm{Relative Return} = 91 \ \% \ !
\end{equation*}

In other words, Thompson Sampling almost doubled the performance of our Random Selection benchmark.

\newpage

And finally, we plot the Histogram of the selected strategies, to check that indeed Strategy 7 (of index 6) was the one most selected, since it is the one having the highest conversion rate:

\

\begin{lstlisting}
# Plotting the Histogram of Selections
plt.hist(strategies_selected_ts)
plt.title('Histogram of Selections')
plt.xlabel('Strategy')
plt.ylabel('Number of times the strategy was selected')
plt.show()
\end{lstlisting}

\

By executing this final code, we obtain the following histogram:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.8]{Histogram.png}
		\end{center}
\end{figure}

And indeed, it is well the strategy of index 6, that is Strategy 7, that was, by far, selected the most. Thompson Sampling was quickly able to identify it. And in fact, if we re-run the same code but with only 1000 customers, we realize that Thompson Sampling is still able to identify Strategy 7 as the best one.

\

Accordingly, Thompson Sampling surely did an amazing job for this Online Retail Business. Because not only it was able to identify the best strategy quickly in a few number of rounds, that is with a few customers, which saved a lot on the advertising and operating related costs. But also because of course, it was able to clearly figure out the strategy with the highest conversion rate. And indeed, if this Online Retail Business has 100 million customers, and if the premium plan has a price of \$ 100 per year, then deploying this best strategy that has a conversion rate of 20 \% would lead to generate an extra revenue of...:

\begin{equation*}
    \textrm{Extra Revenues generated} = 100000000 \times 0.2 \times 100 = \textrm{\$ $2$ Billion !!}
\end{equation*}

In other words, Thompson Sampling clearly and quickly maximized the revenues of this online retail business, while also saving a lot on the costs, therefore maximizing as well the profitability of the business.

\textbf{Regret Curve.}

\

The regret curve of a model (Random Strategy or Thompson Sampling) is the plot of the difference between the best strategy and the deployed model, with respect to the rounds.

\

The best strategy is computed by simply getting, at each round, the maximum of the accumulated rewards over all the different strategies. Therefore in our implementation, we will get the best strategy the following way:

\

\begin{lstlisting}
rewards_strategies = [0] * d
for n in range(0, N):
    # Best Strategy
    for i in range(0, d):
        rewards_strategies[i] = rewards_strategies[i] + X[n, i]
    total_reward_bs = max(rewards_strategies)
\end{lstlisting}

Then, the regret of Thompson Sampling is simply computed as the difference between the best strategy and the Thompson Sampling model:

\

\begin{lstlisting}
# Regret of Thompson Sampling
strategies_selected_ts = []
total_reward_ts = 0
total_reward_bs = 0
numbers_of_rewards_1 = [0] * d
numbers_of_rewards_0 = [0] * d
rewards_strategies = [0] * d
regret = []
for n in range(0, N):
    # Thompson Sampling
    strategy_ts = 0
    max_random = 0
    for i in range(0, d):
        random_beta = random.betavariate(numbers_of_rewards_1[i] + 1,
                                         numbers_of_rewards_0[i] + 1)
        if random_beta > max_random:
            max_random = random_beta
            strategy_ts = i
    reward_ts = X[n, strategy_ts]
    if reward_ts == 1:
        numbers_of_rewards_1[strategy_ts] = numbers_of_rewards_1[strategy_ts] + 1
    else:
        numbers_of_rewards_0[strategy_ts] = numbers_of_rewards_0[strategy_ts] + 1
    strategies_selected_ts.append(strategy_ts)
    total_reward_ts = total_reward_ts + reward_ts
    # Best Strategy
    for i in range(0, d):
        rewards_strategies[i] = rewards_strategies[i] + X[n, i]
    total_reward_bs = max(rewards_strategies)
    # Regret
    regret.append(total_reward_bs - total_reward_ts)
\end{lstlisting}

\

And same, the regret of the Random Strategy is simply computed as the difference between the best strategy and the random selection algorithm:

\

\begin{lstlisting}
# Regret of the Random Strategy
strategies_selected_rs = []
total_reward_rs = 0
total_reward_bs = 0
numbers_of_rewards_1 = [0] * d
numbers_of_rewards_0 = [0] * d
rewards_strategies = [0] * d
regret = []
for n in range(0, N):
    # Random Strategy
    strategy_rs = random.randrange(d)
    strategies_selected_rs.append(strategy_rs)
    reward_rs = X[n, strategy_rs]
    total_reward_rs = total_reward_rs + reward_rs
    # Best Strategy
    for i in range(0, d):
        rewards_strategies[i] = rewards_strategies[i] + X[n, i]
    total_reward_bs = max(rewards_strategies)
    # Regret
    regret.append(total_reward_bs - total_reward_rs)
\end{lstlisting}

\

And finally of course, we plot the regret over the rounds with this simple code (we don't have to specify the x-coordinates in the plt.plot() function because the rounds are already indexes from 0 to N):

\

\begin{lstlisting}
# Plotting the Regret Curve
plt.plot(regret)
plt.title('Regret Curve')
plt.xlabel('Round')
plt.ylabel('Regret')
plt.show()
\end{lstlisting}

\

Let's look at the results in the next page, for both the Random Strategy and Thompson Sampling.

\newpage

If we plot the Regret Curve of the Random Strategy, we thus obtain the following:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=1]{Regret_Curve_Random_Strategy.png}
		\end{center}
\end{figure}

And of course, we observe absolutely no convergence of the Random Strategy towards the Best Strategy.

\newpage

However if now we plot the Regret Curve of the Thompson Sampling model, we get the following beautiful curve:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=1]{Regret_Curve_Thompson_Sampling.png}
		\end{center}
\end{figure}

And obviously, Thompson Sampling is converging very well towards the best strategy.

\newpage

Eventually, here is the final code including that Regret Curve of Thompson Sampling:

\

\begin{lstlisting}
# Thompson Sampling

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import random

# Setting the parameters
N = 10000
d = 9

# Creating the simulation
# conversion_rates = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
conversion_rates = [0.05,0.13,0.09,0.16,0.11,0.04,0.20,0.08,0.01]
X = np.array(np.zeros([N,d]))
for i in range(N):
    for j in range(d):
        if np.random.rand() <= conversion_rates[j]:
            X[i,j] = 1

# Implementing a Random Strategy and Thompson Sampling with Regret Curve
strategies_selected_rs = []
strategies_selected_ts = []
total_reward_rs = 0
total_reward_ts = 0
total_reward_bs = 0
numbers_of_rewards_1 = [0] * d
numbers_of_rewards_0 = [0] * d
rewards_strategies = [0] * d
regret = []
for n in range(0, N):
    # Random Strategy
    strategy_rs = random.randrange(d)
    strategies_selected_rs.append(strategy_rs)
    reward_rs = X[n, strategy_rs]
    total_reward_rs = total_reward_rs + reward_rs
    # Thompson Sampling
    strategy_ts = 0
    max_random = 0
    for i in range(0, d):
        random_beta = random.betavariate(numbers_of_rewards_1[i] + 1,
                                         numbers_of_rewards_0[i] + 1)
        if random_beta > max_random:
            max_random = random_beta
            strategy_ts = i
    reward_ts = X[n, strategy_ts]
    if reward_ts == 1:
        numbers_of_rewards_1[strategy_ts] = numbers_of_rewards_1[strategy_ts] + 1
    else:
        numbers_of_rewards_0[strategy_ts] = numbers_of_rewards_0[strategy_ts] + 1
    strategies_selected_ts.append(strategy_ts)
    total_reward_ts = total_reward_ts + reward_ts
    # Best Strategy
    for i in range(0, d):
        rewards_strategies[i] = rewards_strategies[i] + X[n, i]
    total_reward_bs = max(rewards_strategies)
    # Regret
    regret.append(total_reward_bs - total_reward_ts)

# Computing the Absolute and Relative Return
absolute_return = total_reward_ts - total_reward_rs
relative_return = (total_reward_ts - total_reward_rs) / total_reward_rs * 100
print("Absolute Return: {:.0f} $".format(absolute_return))
print("Relative Return: {:.0f} %".format(relative_return))

# Plotting the Histogram of Selections
plt.hist(strategies_selected_ts)
plt.title('Histogram of Selections')
plt.xlabel('Strategy')
plt.ylabel('Number of times the strategy was selected')
plt.show()
plt.close()

# Plotting the Regret Curve
plt.plot(regret)
plt.title('Regret Curve')
plt.xlabel('Round')
plt.ylabel('Regret')
plt.show()

\end{lstlisting}

