[
["optimización-de-procesos.html", "Parte 1 Optimización de Procesos 1.1 Caso Práctico: Optimización de tareas en un almacén de comercio electrónico 1.2 Solución de Inteligencia Artificial 1.3 Implementación", " Parte 1 Optimización de Procesos Aquí vamos con nuestro primer caso práctico y nuestro primer modelo de IA. ¡Esperamos que estés listo! 1.1 Caso Práctico: Optimización de tareas en un almacén de comercio electrónico 1.1.1 Problema a resolver El problema a resolver será optimizar los flujos dentro del siguiente almacén: El almacén pertenece a una empresa online minorista que vende productos a una variedad de clientes. Dentro de este almacén, los productos se almacenan en 12 ubicaciones diferentes, etiquetadas con las siguientes letras de la A a la L: A medida que los clientes hacen los pedidos online, un robot de almacén autónomo se mueve por el almacén para recoger los productos para futuras entregas. Así es como se ve: Autonomous Warehouse Robot Las 12 ubicaciones están conectadas a un sistema informático, que clasifica en tiempo real las prioridades de recolección de productos para estas 12 ubicaciones. Por ejemplo, en un momento específico \\(t\\), devolverá la siguiente clasificación: Rango de Prioridad Ubicación 1 G 2 K 3 L 4 J 5 A 6 I 7 H 8 C 9 B 10 D 11 F 12 E La ubicación G tiene prioridad 1, lo que significa que es la máxima prioridad, ya que contiene un producto que debe recogerse y entregarse de inmediato. Nuestro robot de almacén autónomo debe moverse a la ubicación G por la ruta más corta, dependiendo de dónde se encuentre. Nuestro objetivo es construir una IA que regrese esa ruta más corta, donde sea que esté el robot. Pero luego, como vemos, las ubicaciones K y L están en las 3 prioridades principales. Por lo tanto, querremos implementar una opción para que nuestro Robot de almacén autónomo pase por algunas ubicaciones intermedias antes de llegar a su ubicación final de máxima prioridad. La forma en que el sistema calcula las prioridades de las ubicaciones está fuera del alcance de este caso práctico. La razón de esto es que puede haber muchas formas, desde reglas o algoritmos simples, hasta cálculos deterministas y aprendizaje automático. Pero la mayoría de estas formas no serían inteligencia artificial como la conocemos hoy. En lo que realmente queremos centrarnos es en la IA central, que abarca Q-Learning, Deep Q-Learning y otras ramas de Reinforcement Learning. Entonces, solo diremos, por ejemplo, que la ubicación G es la máxima prioridad porque uno de los clientes de platino más leales de la compañía hizo un pedido urgente de un producto almacenado en la ubicación G, que por lo tanto debe entregarse lo antes posible. Por lo tanto, en conclusión, nuestra misión es construir una IA que siempre tome la ruta más corta a la ubicación de máxima prioridad, sea cual sea la ubicación desde la que comienza, y tener la opción de ir a una ubicación intermedia que se encuentre entre las 3 prioridades principales. 1.1.2 Entorno a definir Al construir una IA, lo primero que siempre tenemos que hacer es definir el entorno. Y definir un entorno siempre requiere los tres elementos siguientes: Definir los estados Definir las acciones Definir las recompensas Definamos estos tres elementos, uno por uno. Definir los estados. Comencemos con los estados. El estado de entrada es simplemente la ubicación donde está nuestro Robot de almacén autónomo en cada momento \\(t\\). Sin embargo, dado que construiremos nuestra IA con ecuaciones matemáticas, codificaremos los nombres de las ubicaciones (A, B, C, …) en números de índice, con respecto a la siguiente asignación: Ubicación Estado A 0 B 1 C 2 D 3 E 4 F 5 G 6 H 7 I 8 J 9 K 10 L 11 Hay una razón específica por la que codificamos los estados con índices del 0 al 11, en lugar de otros enteros. La razón es que trabajaremos con matrices, una matriz de recompensas y una matriz de valores Q, y cada línea y columna de estas matrices corresponderá a una ubicación específica. Por ejemplo, la primera línea de cada matriz, que tiene el índice 0, corresponde a la ubicación A. La segunda línea / columna, que tiene el índice 1, corresponde a la ubicación B. Etc. Veremos el propósito de trabajar con matrices con más detalles. un poco más tarde. Definir las acciones. Ahora definamos las posibles acciones a realizar Las acciones son simplemente los siguientes movimientos que el robot puede hacer para ir de un lugar a otro. Entonces, por ejemplo, digamos que el robot está en la ubicación J, las posibles acciones que el robot puede llevar a cabo es ir a I, F o K. Y nuevamente, ya que trabajaremos con ecuaciones matemáticas, codificaremos estas acciones con los mismos índices que para los estados. Por lo tanto, siguiendo nuestro mismo ejemplo donde el robot está en la ubicación J en un momento específico, las posibles acciones que el robot puede jugar son, de acuerdo con nuestro mapeo anterior anterior: 5, 8 y 10. De hecho, el índice 5 corresponde a F, el índice 8 corresponde a I y el índice 10 corresponde a K. Por lo tanto, eventualmente, la lista total de acciones que la IA puede llevar a cabo en general es la siguiente: \\[actions = [0,1,2,3,4,5,6,7,8,9,10,11]\\] Obviamente, al estar en una ubicación específica, hay algunas acciones que el robot no puede llevar a cabo. Tomando el mismo ejemplo anterior, si el robot está en la ubicación J, puede ejecutar las acciones 5, 8 y 10, pero no puede ejecutar las otras acciones. Nos aseguraremos de especificar eso al atribuir una recompensa 0 a las acciones que no puede llevar a cabo, y una recompensa 1 a las acciones que si puede realizar. Y eso nos lleva a las recompensas. Definir las recompensas. Lo último que tenemos que hacer ahora para construir nuestro entorno es definir un sistema de recompensas. Más específicamente, tenemos que definir una función de recompensa \\(R\\) que toma como entradas un estado \\(s\\) y una acción \\(a\\), y devuelve una recompensa numérica que la IA obtendrá al llevar a cabo la acción \\(a\\) en el estado \\(s\\): \\[R : (\\textrm{state}, \\textrm{action}) \\mapsto r \\in \\mathbb{R}\\] Entonces, ¿cómo vamos a construir esa función para nuestro caso práctico? Aquí esto es simple. Dado que hay un número discreto y finito de estados (los índices de 0 a 11), así como un número discreto y finito de acciones (mismos índices de 0 a 11), la mejor manera de construir nuestra función de recompensa R es simplemente hacer una matriz. Nuestra función de recompensa será exactamente una matriz de 12 filas y 12 columnas, donde las filas corresponden a los estados y las columnas corresponden a las acciones. De esa forma, en nuestra función $R: (s, a) r in $, \\(s\\) será el índice de la fila de la matriz, \\(a\\) será el índice de la columna de matriz, y \\(r\\) será la celda de los índices \\((s, a)\\) en la matriz. Por lo tanto, lo único que tenemos que hacer ahora para definir nuestra función de recompensa es simplemente llenar esta matriz con las recompensas numéricas. Y como acabamos de decir en el párrafo anterior, lo que tenemos que hacer primero es atribuir, para cada una de las 12 ubicaciones, una recompensa 0 por las acciones que el robot no puede ejecutar, y una recompensa 1 por las acciones que el robot puede llevar a cabo. Al hacer eso para cada una de las 12 ubicaciones, terminaremos con una matriz de recompensas. Vamos a construirlo paso a paso, comenzando con la primera ubicación: Ubicación A. Cuando se encuentra en la ubicación A, el robot solo puede ir a la ubicación B. Por lo tanto, dado que la ubicación A tiene el índice 0 (primera fila de la matriz) y la ubicación B tiene el índice 1 (segunda columna de la matriz), la primera fila de la matriz de las recompensas obtendrá un 1 en la segunda columna y un 0 en todas las otras columnas, así: Ubicación B. Al estar en la ubicación B, el robot solo puede ir a tres ubicaciones diferentes: A, C y F. Dado que B tiene el índice 1 (segunda fila), y A, C, F tienen los índices respectivos 0, 2, 5 (1ra, 3ra. , y sexta columna), entonces la segunda fila de la matriz de recompensas obtendrá un 1 en las columnas 1a, 3a y 6a, y 0 en todas las otras columnas. Por lo tanto obtenemos: Ubicación C. Ocurre lo mismo, la ubicación C (de índice 2) solo está conectada a B y G (de índices 1 y 6), por lo que la tercera fila de la matriz de recompensas es: En el resto de ubicaciones… Al hacer lo mismo para todas las demás ubicaciones, finalmente obtenemos nuestra matriz final de recompensas: Felicidades, acabamos de definir las recompensas. Lo hicimos simplemente construyendo esta matriz de recompensas. Es importante entender que esta es la forma en que definimos el sistema de recompensas cuando hacemos Q-Learning con un número finito de entradas y acciones. En el Caso Práctico 2, veremos que procederemos de manera muy diferente. Ya casi hemos terminado, lo único que tenemos que hacer es atribuir grandes recompensas a las ubicaciones de mayor prioridad. Esto lo hará el sistema informático que devuelve las prioridades de recolección de productos para cada una de las 12 ubicaciones. Por lo tanto, dado que la ubicación G es la máxima prioridad, el sistema informático actualizará la matriz de recompensas atribuyendo una alta recompensa en la celda \\((G, G)\\): Y así es como el sistema de recompensas funcionará con Q-Learning. Atribuimos la recompensa más alta (aquí 1000) a la ubicación de máxima prioridad G. Luego puedes ver en las clases de vídeo del curso cómo podemos atribuir una recompensa más alta a la segunda ubicación de mayor prioridad (ubicación K), para hacer que nuestro robot pase por esto ubicación intermedia de máxima prioridad, optimizando así los flujos de movimiento por el almacén. 1.2 Solución de Inteligencia Artificial The AI Solution that will solve the problem described above is a Q-Learning model. Since the latter is based on Markov Decision Processes, or MDPs, we will start by explaining what they are, and then we will move on to the intuition and maths details behind the Q-Learning model. 1.2.1 Proceso de Decisión de Markov A Markov Decision Process is a tuple \\((S, A, T, R)\\) where: After defining the MDP, it is now important to remind that it relies on the following assumption: the probability of the future state \\(s_{t+1}\\) only depends on the current state \\(s_t\\) and action \\(a_t\\), and doesn’t depend on any of the previous states and actions. That is: \\[\\begin{equation*} \\mathbb{P}(s_{t+1}|s_0,a_0,s_1,a_1,...,s_t,a_t) = \\mathbb{P}(s_{t+1}|s_t,a_t) \\end{equation*}\\] Hence in other words, a Markov Decision Process has no memory. Now let’s recap what is going on in terms of MDPs. At every time \\(t\\): So now the question is: \\[\\begin{equation*} \\textbf{How does the AI know which action to play at each time $t$?} \\end{equation*}\\] To answer this question, we need to introduce the policy function. The policy function \\(\\pi\\) is exactly the function that, given a state \\(s_t\\), returns the action \\(a_t\\): \\[\\begin{equation*} \\pi: s_t \\in S \\mapsto a_t \\in A \\end{equation*}\\] Let’s denote by \\(\\Pi\\) the set of all possible policy functions. Then the choice of the best actions to play becomes an optimization problem. Indeed, it comes down to finding the optimal policy \\(\\pi^*\\) that maximizes the accumulated reward: \\[\\begin{equation*} \\pi^* = \\underset{\\pi \\in \\Pi}{\\textrm{argmax}} \\sum_{t \\ge 0} R(s_t,\\pi(s_t)) \\end{equation*}\\] Therefore of course the question becomes: \\[\\begin{equation*} \\textbf{How to find this optimal policy $\\pi^*$ ?} \\end{equation*}\\] This is where Q-Learning comes into play. 1.2.2 Q-Learning Before we start getting into the details of Q-Learning, we need to explain the concept of the Q-Value. To each couple of state and action \\((s,a)\\), we are going to associate a numeric value \\(Q(s,a)\\): \\[\\begin{equation*} Q: (s \\in S, a \\in A) \\mapsto Q(s,a) \\in \\mathbb{R} \\end{equation*}\\] We will say that \\(Q(s,a)\\) is “the Q-value of the action \\(a\\) played in the state \\(s\\)”. To understand the purpose of this “Q-Value”, we need to introduce the Temporal Difference. At the beginning \\(t=0\\), all the Q-values are initialized to 0: \\[\\begin{equation*} \\forall s \\in S, a \\in A, Q(s,a) = 0 \\end{equation*}\\] Now let’s suppose we are at time \\(t\\), in a certain state \\(s_t\\). We play a random action \\(a_t\\), which brings us to the state \\(s_{t+1}\\) and we get the reward \\(R(s_t,a_t)\\). We can now introduce the Temporal Difference, which is at the heart of Q-Learning. The Temporal Difference at time \\(t\\), denoted by \\(TD_t(s_t,a_t)\\), is the difference between: thus leading to: \\[\\begin{equation*} TD_t(s_t,a_t) = R(s_t,a_t) + \\gamma \\underset{a}{\\max}(Q(s_{t+1},a)) - Q(s_t,a_t) \\end{equation*}\\] \\[\\begin{equation*} \\textbf{OK great, but what exactly is the purpose of this Temporal Difference $TD_t(s_t,a_t)$?} \\end{equation*}\\] Let’s answer this question to give us some better AI intuition. \\(TD_t(s_t,a_t)\\) is like an intrinsic reward. The AI will learn the Q-values in such a way that: To that extent, the AI will iterate some updates of the Q-Values (through an equation called the Bellman equation) towards higher temporal differences. Accordingly, in the final next step of the Q-Learning algorithm, we use the Temporal Difference to reinforce the couples (state, action) from time \\(t-1\\) to time \\(t\\), according to the following equation: \\[\\begin{equation*} Q_t(s_t,a_t) = Q_{t-1}(s_t,a_t) + \\alpha TD_t(s_t,a_t) \\end{equation*}\\] where \\(\\alpha \\in \\mathbb{R}\\) is the learning rate, which dictates how fast the learning of the Q-Values goes, or how big the updates of the Q-Values are. Its value is usually a real number chosen between 0 and 1, like for example 0.01, 0.05, 0.1 or 0.5. The lower is its value, the smaller will be the updates of the Q-Values and the longer will be the Q-Learning. The higher is its value, the bigger will be the updates of the Q-Values and the faster will be the Q-Learning. This equation above is the Bellman equation. It is the pillar of Q-Learning. With this point of view, the Q-Values measure the accumulation of surprise or frustration associated with the couple of action and state \\((s_t,a_t)\\). In the surprise case, the AI is reinforced, and in the frustration case, the AI is weakened. Hence we want to learn the Q-Values that will give the AI the maximum “good surprise”. Accordingly, the decision of which action to play mostly depends on the Q-value \\(Q(s_t, a_t)\\). If the action \\(a_t\\) played in the state \\(s_t\\) is associated with a high Q-Value \\(Q(s_t,a_t)\\), the AI will have a higher tendency to choose \\(a_t\\). On the other hand if the action \\(a_t\\) played in the state \\(s_t\\) is associated with a small Q-value \\(Q(s_t, a_t)\\), the AI will have a smaller tendency to choose \\(a_t\\). There are several ways of choosing the best action to play. First, when being in a certain state \\(s_t\\), we could simply take the action \\(a_t\\) that maximizes the Q-Value \\(Q(s_t,a_t)\\): \\[\\begin{equation*} a_t = \\underset{a}{\\textrm{argmax}}(Q(s_t,a)) \\end{equation*}\\] This solution is the Argmax method. Another great solution, which turns out to be an even better solution for complex problems, is the Softmax method. The Softmax method consists of considering for each state \\(s\\) the following distribution: \\[\\begin{equation*} W_s: a \\in A \\mapsto \\frac{\\exp(Q(s,a))^{\\tau}}{\\sum_{a&#39;}\\exp(Q(s,a&#39;))^{\\tau}} \\textrm{ with } \\tau \\ge 0 \\end{equation*}\\] Then we choose which action \\(a\\) to play by taking a random draw from that distribution: \\[\\begin{equation*} a \\sim W_s(.) \\end{equation*}\\] However the problem we will solve in Case Study 1 will be simple enough to use the Argmax method, so this is what we will choose. 1.2.3 El algoritmo de Q-Learning al completo Let’s summarize the different steps of the whole Q-Learning process: : For all couples of states \\(s\\) and actions \\(a\\), the Q-Values are initialized to 0: \\[\\begin{equation*} \\forall s \\in S, a \\in A, Q_0(s,a) = 0 \\end{equation*}\\] We start in the initial state \\(s_0\\). We play a random possible action and we reach the first state \\(s_1\\). , we will repeat for a certain number of times (1000 times in our code) the following: 1.3 Implementación Now let’s provide and explain the whole implementation of this Q-Learning model, the solution of our warehouse flows optimization problem. First, we start by importing the libraries that will be used in this implementation. These only include the numpy library, which offers a practical way of working with arrays and mathematical operations: # Importar las librerías import numpy as np Then we set the parameters of our model. These include the discount factor \\(\\gamma\\) and the learning rate \\(\\alpha\\), which as we saw in Section 1.2, are the only parameters of the Q-Learning algorithm: # Setting the parameters gamma and alpha for the Q-Learning gamma = 0.75 alpha = 0.9 The two previous code sections were simply the introductory sections, before really starting to build our AI model. Now the next step is to start the first part of our implementation: Part 1 - Defining the Environment. And for that of course, we begin by defining the states, with a dictionary mapping the locations names (in letters from A to L) into the states (in indexes from 0 to 11): # PART 1 - DEFINING THE ENVIRONMENT # Defining the states location_to_state = {&#39;A&#39;: 0, &#39;B&#39;: 1, &#39;C&#39;: 2, &#39;D&#39;: 3, &#39;E&#39;: 4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: 7, &#39;I&#39;: 8, &#39;J&#39;: 9, &#39;K&#39;: 10, &#39;L&#39;: 11} Then we define the actions, with a simple list of indexes from 0 to 11. Remember that each action index corresponds to the next state (next location) where that action leads to: # Defining the actions actions = [0,1,2,3,4,5,6,7,8,9,10,11] And eventually, we define the rewards, by creating a matrix of rewards, where the rows correspond to the current states \\(s_t\\), the columns correspond to the actions \\(a_t\\) leading to the next state \\(s_{t+1}\\), and the cells contain the rewards \\(R(s_t,a_t)\\). If a cell \\((s_t,a_t)\\) has a 1, that means that we can play the action \\(a_t\\) from the current state \\(s_t\\) to reach the next state \\(s_{t+1}\\). If a cell \\((s_t,a_t)\\) has a 0, that means that we cannot play the action \\(a_t\\) from the current state \\(s_t\\) to reach any next state \\(s_{t+1}\\). And for now we will manually put a high reward (1000) inside the cell corresponding to location G, because it is the top priority location where the autonomous warehouse has to go to collect the products. Since location G has encoded index state 6, we put a 1000 reward on the cell of row 6 and column 6. Then later on we will improve our solution by implementing an automatic way of going to the top priority location, without having to manually update the matrix of rewards and leaving it initialized with 0s and 1s just as it should be. But in the meantime, here is below our matrix of rewards including the manual update: # Defining the rewards R = np.array([[0,1,0,0,0,0,0,0,0,0,0,0], [1,0,1,0,0,1,0,0,0,0,0,0], [0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,0,0,0,0], [0,0,0,0,0,0,0,0,1,0,0,0], [0,1,0,0,0,0,0,0,0,1,0,0], [0,0,1,0,0,0,1000,1,0,0,0,0], [0,0,0,1,0,0,1,0,0,0,0,1], [0,0,0,0,1,0,0,0,0,1,0,0], [0,0,0,0,0,1,0,0,1,0,1,0], [0,0,0,0,0,0,0,0,0,1,0,1], [0,0,0,0,0,0,0,1,0,0,1,0]]) That closes this first part. Now let’s begin the second part of our implementation: Part 2 - Building the AI Solution with Q-Learning. To that extent, we are going to follow the Q-Learning algorithm exactly as it was provided in Section 1.2. Hence we first initialize all the Q-Values, by creating our matrix of Q-Values full of zeros (in which same, the rows correspond to the current states \\(s_t\\), the columns correspond to the actions \\(a_t\\) leading to the next state \\(s_{t+1}\\), and the cells contain the Q-Values \\(Q(s_t,a_t))\\): # PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING # Initializing the Q-Values Q = np.array(np.zeros([12,12])) Then of course we implement the Q-Learning process, with a for loop over 1000 iterations, repeating 1000 times the steps of the Q-Learning process provided at the end of Section 1.2: # Implementing the Q-Learning process for i in range(1000): current_state = np.random.randint(0,12) playable_actions = [] for j in range(12): if R[current_state, j] &gt; 0: playable_actions.append(j) next_state = np.random.choice(playable_actions) TD = R[current_state, next_state] + gamma*Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state] Q[current_state, next_state] = Q[current_state, next_state] + alpha*TD Optional: at this stage of the code, our matrix of Q-Values is ready. We can have a look at it by executing the whole code we have implemented so far, and by entering the following print in the console: print(&quot;Q-Values:&quot;) print(Q.astype(int)) And we obtain the following matrix of final Q-Values: For more visual clarity, you can even check the matrix of Q-Values directly in Variable Explorer, by double clicking on Q. Then to get the Q-Values as integers you can click on “Format” and inside enter a float formatting of “%.0f”. You will obtain this, which is a bit more clear since you can see the indexes of the rows and columns in your matrix Q: Good, now that we have our matrix of Q-Values, we are ready to go into production! Hence we can move on to the third part of the implementation, Part 3 - Going into Production, inside which we will compute the optimal path from any starting location to any ending top priority location. The idea here will be to implement a “route” function, that will take as inputs the starting location where our autonomous warehouse robot is located at a specific time and the ending location where it has to go in top priority, and that will return as output the shortest route inside a list. However since we want to input the locations with there names (in letters), as opposed to their states (in indexes), we will need a dictionary that maps the locations states (in indexes) to the locations names (in letters). And that is the first thing we will do here in this third part, using a trick to inverse our previous dictionary “location_to_state”, since indeed we simply want to get the exact inverse mapping from this dictionary: # PART 3 - GOING INTO PRODUCTION # Making a mapping from the states to the locations state_to_location = {state: location for location, state in location_to_state.items()} This is when the most important code section comes into play. We are about to implement the final “route()” function that will take as inputs the starting and ending locations, and that will return the optimal path between these two locations. To explain exactly what this route function will do, let’s enumerate the different steps of the process, when going from location E to location G: We start at our starting location E. We get the state of location E, which according to our location_to_state mapping is \\(s_0 = 4\\). On the row of index \\(s_0 = 4\\) in our matrix of Q-Values, we find the column that has the maximum Q-Value (703). This column has index 8, so we play the action of index 8 which leads us to the next state \\(s_{t+1} = 8\\). We get the location of state 8, which according to our state_to_location mapping is location I. Hence our next location is location I, which is appended to our list containing the optimal path. We repeat the same previous 5-steps from our new starting location I, until we reach our final destination, location G. Hence, since we don’t know how many locations we will have to go through between the starting and ending locations, we have to make a while loop that will repeat the 5-steps process described above, and that will stop as soon as we reach the ending top priority location: # Making the final function that will return the optimal route def route(starting_location, ending_location): route = [starting_location] next_location = starting_location while (next_location != ending_location): starting_state = location_to_state[starting_location] next_state = np.argmax(Q[starting_state,]) next_location = state_to_location[next_state] route.append(next_location) starting_location = next_location return route Congratulations, our tool is now ready! When we test it to go from E to G, we get indeed the two possible optimal paths after printing the final route executing the whole code several times: # Printing the final route print(&#39;Route:&#39;) route(&#39;E&#39;, &#39;G&#39;) Route: Out[1]: [&#39;E&#39;, &#39;I&#39;, &#39;J&#39;, &#39;F&#39;, &#39;B&#39;, &#39;C&#39;, &#39;G&#39;] Out[2]: [&#39;E&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;H&#39;, &#39;G&#39;] Good, we have a first version of the model that is well functioning. But we can improve it in two ways. First, by automating the reward attribution to the top priority location, so that we don’t have to do it manually. And second, by adding a feature that gives us the option to go by an intermediary location before going to the top priority location. That intermediary location should be of course in the Top 3 priority locations. And as a matter of fact, in our top priority locations ranking, the second top priority location is location K. Therefore, in order to optimize even more the warehouse flows, our autonomous warehouse robot must go by location K to collect the products on its way to the top priority location G. A way to do this is to have the option to go by any intermediary location in the process of our “route()” function. And this is exactly what we will implement as a second improvement. But first, let’s implement the first improvement, that automates the reward attribution. The way to do that is two folds: first we must make a copy (called R_new) of our reward matrix inside which the route() function will automatically update the reward in the cell of the ending location. Indeed, the ending location is one of the inputs of the route() function, so using our location_to_state dictionary we can very easily find that cell and update its reward to 1000. And second, we must include the whole Q-Learning algorithm (including the initialization step) inside the route function, right after we make that update of the reward in our copy of the rewards matrix. Indeed, in our previous implementation above, the Q-Learning process happens on the original version of the rewards matrix, which is now supposed to stay as it is, i.e. initialized to 1s and 0s only. Therefore we must include the Q-Learning process inside the route function, and make it happen on our copy R_new of the rewards matrix, instead of the original rewards matrix R. Hence, our full implementation becomes the following: # Artificial Intelligence for Business # Optimizing Warehouse Flows with Q-Learning # Importing the libraries import numpy as np # Setting the parameters gamma and alpha for the Q-Learning gamma = 0.75 alpha = 0.9 # PART 1 - DEFINING THE ENVIRONMENT # Defining the states location_to_state = {&#39;A&#39;: 0, &#39;B&#39;: 1, &#39;C&#39;: 2, &#39;D&#39;: 3, &#39;E&#39;: 4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: 7, &#39;I&#39;: 8, &#39;J&#39;: 9, &#39;K&#39;: 10, &#39;L&#39;: 11} # Defining the actions actions = [0,1,2,3,4,5,6,7,8,9,10,11] # Defining the rewards R = np.array([[0,1,0,0,0,0,0,0,0,0,0,0], [1,0,1,0,0,1,0,0,0,0,0,0], [0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,0,0,0,0], [0,0,0,0,0,0,0,0,1,0,0,0], [0,1,0,0,0,0,0,0,0,1,0,0], [0,0,1,0,0,0,1,1,0,0,0,0], [0,0,0,1,0,0,1,0,0,0,0,1], [0,0,0,0,1,0,0,0,0,1,0,0], [0,0,0,0,0,1,0,0,1,0,1,0], [0,0,0,0,0,0,0,0,0,1,0,1], [0,0,0,0,0,0,0,1,0,0,1,0]]) # PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING # Making a mapping from the states to the locations state_to_location = {state: location for location, state in location_to_state.items()} # Making the final function that will return the route def route(starting_location, ending_location): R_new = np.copy(R) ending_state = location_to_state[ending_location] R_new[ending_state, ending_state] = 1000 Q = np.array(np.zeros([12,12])) for i in range(1000): current_state = np.random.randint(0,12) playable_actions = [] for j in range(12): if R_new[current_state, j] &gt; 0: playable_actions.append(j) next_state = np.random.choice(playable_actions) TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state] Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD route = [starting_location] next_location = starting_location while (next_location != ending_location): starting_state = location_to_state[starting_location] next_state = np.argmax(Q[starting_state,]) next_location = state_to_location[next_state] route.append(next_location) starting_location = next_location return route # PART 3 - GOING INTO PRODUCTION # Printing the final route print(&#39;Route:&#39;) route(&#39;E&#39;, &#39;G&#39;) By executing this new code several times, we get of course the same two possible optimal paths as before. Now let’s tackle the second improvement. There are three ways to add the option of going by the intermediary location K, the second top priority location: We give a high reward to the action leading from location J to location K. This high reward has to be larger than 1, and below 1000. Indeed it has to be larger than 1 so that the Q-Learning process favors the action leading from J to K, as opposed to the action leading from J to F which has reward 1. And it must be below than 1000 so we have to keep the highest reward on the top priority location, to make sure we end up there. Hence for example, in our rewards matrix we can give a high reward of 500 to the cell in the row of index 9 and the column of index 10, since indeed that cell corresponds to the action leading from location J (state index 9) to location K (state index 10). That way our autonomous warehouse robot will always go by location K on its way to location G. Here is how the matrix of rewards would be in that case: # Defining the rewards R = np.array([[0,1,0,0,0,0,0,0,0,0,0,0], [1,0,1,0,0,1,0,0,0,0,0,0], [0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,0,0,0,0], [0,0,0,0,0,0,0,0,1,0,0,0], [0,1,0,0,0,0,0,0,0,1,0,0], [0,0,1,0,0,0,1,1,0,0,0,0], [0,0,0,1,0,0,1,0,0,0,0,1], [0,0,0,0,1,0,0,0,0,1,0,0], [0,0,0,0,0,1,0,0,1,0,500,0], [0,0,0,0,0,0,0,0,0,1,0,1], [0,0,0,0,0,0,0,1,0,0,1,0]]) We give a bad reward to the action leading from location J to location F. This bad reward just has to be below 0. Indeed by punishing this action with a bad reward the Q-Learning process will never favor that action leading from J to F. Hence for example, in our rewards matrix we can give a bad reward of -500 to the cell in the row of index 9 and the column of index 5, since indeed that cell corresponds to the action leading from location J (state index 9) to location F (state index 5). That way our autonomous warehouse robot will never go trough location F on its way to location G. Here is how the matrix of rewards would be in that case: # Defining the rewards R = np.array([[0,1,0,0,0,0,0,0,0,0,0,0], [1,0,1,0,0,1,0,0,0,0,0,0], [0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,0,0,0,0], [0,0,0,0,0,0,0,0,1,0,0,0], [0,1,0,0,0,0,0,0,0,1,0,0], [0,0,1,0,0,0,1,1,0,0,0,0], [0,0,0,1,0,0,1,0,0,0,0,1], [0,0,0,0,1,0,0,0,0,1,0,0], [0,0,0,0,0,-500,0,0,1,0,1,0], [0,0,0,0,0,0,0,0,0,1,0,1], [0,0,0,0,0,0,0,1,0,0,1,0]]) We make an additional best_route() function, taking as inputs the three starting, intermediary and ending locations, that will call our previous route() function twice, a first time from the starting location to the intermediary location, and a second time from the intermediary location to the ending location. The first two ideas are easy to implement manually, but very tricky to implement automatically. Indeed, it is easy to find automatically the index of the intermediary location where we want to go by, but very difficult to get the index of the location that leads to that intermediary location, since it depends on the starting location and the ending location. You can try to implement either the first or second idea, you will see what I mean. Accordingly, we will implement the third idea, which can be coded in just two extra lines of code: # Making the final function that returns the optimal route def best_route(starting_location, intermediary_location, ending_location): return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:] Eventually, the final code including that major improvement for our warehouse flows optimization, becomes: # Artificial Intelligence for Business # Optimizing Warehouse Flows with Q-Learning # Importing the libraries import numpy as np # Setting the parameters gamma and alpha for the Q-Learning gamma = 0.75 alpha = 0.9 # PART 1 - DEFINING THE ENVIRONMENT # Defining the states location_to_state = {&#39;A&#39;: 0, &#39;B&#39;: 1, &#39;C&#39;: 2, &#39;D&#39;: 3, &#39;E&#39;: 4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: 7, &#39;I&#39;: 8, &#39;J&#39;: 9, &#39;K&#39;: 10, &#39;L&#39;: 11} # Defining the actions actions = [0,1,2,3,4,5,6,7,8,9,10,11] # Defining the rewards R = np.array([[0,1,0,0,0,0,0,0,0,0,0,0], [1,0,1,0,0,1,0,0,0,0,0,0], [0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,0,0,0,0], [0,0,0,0,0,0,0,0,1,0,0,0], [0,1,0,0,0,0,0,0,0,1,0,0], [0,0,1,0,0,0,1,1,0,0,0,0], [0,0,0,1,0,0,1,0,0,0,0,1], [0,0,0,0,1,0,0,0,0,1,0,0], [0,0,0,0,0,1,0,0,1,0,1,0], [0,0,0,0,0,0,0,0,0,1,0,1], [0,0,0,0,0,0,0,1,0,0,1,0]]) # PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING # Making a mapping from the states to the locations state_to_location = {state: location for location, state in location_to_state.items()} # Making a function that returns the shortest route from a starting to ending location def route(starting_location, ending_location): R_new = np.copy(R) ending_state = location_to_state[ending_location] R_new[ending_state, ending_state] = 1000 Q = np.array(np.zeros([12,12])) for i in range(1000): current_state = np.random.randint(0,12) playable_actions = [] for j in range(12): if R_new[current_state, j] &gt; 0: playable_actions.append(j) next_state = np.random.choice(playable_actions) TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state] Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD route = [starting_location] next_location = starting_location while (next_location != ending_location): starting_state = location_to_state[starting_location] next_state = np.argmax(Q[starting_state,]) next_location = state_to_location[next_state] route.append(next_location) starting_location = next_location return route # PART 3 - GOING INTO PRODUCTION # Making the final function that returns the optimal route def best_route(starting_location, intermediary_location, ending_location): return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:] # Printing the final route print(&#39;Route:&#39;) best_route(&#39;E&#39;, &#39;K&#39;, &#39;G&#39;) By executing this whole new code as many times as we want, we will always get the same expected output: Best Route: Out[1]: [&#39;E&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;H&#39;, &#39;G&#39;] "]
]
