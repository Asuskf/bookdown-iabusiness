<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Parte 5 Annex 2: Three Extra AI Models | Inteligencia Artificial aplicada a Negocios y Empresas</title>
  <meta name="description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Parte 5 Annex 2: Three Extra AI Models | Inteligencia Artificial aplicada a Negocios y Empresas" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" />
  <meta property="og:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />
  <meta property="og:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="github-repo" content="https://github.com/joanby/ia4business" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Parte 5 Annex 2: Three Extra AI Models | Inteligencia Artificial aplicada a Negocios y Empresas" />
  
  <meta name="twitter:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="twitter:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />

<meta name="author" content="Hadelin de Ponteves y Kirill Ermenko" />


<meta name="date" content="2020-03-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="Images/apple-icon-120x120.png" />
  <link rel="shortcut icon" href="Images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="annex-1-artificial-neural-networks.html"/>
<link rel="next" href="annex-3-questions-and-answers.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inteligencia Artificial aplicada Negocios y Empresas</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introducción</a></li>
<li class="chapter" data-level="1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html"><i class="fa fa-check"></i><b>1</b> Optimización de Procesos</a><ul>
<li class="chapter" data-level="1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#caso-práctico-optimización-de-tareas-en-un-almacén-de-comercio-electrónico"><i class="fa fa-check"></i><b>1.1</b> Caso Práctico: Optimización de tareas en un almacén de comercio electrónico</a><ul>
<li class="chapter" data-level="1.1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#problema-a-resolver"><i class="fa fa-check"></i><b>1.1.1</b> Problema a resolver</a></li>
<li class="chapter" data-level="1.1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#entorno-a-definir"><i class="fa fa-check"></i><b>1.1.2</b> Entorno a definir</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#solución-de-inteligencia-artificial"><i class="fa fa-check"></i><b>1.2</b> Solución de Inteligencia Artificial</a><ul>
<li class="chapter" data-level="1.2.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#proceso-de-decisión-de-markov"><i class="fa fa-check"></i><b>1.2.1</b> Proceso de Decisión de Markov</a></li>
<li class="chapter" data-level="1.2.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#q-learning"><i class="fa fa-check"></i><b>1.2.2</b> Q-Learning</a></li>
<li class="chapter" data-level="1.2.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#el-algoritmo-de-q-learning-al-completo"><i class="fa fa-check"></i><b>1.2.3</b> El algoritmo de Q-Learning al completo</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#implementación"><i class="fa fa-check"></i><b>1.3</b> Implementación</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html"><i class="fa fa-check"></i><b>2</b> Parte 2 - Minimización de Costes</a><ul>
<li class="chapter" data-level="2.1" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#caso-práctico-minimización-de-costes-en-el-consumo-energético-de-un-centro-de-datos"><i class="fa fa-check"></i><b>2.1</b> Caso Práctico: Minimización de Costes en el Consumo Energético de un Centro de Datos</a><ul>
<li class="chapter" data-level="2.1.1" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#problema-a-resolver-1"><i class="fa fa-check"></i><b>2.1.1</b> Problema a resolver</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#solución-de-ia"><i class="fa fa-check"></i><b>2.2</b> Solución de IA</a><ul>
<li class="chapter" data-level="2.2.1" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#q-learning-en-deep-learning"><i class="fa fa-check"></i><b>2.2.1</b> Q-Learning en Deep Learning</a></li>
<li class="chapter" data-level="2.2.2" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#experience-replay"><i class="fa fa-check"></i><b>2.2.2</b> Experience Replay</a></li>
<li class="chapter" data-level="2.2.3" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#el-cerebro"><i class="fa fa-check"></i><b>2.2.3</b> El cerebro</a></li>
<li class="chapter" data-level="2.2.4" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#el-algoritmo-de-deep-q-learning-al-completo"><i class="fa fa-check"></i><b>2.2.4</b> El algoritmo de Deep Q-Learning al completo</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#implementation"><i class="fa fa-check"></i><b>2.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="part-3-maximizing-revenues.html"><a href="part-3-maximizing-revenues.html"><i class="fa fa-check"></i><b>3</b> Part 3 - Maximizing Revenues</a><ul>
<li class="chapter" data-level="3.1" data-path="part-3-maximizing-revenues.html"><a href="part-3-maximizing-revenues.html#case-study-maximizing-revenue-of-an-online-retail-business"><i class="fa fa-check"></i><b>3.1</b> Case Study: Maximizing Revenue of an Online Retail Business</a><ul>
<li class="chapter" data-level="3.1.1" data-path="part-3-maximizing-revenues.html"><a href="part-3-maximizing-revenues.html#problem-to-solve"><i class="fa fa-check"></i><b>3.1.1</b> Problem to solve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="4" data-path="annex-1-artificial-neural-networks.html"><a href="annex-1-artificial-neural-networks.html"><i class="fa fa-check"></i><b>4</b> Annex 1: Artificial Neural Networks</a></li>
<li class="chapter" data-level="5" data-path="annex-2-three-extra-ai-models.html"><a href="annex-2-three-extra-ai-models.html"><i class="fa fa-check"></i><b>5</b> Annex 2: Three Extra AI Models</a></li>
<li class="chapter" data-level="6" data-path="annex-3-questions-and-answers.html"><a href="annex-3-questions-and-answers.html"><i class="fa fa-check"></i><b>6</b> Annex 3: Questions and Answers</a></li>
<li class="divider"></li>
<li><a href="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" target="blank">Curso en Udemy</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inteligencia Artificial aplicada a Negocios y Empresas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="annex-2-three-extra-ai-models" class="section level1">
<h1><span class="header-section-number">Parte 5</span> Annex 2: Three Extra AI Models</h1>
<p>As a Bonus, in this section we provide three extra AI models, closer to the State of the Art. However, these AI models are not necessarily adapted to solve business problems, but more to solve specific tasks like playing games or training a virtual robot to walk. We are going to study three powerful models, including two in the Deep Reinforcement Learning branch of AI, and one in the Policy Gradient branch:</p>


<p>In the previous section, our inputs were vectors encoded values defining the states of the environment. But since an encoded vector doesn’t preserve the spatial structure of an image, this is not the best form to describe a state. The spatial structure is indeed important because it gives us more information to predict the next state, and predicting the next state is of course essential for our AI to know what is the right next move. Therefore we need to preserve the spatial structure and to do that, our inputs must be 3D images (2D for the array of pixels plus one additional dimension for the colors). In that case, the inputs are simply the images of the screen itself, exactly like what a human sees when playing the game. Following this analogy, the AI acts like a human: it observes the input images of the screen when playing the game, the input images go into a convolutional neural network (the brain for a human) which will detect the state in each image. However, this convolutional neural network doesn’t contain pooling layers, because they would loose the location of the objects inside the image, and of course the AI need to keep track of the objects. Therefore we only keep the convolutional layers, and then by flattening them into a 1-dimensional vector, we get the input of our previous Deep Q-Learning network. Then the same process is being ran.</p>
<p><br />
</p>
<p>Therefore in summary, Deep Convolutional Q-Learning is the same as Deep Q-Learning, with the only difference that the inputs are now images, and a Convolutional Neural Network is added at the beginning of the fully-connected Deep Q-Learning network to detect the states (or simply the objects) of the images.</p>
<p><br />
</p>

<div style="page-break-after: always;"></div>


<p>So far, the action played at each time has been the output of one neural network, as if only one agent was deciding the strategy to play the game. This will no longer be the case with A3C. This time, we are going to have several agents, each one interacting with its own copy of the environment. Let’s say there are <span class="math inline">\(n\)</span> agents <span class="math inline">\(A_1\)</span>, <span class="math inline">\(A_2\)</span>,…, <span class="math inline">\(A_n\)</span>.</p>
<p><br />
</p>
<p>Each agent is sharing two networks: the actor and the critic. The critic evaluates the present states, while the actor evaluates the possible values in the present state. The actor is used to make decisions. At each epoch time of training for on agent, it takes the last version of the shared networks and uses the actor during n steps in order to make a decision. Over the n steps, it collects all the observed new states, the values of these new states, the rewards, etc… After the n steps, the agent uses the collected observations in order to update the shared models. The times of epoch, and therefore the times of updates of the shared network by the agent are not synchronous, hence the name.</p>
<p><br />
</p>
<p>That way, if an unlucky agent starts to be stuck into a suboptimal but attractive policy, it will reach out that state – because other agents also updated the shared policy before the agent got stuck – and will continue effective exploration.</p>
<p><br />
</p>
<p>In order to explain the update rules of the actor and the critic, let us see the networks as functions that depend on vectors of parameters <span class="math inline">\(\theta\)</span> (for the actor) and <span class="math inline">\(\theta_v\)</span> (for the critic).</p>

<p>The official A3C algorithm is the one of the Google DeepMind paper, “Asynchronous Methods for Deep Reinforcement Learning” (<a href="https://arxiv.org/pdf/1602.01783.pdf" class="uri">https://arxiv.org/pdf/1602.01783.pdf</a>). In this paper you will find it in the following S3 algorithm:</p>

<p><br />
</p>

<p><br />
</p>
<p>In this figure above we can clearly see the three As of the A3C:</p>

<div style="page-break-after: always;"></div>


<p>We want to build and train an AI that walks or runs across a field. The field is a flat ground that looks like this:</p>

<p>On this same field you can see a Half-Cheetah. This will be one of the agents we will train to walk on this field. Both the field and the agent form what we call an environment, which belongs to PyBullet, the official Python Interface for the Bullet Physics SDK specialized for Robotics Simulation and Reinforcement Learning, built and developed by Erwin Coumans. For more info click . You can also check the GitHub page on this .</p>

<p>The solution to our problem is a very recent AI model called , or . The related research paper was released by Horia Mania, Aurelia Guy and Benjamin Recht on March 20, 2018. You can find the full research paper .</p>
<p><br />
</p>
<p>ARS is based on a specific branch of Artificial Intelligence called Evolution Strategies. The difference is that ES uses parallelized deep neural networks of several layers, while ARS uses a simple linear policy, which is a Perceptron (a shallow neural network of one layer composed of several neurones). ARS is also slightly similar to  - , in the sense that ARS aims to optimize a policy (a function of the states returning the actions to play) that performs the best actions allowing the AI to walk. However the technique is different. If you are curious about , you can check out the research paper .</p>
<p><br />
</p>
<p>Now let’s deep dive into the ARS.</p>
<p><br />
</p>
<p>The whole picture is pretty simple. We have a policy, that takes as inputs the states of the environment, and returns as outputs the actions to play in order to walk and run across a field. Now before we start explaining the algorithm, let’s describe in more details the inputs, the outputs and the policy.</p>
<p><br />
</p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>The input is a vector encoding the states of the environment. What does it mean? First let’s explain what exactly is a state of the environment. A state is the exact situation happening at a specific time <span class="math inline">\(t\)</span>, e.g:</p>

<p>We can see the cheetah in the air, back legs up, front legs bent, about to land on the ground. All this is encoded into a vector. How? By simply gathering enough values that can describe what is happening here. So the encoded vector will contain the coordinates of the angular points of the cheetah, as well as the angles of rotation around the rotors, and more values like the velocity. Therefore at each time <span class="math inline">\(t\)</span>, a vector of same format is encoding what is happening exactly in the environment. This encoded vector is what we call the input state of the environment, and will be the input of our policy that we will try to optimize.</p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>The output, returned by our policy, is a group of actions played by the agent. More precisely, these actions are the different muscles impulsions of the agent. For example, one of the actions will be the intensity of the muscle pushing the back leg at the level of the foot. What is important to understand here is more the fact that the policy is returning a  as opposed to a single action. Indeed a common practice in Reinforcement Learning is to return one discreet action at each time <span class="math inline">\(t\)</span>. Here, not only we return a group of actions, but each of these actions is continuous. Indeed, in order for an agent to walk on a field, it has to move all the parts of its body at each time <span class="math inline">\(t\)</span>, as opposed to only one leg for example. And the actions are continuous because the impulsions of the muscles are measured by continuous metrics. Hence, the output is also a vector of several continuous values, just like the input state.</p>
<p></p>
<p><br />
</p>
<p>Between the inputs and the outputs we have a policy, which is nothing else than a function, taking as inputs the input states, and returning as outputs the actions to play, i.e. the muscle impulsions. This policy will be linear, since indeed it will be a perceptron, which is a simple neural network of one layer and several neurones:</p>
<p><br />
</p>
<p><br />
</p>

<p><br />
</p>
<p><br />
</p>
<p>The Hidden layer in the middle contains the different neurones of the perceptron. To each couple of (input value, output value) is attributed a weight. Therefore in total we have number_of_inputs <span class="math inline">\(\times\)</span> number_of_outputs weights. All these weights are gathered in a matrix, which is nothing else than the matrix of our linear policy. In this matrix, the rows correspond to the output values (the actions) and the columns correspond to the input values (of the states). Therefore this matrix of weights, called <span class="math inline">\(\Theta\)</span>, is composed of <span class="math inline">\(n = \textrm{number\_of\_outputs}\)</span> rows and <span class="math inline">\(m = \textrm{number\_of\_inputs columns}\)</span>:</p>
<p><br />
</p>
<p><span class="math display">\[\begin{equation*}
\boldsymbol{\Theta}
=
\begin{pmatrix}
(\textrm{input 1, output 1}) &amp; (\textrm{input 2, output 1}) &amp; \cdots &amp; (\textrm{input m, output 1}) \\
(\textrm{input 1, output 2}) &amp; (\textrm{input 2, output 2}) &amp; \cdots &amp; (\textrm{input m, output 2}) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
(\textrm{input 1, output n}) &amp; (\textrm{input 2, output n}) &amp; \cdots &amp; (\textrm{input m, output n})
\end{pmatrix}
=
\begin{pmatrix}
\theta_{1,1} &amp; \theta_{2,1} &amp; \cdots &amp; \theta_{m,1} \\
\theta_{1,2} &amp; \theta_{2,2} &amp; \cdots &amp; \theta_{m,2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\theta_{1,n} &amp; \theta_{2,n} &amp; \cdots &amp; \theta_{m,n}
\end{pmatrix}
\end{equation*}\]</span></p>
<div style="page-break-after: always;"></div>
<p></p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>At the very beginning, all the weights <span class="math inline">\(\theta_{i,j}\)</span> of our linear policy are initialized to zero:</p>
<p><span class="math display">\[\begin{equation*}
\forall i,j \in \{1,n\}\times\{1,m\}, \theta_{i,j} = 0
\end{equation*}\]</span></p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>Then, we are going to apply some very little perturbations to each of these weights, by adding some very small values <span class="math inline">\(\delta_{i,j}\)</span> to each of the <span class="math inline">\(\theta_{i,j}\)</span> in our matrix of weights:</p>
<p><span class="math display">\[\begin{equation*}
\begin{pmatrix}
\theta_{1,1} &amp; \theta_{2,1} &amp; \cdots &amp; \theta_{m,1} \\
\theta_{1,2} &amp; \theta_{2,2} &amp; \cdots &amp; \theta_{m,2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\theta_{1,n} &amp; \theta_{2,n} &amp; \cdots &amp; \theta_{m,n}
\end{pmatrix}
\longrightarrow
\begin{pmatrix}
\theta_{1,1} + \delta_{1,1} &amp; \theta_{2,1} + \delta_{2,1} &amp; \cdots &amp; \theta_{m,1} + \delta_{m,1} \\
\theta_{1,2} + \delta_{1,2} &amp; \theta_{2,2} + \delta_{2,2} &amp; \cdots &amp; \theta_{m,2} + \delta_{m,1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\theta_{1,n} + \delta_{1,n} &amp; \theta_{2,n} + \delta_{2,n} &amp; \cdots &amp; \theta_{m,n} + \delta_{m,n}
\end{pmatrix}
\end{equation*}\]</span></p>
<p><br />
</p>
<p>We will call this: “applying some perturbations in one ” <span class="math inline">\(+\Delta_k\)</span>, where <span class="math inline">\(\Delta_k\)</span> is the following matrix of perturbations:</p>
<p><span class="math display">\[\begin{equation*}
\Delta_k
=
\begin{pmatrix}
\delta_{1,1} &amp; \delta_{2,1} &amp; \cdots &amp; \delta_{m,1} \\
\delta_{1,2} &amp; \delta_{2,2} &amp; \cdots &amp; \delta_{m,2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\delta_{1,n} &amp; \delta_{2,n} &amp; \cdots &amp; \delta_{m,n}
\end{pmatrix}
\end{equation*}\]</span></p>
<p><br />
</p>
<p>“Positive” comes from the fact that we are  the small values <span class="math inline">\(\delta_{i,j}\)</span> to our weights <span class="math inline">\(\theta_{i,j}\)</span>. These little perturbations <span class="math inline">\(\delta_{i,j}\)</span> are sampled from a Gaussian distribution <span class="math inline">\(\mathcal{N}(0,\sigma)\)</span> (the standard deviation <span class="math inline">\(\sigma\)</span> is what we call “noise” in the ARS model.</p>
<p><br />
</p>
<p>And each time we do this, we are also going to apply the exact same perturbations <span class="math inline">\(\delta_{i,j}\)</span> to our weights <span class="math inline">\(\theta_{i,j}\)</span>, but in the opposite direction <span class="math inline">\(-\Delta_k\)</span>, by simply this time subtracting the exact same <span class="math inline">\(\delta_{i,j}\)</span>:</p>
<p><span class="math display">\[\begin{equation*}
\begin{pmatrix}
\theta_{1,1} &amp; \theta_{2,1} &amp; \cdots &amp; \theta_{m,1} \\
\theta_{1,2} &amp; \theta_{2,2} &amp; \cdots &amp; \theta_{m,2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\theta_{1,n} &amp; \theta_{2,n} &amp; \cdots &amp; \theta_{m,n}
\end{pmatrix}
\longrightarrow
\begin{pmatrix}
\theta_{1,1} - \delta_{1,1} &amp; \theta_{2,1} - \delta_{2,1} &amp; \cdots &amp; \theta_{m,1} - \delta_{m,1} \\
\theta_{1,2} - \delta_{1,2} &amp; \theta_{2,2} - \delta_{2,2} &amp; \cdots &amp; \theta_{m,2} - \delta_{m,1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\theta_{1,n} - \delta_{1,n} &amp; \theta_{2,n} - \delta_{2,n} &amp; \cdots &amp; \theta_{m,n} - \delta_{m,n}
\end{pmatrix}
\end{equation*}\]</span></p>
<p><br />
</p>
<p>We will call this: “applying some perturbations in the ” <span class="math inline">\(-\Delta_k\)</span>.</p>
<p><br />
</p>
<p>Therefore in conclusion, we sample one specific matrix of perturbations <span class="math inline">\(\Delta_k\)</span> with some values <span class="math inline">\(\delta_{i,j}\)</span> close to zero and we update the weights of our matrix <span class="math inline">\(\Theta\)</span> in the positive direction <span class="math inline">\(+ \Delta_k\)</span> and the negative direction <span class="math inline">\(- \Delta_k\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\textbf{Positive Direction: } &amp; \Theta \rightarrow \Theta + \Delta_k \\
\textbf{Negative Direction: } &amp; \Theta \rightarrow \Theta - \Delta_k
\end{align*}\]</span></p>
<p><br />
</p>
<p>And in fact, over each full episode, we are going to apply these positive and negative perturbations for many different directions <span class="math inline">\(\Delta_1\)</span>, <span class="math inline">\(\Delta_2\)</span>, <span class="math inline">\(\Delta_3\)</span>, etc. We will do this for 16 different directions:</p>
<p><span class="math display">\[\begin{align*}
\textbf{Positive Directions: } &amp; \Theta \rightarrow \Theta + \Delta_1, \ \Theta \rightarrow \Theta + \Delta_2, \ ... \ , \ \Theta \rightarrow \Theta + \Delta_{16} \\
\textbf{Negative Directions: } &amp; \Theta \rightarrow \Theta - \Delta_1, \ \Theta \rightarrow \Theta - \Delta_2, \ ... \ , \ \Theta \rightarrow \Theta - \Delta_{16}
\end{align*}\]</span></p>
<p><br />
</p>
<p>Now it is time to ask: why are we doing this?</p>
<p><br />
</p>
<p>The reason is actually simple and intuitive to understand. We want to update the weights in these different directions to find the ones that will increase the most the total reward over the episodes. We want to figure out which updates of the weights will lead to the highest rewards. Indeed, increasing the total reward accumulated over the episode is our ultimate goal, since the higher is the reward, the better the agent will have the ability to walk.</p>
<p><br />
</p>
<p>Now another question, less obvious: Why, for each direction, do we want to take the positive and the negative one?</p>
<p><br />
</p>
<p>That is because, once we figure out the directions that increase the most the rewards (by simply getting the accumulated reward over the full episode for each direction and then sorting them by the highest obtained), we will do one step of gradient descent to update the weights in these best directions. However we don’t have any reward function of the weights, so we couldn’t apply gradient descent directly. Indeed, in order to apply gradient descent we would need to have a reward function of the weights, <span class="math inline">\(r(\Theta)\)</span>, differentiate it with respect to the weights:</p>
<p><span class="math display">\[\begin{equation*}
\frac{\partial r(\Theta)}{\partial \Theta}
\end{equation*}\]</span></p>
<p>and then do this one step of gradient descent to update the weights:</p>
<p><span class="math display">\[\begin{equation*}
\Theta(\textrm{new}) := \Theta(\textrm{old}) + \frac{\partial r(\Theta)}{\partial \Theta} d \Theta
\end{equation*}\]</span></p>
<p><br />
</p>
<p>But we cannot do that because we don’t have an explicit expression of the reward with respect to the weights. So instead of computing directly this gradient, we will approximate it. And that’s where the combo of positive &amp; negative directions comes into play, with the method of finite differences.</p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>So now we understand that we have to do one step of gradient descent to update the weights in the directions that increase the most the reward, and that to do this one step we have no choice but to approximate the gradient of the rewards with respect to the weights. More specifically, we have to approximate:</p>
<p><span class="math display">\[\begin{equation*}
\frac{\partial r(\Theta)}{\partial \Theta} d \Theta
\end{equation*}\]</span></p>
<p><br />
</p>
<p>Well with what we have done before applying the perturbations in the positive and negative directions, we will be able to approximate this easily. Since the value of each perturbation <span class="math inline">\(\delta\)</span> is a very small number close to zero, then the difference between the reward <span class="math inline">\(r_{+}\)</span> we get when applying the perturbation in the positive direction (<span class="math inline">\(\Theta \rightarrow \Theta + \Delta\)</span>) and the reward <span class="math inline">\(r_{-}\)</span> we get when applying the perturbation in the negative (or opposite) direction (<span class="math inline">\(\Theta \rightarrow \Theta - \Delta\)</span>) is approximately equal to that gradient:</p>
<p><span class="math display">\[\begin{equation*}
r_{+} - r_{-} \approx \frac{\partial r(\Theta)}{\partial \Theta}
\end{equation*}\]</span></p>
<p>so that we get the following approximation:</p>
<p><span class="math display">\[\begin{equation*}
(r_{+} - r_{-}) \Delta \approx \frac{\partial r(\Theta)}{\partial \Theta} d \Theta
\end{equation*}\]</span></p>
<p><br />
</p>
<p>This approximation is the result of the method of finite differences and allows us to do this one step of approximated gradient descent.</p>
<p><br />
</p>
<p>Then we choose a number of best directions we want to keep as the ones leading to the highest rewards and we do this one step of approximated gradient descent on all these best directions. How do we know the top directions that increase the most the rewards? Well let’s say we want to keep 16 best directions, we simply apply the positive and negative perturbations for each of all our directions over one full episode, we store the couple of rewards <span class="math inline">\((r_{+}, r_{-})\)</span> we get for each of these directions, and eventually we keep the 16 highest maximums of <span class="math inline">\(r_{+}\)</span> and <span class="math inline">\(r_{-}\)</span>. These 16 highest rewards correspond to our 16 best directions.</p>
<p><br />
</p>
<p>Then eventually we average our approximated gradients over those 16 best directions to update the whole matrix of weights <span class="math inline">\(\Theta\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\Theta(\textrm{new}) = \Theta(\textrm{old}) + \frac{1}{16}\sum_{k=1}^{16} [r_{+}(\textrm{$k^{th}$ best direction}) - r_{-}(\textrm{$k^{th}$ best direction})] \Delta_{\textrm{$k^{th}$ best direction}}
\end{equation}\]</span></p>
<p><br />
</p>
<p>Right after this update, the step of gradient descent is applied to the whole matrix of weights <span class="math inline">\(\Theta\)</span>, so that the weights of our policy are updated into the top directions that increase the most the accumulated reward.</p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>Eventually, we repeat this whole process (apart from the initialization of the weights to zero) for a certain number of steps (e.g. 1000 steps).</p>
<p><br />
</p>
<p>We can improve the performance of the ARS with the two following action items:</p>
<p><br />
</p>

<p><br />
</p>
<p>Let’s have a look at each of these improvement solutions in the next page.</p>
<div style="page-break-after: always;"></div>
<p></p>
<p><br />
</p>
<p>In the , we have the options between  and  (see page 6).  is the algorithm above without normalizing the input states, and  is the ARS with normalized input states.</p>
<p><br />
</p>
<p>Normalizing the states clearly improves the performance.</p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>We can scale by dividing the previous sum in equation (1) by the standard deviation <span class="math inline">\(\sigma_r\)</span> of the reward, so that we get:</p>
<p><span class="math display">\[\begin{equation}
\Theta(\textrm{new}) = \Theta(\textrm{old}) + \frac{1}{16 \sigma_r} \sum_{k=1}^{16} [r_{+}(\textrm{$k^{th}$ best direction}) - r_{-}(\textrm{$k^{th}$ best direction})] \Delta_{\textrm{$k^{th}$ best direction}}
\end{equation}\]</span></p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>For tuning purposes we can add a learning rate factor in equation (2) (denoted by <span class="math inline">\(\alpha\)</span> in the paper):</p>
<p><span class="math display">\[\begin{equation*}
\Theta(\textrm{new}) = \Theta(\textrm{old}) + \frac{\alpha}{16 \sigma_r} \sum_{k=1}^{16} [r_{+}(\textrm{$k^{th}$ best direction}) - r_{-}(\textrm{$k^{th}$ best direction})] \Delta_{\textrm{$k^{th}$ best direction}}
\end{equation*}\]</span></p>
<div style="page-break-after: always;"></div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="annex-1-artificial-neural-networks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="annex-3-questions-and-answers.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/joanby/ia4business/edit/master/5.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["curso-ia-business-udemy.pdf", "curso-ia-business-udemy.epub"],
"toc": {
"collapse": "subsection"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
