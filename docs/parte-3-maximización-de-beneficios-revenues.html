<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Parte 3 Parte 3 - Maximización de Beneficios Revenues | Inteligencia Artificial aplicada a Negocios y Empresas</title>
  <meta name="description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Parte 3 Parte 3 - Maximización de Beneficios Revenues | Inteligencia Artificial aplicada a Negocios y Empresas" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" />
  <meta property="og:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />
  <meta property="og:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="github-repo" content="https://github.com/joanby/ia4business" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Parte 3 Parte 3 - Maximización de Beneficios Revenues | Inteligencia Artificial aplicada a Negocios y Empresas" />
  
  <meta name="twitter:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="twitter:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />

<meta name="author" content="Hadelin de Ponteves y Kirill Ermenko" />


<meta name="date" content="2020-03-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="Images/apple-icon-120x120.png" />
  <link rel="shortcut icon" href="Images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="parte-2-minimización-de-costes.html"/>
<link rel="next" href="conclusión.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inteligencia Artificial aplicada Negocios y Empresas</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introducción</a></li>
<li class="chapter" data-level="1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html"><i class="fa fa-check"></i><b>1</b> Optimización de Procesos</a><ul>
<li class="chapter" data-level="1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#caso-práctico-optimización-de-tareas-en-un-almacén-de-comercio-electrónico"><i class="fa fa-check"></i><b>1.1</b> Caso Práctico: Optimización de tareas en un almacén de comercio electrónico</a><ul>
<li class="chapter" data-level="1.1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#problema-a-resolver"><i class="fa fa-check"></i><b>1.1.1</b> Problema a resolver</a></li>
<li class="chapter" data-level="1.1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#entorno-a-definir"><i class="fa fa-check"></i><b>1.1.2</b> Entorno a definir</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#solución-de-inteligencia-artificial"><i class="fa fa-check"></i><b>1.2</b> Solución de Inteligencia Artificial</a><ul>
<li class="chapter" data-level="1.2.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#proceso-de-decisión-de-markov"><i class="fa fa-check"></i><b>1.2.1</b> Proceso de Decisión de Markov</a></li>
<li class="chapter" data-level="1.2.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#q-learning"><i class="fa fa-check"></i><b>1.2.2</b> Q-Learning</a></li>
<li class="chapter" data-level="1.2.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#el-algoritmo-de-q-learning-al-completo"><i class="fa fa-check"></i><b>1.2.3</b> El algoritmo de Q-Learning al completo</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#implementación"><i class="fa fa-check"></i><b>1.3</b> Implementación</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html"><i class="fa fa-check"></i><b>2</b> Parte 2 - Minimización de Costes</a><ul>
<li class="chapter" data-level="2.1" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#caso-práctico-minimización-de-costes-en-el-consumo-energético-de-un-centro-de-datos"><i class="fa fa-check"></i><b>2.1</b> Caso Práctico: Minimización de Costes en el Consumo Energético de un Centro de Datos</a><ul>
<li class="chapter" data-level="2.1.1" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#problema-a-resolver-1"><i class="fa fa-check"></i><b>2.1.1</b> Problema a resolver</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#solución-de-ia"><i class="fa fa-check"></i><b>2.2</b> Solución de IA</a><ul>
<li class="chapter" data-level="2.2.1" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#q-learning-en-deep-learning"><i class="fa fa-check"></i><b>2.2.1</b> Q-Learning en Deep Learning</a></li>
<li class="chapter" data-level="2.2.2" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#experience-replay"><i class="fa fa-check"></i><b>2.2.2</b> Experience Replay</a></li>
<li class="chapter" data-level="2.2.3" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#el-cerebro"><i class="fa fa-check"></i><b>2.2.3</b> El cerebro</a></li>
<li class="chapter" data-level="2.2.4" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#el-algoritmo-de-deep-q-learning-al-completo"><i class="fa fa-check"></i><b>2.2.4</b> El algoritmo de Deep Q-Learning al completo</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#implementation"><i class="fa fa-check"></i><b>2.3</b> Implementation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#paso-1-construcción-del-entorno"><i class="fa fa-check"></i><b>2.3.1</b> Paso 1: Construcción del Entorno</a></li>
<li class="chapter" data-level="2.3.2" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#paso-2-construcción-del-cerebro"><i class="fa fa-check"></i><b>2.3.2</b> Paso 2: Construcción del cerebro</a></li>
<li class="chapter" data-level="2.3.3" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#paso-3-implementación-del-algoritmo-de-deep-reinforcement-learning"><i class="fa fa-check"></i><b>2.3.3</b> Paso 3: Implementación del algoritmo de Deep Reinforcement Learning</a></li>
<li class="chapter" data-level="2.3.4" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#paso-4-entrenar-la-ia"><i class="fa fa-check"></i><b>2.3.4</b> Paso 4: Entrenar la IA</a></li>
<li class="chapter" data-level="2.3.5" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#paso-5-probar-nuestra-ia"><i class="fa fa-check"></i><b>2.3.5</b> Paso 5: Probar nuestra IA</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="parte-2-minimización-de-costes.html"><a href="parte-2-minimización-de-costes.html#resumen-el-algoritmo-general-de-ia"><i class="fa fa-check"></i><b>2.4</b> Resumen: El Algoritmo General de IA</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="parte-3-maximización-de-beneficios-revenues.html"><a href="parte-3-maximización-de-beneficios-revenues.html"><i class="fa fa-check"></i><b>3</b> Parte 3 - Maximización de Beneficios Revenues</a><ul>
<li class="chapter" data-level="3.1" data-path="parte-3-maximización-de-beneficios-revenues.html"><a href="parte-3-maximización-de-beneficios-revenues.html#caso-práctico-maximización-de-beeficios-de-un-negocio-de-venta-online-en-línea"><i class="fa fa-check"></i><b>3.1</b> Caso Práctico: Maximización de beeficios de un negocio de venta online en línea</a><ul>
<li class="chapter" data-level="3.1.1" data-path="parte-3-maximización-de-beneficios-revenues.html"><a href="parte-3-maximización-de-beneficios-revenues.html#problema-a-reesolver"><i class="fa fa-check"></i><b>3.1.1</b> Problema a reesolver</a></li>
<li class="chapter" data-level="3.1.2" data-path="parte-3-maximización-de-beneficios-revenues.html"><a href="parte-3-maximización-de-beneficios-revenues.html#definición-del-entorno"><i class="fa fa-check"></i><b>3.1.2</b> Definición del Entorno</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="parte-3-maximización-de-beneficios-revenues.html"><a href="parte-3-maximización-de-beneficios-revenues.html#solución-de-ia-1"><i class="fa fa-check"></i><b>3.2</b> Solución de IA</a></li>
<li class="chapter" data-level="3.3" data-path="parte-3-maximización-de-beneficios-revenues.html"><a href="parte-3-maximización-de-beneficios-revenues.html#implementación-1"><i class="fa fa-check"></i><b>3.3</b> Implementación</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusión.html"><a href="conclusión.html"><i class="fa fa-check"></i>Conclusión</a></li>
<li class="chapter" data-level="4" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html"><i class="fa fa-check"></i><b>4</b> Anexos adicionales</a><ul>
<li class="chapter" data-level="4.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#anexo-1-redes-neuronales-artificiales"><i class="fa fa-check"></i><b>4.1</b> Anexo 1: Redes Neuronales Artificiales</a><ul>
<li class="chapter" data-level="4.1.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#la-neurona"><i class="fa fa-check"></i><b>4.1.1</b> La Neurona</a></li>
<li class="chapter" data-level="4.1.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#la-función-de-activación"><i class="fa fa-check"></i><b>4.1.2</b> La Función de Activación</a></li>
<li class="chapter" data-level="4.1.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#cómo-funcionan-las-redes-neuronales"><i class="fa fa-check"></i><b>4.1.3</b> ¿Cómo funcionan las Redes Neuronales?</a></li>
<li class="chapter" data-level="4.1.4" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#cómo-aprenden-las-redes-neuronales"><i class="fa fa-check"></i><b>4.1.4</b> ¿Cómo aprenden las Redes Neuronales?</a></li>
<li class="chapter" data-level="4.1.5" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#propagación-hacia-adelante-and-propagación-hacia-atrás"><i class="fa fa-check"></i><b>4.1.5</b> Propagación hacia adelante and propagación hacia atrás</a></li>
<li class="chapter" data-level="4.1.6" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#gradiente-descendente"><i class="fa fa-check"></i><b>4.1.6</b> Gradiente Descendente</a></li>
<li class="chapter" data-level="4.1.7" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#gradiente-descendente-en-bloques"><i class="fa fa-check"></i><b>4.1.7</b> Gradiente Descendente en bloques</a></li>
<li class="chapter" data-level="4.1.8" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#optimizers"><i class="fa fa-check"></i><b>4.1.8</b> Optimizers</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#anexo-2-three-extra-ai-models"><i class="fa fa-check"></i><b>4.2</b> Anexo 2: Three Extra AI Models</a><ul>
<li class="chapter" data-level="4.2.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#deep-convolutional-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Deep Convolutional Q-Learning</a></li>
<li class="chapter" data-level="4.2.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#asynchronous-actor-critic-agents-a3c"><i class="fa fa-check"></i><b>4.2.2</b> Asynchronous Actor-Critic Agents (A3C)</a></li>
<li class="chapter" data-level="4.2.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#augmented-random-search"><i class="fa fa-check"></i><b>4.2.3</b> Augmented Random Search</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#anexo-3-questions-and-answers"><i class="fa fa-check"></i><b>4.3</b> Anexo 3: Questions and Answers</a><ul>
<li class="chapter" data-level="4.3.1" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#qas-on-part-1---optimizing-processes"><i class="fa fa-check"></i><b>4.3.1</b> Q&amp;As on Part 1 - Optimizing Processes</a></li>
<li class="chapter" data-level="4.3.2" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#qas-on-part-2---minimizing-costs"><i class="fa fa-check"></i><b>4.3.2</b> Q&amp;As on Part 2 - Minimizing Costs</a></li>
<li class="chapter" data-level="4.3.3" data-path="anexos-adicionales.html"><a href="anexos-adicionales.html#qas-on-part-3---maximizing-revenues"><i class="fa fa-check"></i><b>4.3.3</b> Q&amp;As on Part 3 - Maximizing Revenues</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" target="blank">Curso en Udemy</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inteligencia Artificial aplicada a Negocios y Empresas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parte-3---maximización-de-beneficios-revenues" class="section level1">
<h1><span class="header-section-number">Parte 3</span> Parte 3 - Maximización de Beneficios Revenues</h1>
<p>¡Felicidades por seguir el primer y segundo estudio de caso! Ahora pasemos a un tipo muy diferente de Inteligencia Artificial, que tiene una eficiencia tremenda para las empresas y negocios y que sin duda alguna debes conocer.</p>
<div id="caso-práctico-maximización-de-beeficios-de-un-negocio-de-venta-online-en-línea" class="section level2">
<h2><span class="header-section-number">3.1</span> Caso Práctico: Maximización de beeficios de un negocio de venta online en línea</h2>
<div id="problema-a-reesolver" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Problema a reesolver</h3>
<p>Imagina un negocio minorista en línea que tiene millones de clientes. Estos clientes son solo personas que compran algunos productos en el sitio web de vez en cuando y se los entregan en casa (como Amazon). El negocio está funcionando bien, pero la junta directiva ha decidido tomar algún plan de acción para maximizar aún más los ingresos. Este plan consiste en ofrecer a los clientes la opción de suscribirse a un plan premium, que les dará algunos beneficios como precios reducidos, ofertas especiales, etc. Este plan premium se ofrece a un precio anual de 100 dólares y el objetivo de este negocio minorista en línea es, por supuesto, conseguir que el máximo de clientes se suscriba a este plan premium. Hagamos algunos cálculos rápidos para motivarnos a construir una IA para maximizar los ingresos de este negocio. Digamos que este negocio minorista en línea tiene 100 millones de clientes. Ahora consideremos dos estrategias de conversión que intentan convertir a los clientes al plan premium: una mala, con una tasa de conversión del 1%, y una buena, con una tasa de conversión del 11%. Si el negocio lleva a camo la estrategia mala, obtendrá en un año un ingreso extra total proveniente de la suscripción al plan premium de: <span class="math inline">\(100.000.000 \times 0.01 \times 100 = 100.000.000\$\)</span>. Por otro lado, si el negocio implementa la buena estrategia, obtendrá en un año un ingreso extra total proveniente de la suscripción al plan premium de <span class="math inline">\(100.000.000 \times 0.11 \times 100 = 1.100.000.000\$\)</span>. Por lo tanto, al descubrir la buena estrategia para implementar, el negocio maximizará sus ingresos adicionales al ganar más de mil millones de dólares adicionales.</p>
<p>En este ejemplo utópico anterior, solo teníamos dos estrategias, y además sabíamos sus tasas de conversión. Sin embargo, en nuestro caso práctico enfrentaremos 9 estrategias diferentes, y nuestra IA no tendrá idea de cuál es la mejor, y absolutamente ninguna información previa sobre ninguna de sus tasas de conversión. Sin embargo, asumiremos que cada una de estas 9 estrategias tiene una tasa de conversión fija. Estas estrategias fueron elaboradas de forma cuidadosa e inteligente por el equipo de marketing, y cada una de ellas tiene el mismo objetivo: convertir a los clientes máximos en el plan premium. Sin embargo, estas 9 estrategias son todas diferentes. Tienen diferentes formas, diferentes paquetes, diferentes anuncios y diferentes ofertas especiales para convencer y persuadir a los clientes a suscribirse al plan premium. Por supuesto, el equipo de marketing no tiene idea de cuál de estas 9 estrategias es la mejor. Pero quieren resolverlo lo antes posible y ahorrando los costes máximos, cuál tiene la tasa de conversión más alta, porque saben cómo encontrar e implementar esa mejor estrategia puede maximizar significativamente los ingresos. Además, los expertos en marketing optan por no enviar un correo electrónico a sus 100 millones de clientes, ya que sería costoso y correrían el riesgo de enviar spam a demasiados clientes. En su lugar, buscarán sutilmente esa mejor estrategia a través del aprendizaje en línea. ¿Qué es el aprendizaje en línea? Consistirá en implementar una estrategia cada vez que un cliente navegue por el sitio web de negocios minoristas en línea para pasar el rato o comprar algunos productos. Luego, mientras el cliente navega por el sitio web, de repente recibirá un anuncio emergente, sugiriéndole que se suscriba al plan premium. Y para cada cliente que navega por el sitio web, solo se implementará una de las 9 estrategias. Luego, el usuario elegirá, o no, tomar medidas y suscribirse al plan premium. Si el cliente se suscribe, es un éxito, de lo contrario, es un fracaso. Cuantos más clientes hagan esto, más comentarios recibiremos y mejor podremos tener una idea de cuál es la mejor estrategia. Pero, por supuesto, no lo resolveremos manualmente, visualmente o con algunas matemáticas simples. En cambio, queremos implementar el algoritmo más inteligente que descubra cuál es la mejor estrategia en el menor tiempo posible. Y eso es por las mismas dos razones: primero porque implementar cada estrategia tiene un coste (por ejemplo, proveniente del anuncio emergente en la web), y segundo porque la compañía quiere molestar a los clientes lo menos posible con su anuncio.</p>
<p>Resumamos las diferencias en las características de estas 9 estrategias simplemente de esta manera:</p>
<p><img src="Images/Strategies_Slide.png" /></p>
<p><strong>Simulación</strong></p>
<p>Para simular este Caso Práctico, asumiremos que estas estrategias tienen las siguientes tasas de conversión:</p>
<p><img src="Images/Simulation_Slide.png" /></p>
<p>Sin embargo, asegúrate de comprender que en una situación de la vida real <strong>no tendríamos idea</strong> de cuáles serían estas tasas de conversión. Solo las conocemos aquí para fines de simulación, solo para que podamos verificar al final que nuestra IA logra descubrir la mejor estrategia, que según la tabla anterior, es la estrategia número 7 (la que tiene la tasa de conversión más alta).</p>
</div>
<div id="definición-del-entorno" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Definición del Entorno</h3>
<p>El aprendizaje en línea es una rama especial de la inteligencia artificial, donde no hay mucha necesidad de definir los estados y las acciones. Aquí, un estado sería simplemente un cliente específico en el que desplegaríamos una estrategia, y la acción sería simplemente la estrategia seleccionada. Luego lo verás más claro en el algoritmo de IA, donde no tenemos los estados como entradas y las acciones como salidas como en nuestros dos casos prácticos anteriores, porque esta vez no estamos haciendo Q-Learning o Deep Q-Learning. Aquí estamos haciendo aprendizaje en línea. Sin embargo, tenemos que definir las recompensas, ya que nuevamente tendremos que hacer una matriz de recompensas, donde cada fila corresponde a un usuario que está implementando una estrategia, y cada columna corresponde a una de las 9 estrategias. Por lo tanto, dado que realmente ejecutaremos este experimento de aprendizaje en línea en 10.000 clientes, esta matriz de recompensas tendrá 10.000 filas y 9 columnas. Luego, cada celda obtendrá un 0 si el cliente no se suscribe al plan premium después de ser abordado por la estrategia seleccionada, y un 1 si el cliente se suscribe después de ser abordado por la estrategia seleccionada. Y los valores en la celda son exactamente, las recompensas.</p>
<p>Ahora, una cosa muy importante para entender es que la matriz de recompensas solo está aquí para la simulación, y en la vida real no tendríamos una matriz de recompensas. Simplemente simularemos 10.000 clientes siendo abordados sucesivamente por una de las 9 estrategias, y gracias a la matriz de recompensas simularemos la decisión del cliente de suscribirse sí o no al plan premium. Si la celda correspondiente a un cliente específico y una estrategia seleccionada específica tiene un 1, eso simulará una conversión por parte del cliente al plan premium, y si la celda tiene un 0, simulará un rechazo. A continuación, como ejemplo, las primeras filas de una matriz de recompensas simulada:</p>
<p><img src="Images/Rewards_Matrix.png" /></p>
<p>De acuerdo con esta simulación, todo dado en la matriz de recompensas anterior:</p>
<ol style="list-style-type: decimal">
<li>El primer cliente (fila con índice 0) no se suscribirá al plan premium después de haber sido abordado por cualquier estrategia.</li>
<li>El segundo cliente (fila con índice 1) se suscribiría al plan premium después de ser abordado únicamente por la estrategia 5 o la estrategia 7.</li>
<li>El tercer cliente (fila con índice 2) no se suscribiría al plan premium después de haber sido abordado por cualquier estrategia.</li>
</ol>
<p>El muestreo de Thompson recopilará los comentarios de si cada uno de estos clientes se suscribe al plan premium uno tras otro y, gracias a su poderoso algoritmo, descubrirá rápidamente la estrategia con la tasa de conversión más alta, esa es la mejor. para ser implementado en los millones de clientes, maximizando así los ingresos de la compañía de esta nueva fuente de ingresos.</p>
</div>
</div>
<div id="solución-de-ia-1" class="section level2">
<h2><span class="header-section-number">3.2</span> Solución de IA</h2>
<p>La solución de IA que determinará la mejor estrategia se llama <em>muestreo de Thompson</em>. Es, con diferencia, el mejor modelo para ese tipo de problemas en esta rama de Aprendizaje en línea de Inteligencia Artificial. En resumen, cada vez que un nuevo cliente se conecta al sitio web de negocios minoristas en línea, esa es una nueva ronda <span class="math inline">\(n\)</span> y seleccionamos una de nuestras 9 estrategias para intentar una conversión (suscripción al plan premium). El objetivo es seleccionar la mejor estrategia en cada ronda, y entrenar durante muchas rondas. Así es como el muestreo de Thompson lo hará:</p>
<p><strong>Para cada ronda <span class="math inline">\(n\)</span>, repetimos durante más de 1000 iteraciones, los siguientes tres pasos</strong>:</p>
<ul>
<li><strong>Paso 1</strong> Para cada iteración <span class="math inline">\(i\)</span>, se elige un valor aleatorio que siga la distribución:</li>
</ul>
<p><span class="math display">\[\theta_i(n) \sim \beta(N_i^1(n)+1,N_i^0(n)+1)\]</span></p>
<p>donde:
* <span class="math inline">\(N_i^1(n)\)</span> es el número de veces que la estrategia <span class="math inline">\(i\)</span> ha recibido una recompensa igual a 1 en la ronda <span class="math inline">\(n\)</span>,
* <span class="math inline">\(N_i^0(n)\)</span> es el número de veces que la estrategia <span class="math inline">\(i\)</span> ha recibido una recompensa igual a 0 en la ronda <span class="math inline">\(n\)</span>.</p>
<ul>
<li><strong>Paso 2</strong> Seleccionamos la estrategia <span class="math inline">\(s(n)\)</span> que nos da el mayor valor<span class="math inline">\(\theta_i(n)\)</span>:</li>
</ul>
<p><span class="math display">\[s(n) = \underset{i\in\{1,...,9\}}{\textrm{argmax}}(\theta_i(n))\]</span></p>
<ul>
<li><strong>Paso 3</strong> Actualizamos <span class="math inline">\(N_{s(n)}^1(n)\)</span> y <span class="math inline">\(N_{s(n)}^0(n)\)</span> según las siguientes condiciones:
<ul>
<li>Si la estrategia selecxcionada <span class="math inline">\(s(n)\)</span> tiene una recompensa igual a 1:</li>
</ul></li>
</ul>
<p><span class="math display">\[N_{s(n)}^1(n) := N_{s(n)}^1(n) + 1\]</span></p>
<ul>
<li>Si la estrategia selecxcionada <span class="math inline">\(s(n)\)</span> tiene una recompensa igual a 0:</li>
</ul>
<p><span class="math display">\[N_{s(n)}^0(n) := N_{s(n)}^0(n) + 1\]</span></p>
<p><strong>Intuición.</strong> Cada estrategia tiene su propia distribución beta. A lo largo de las rondas, la distribución beta de la estrategia con la tasa de conversión más alta se desplazará progresivamente hacia la derecha, y las distribuciones beta de las estrategias con tasas de conversión más bajas se desplazarán progresivamente hacia la izquierda (Pasos 1 y 3). Por lo tanto, debido al Paso 2, la estrategia con la tasa de conversión más alta se seleccionará por probabilidad cada vez más. A continuación se muestra un gráfico que muestra tres distribuciones beta de tres estrategias, que te ayudarán a visualizar este hecho:</p>
<p><img src="Images/Beta_Distribution_Slide.png" /></p>
</div>
<div id="implementación-1" class="section level2">
<h2><span class="header-section-number">3.3</span> Implementación</h2>
<p>Vamos a ver a continuación la implementación completa del muestreo de Thompson para este caso práctico específico, siguiendo la misma simulación vista anteriormente.</p>
<p>Al implementar el muestreo de Thompson, también implementaremos el algoritmo de selección aleatoria, que simplemente seleccionará una estrategia aleatoria en cada ronda. Este será nuestro punto de referencia para evaluar el rendimiento de nuestro modelo de muestreo de Thompson. Por supuesto, el muestreo de Thompson y el algoritmo de selección aleatoria competirán en la misma simulación, es decir, utilizando la misma matriz de recompensas. Y al final, una vez realizada la simulación completa, evaluaremos el rendimiento de Thompson Sampling calculando el rendimiento relativo, definido por la siguiente fórmula:</p>
<p><span class="math display">\[\textrm{Rendimiento Rel.} = \frac{\textrm{(Rec. del m. de Thompson)} - (\textrm{Rec. de la sel. Aleatoria})}{\textrm{Recompensa de la s. Aleatoria}} \times 100\]</span></p>
<p>También representaremos el histograma de los anuncios seleccionados, solo para verificar que la estrategia con la tasa de conversión más alta (Estrategia 7) ha sido en efecto la más seleccionada.</p>
<p>Pues si estás listo, aquí vamos:</p>
<p>Primero, importamos las librerías necesarias y establecemos los parámetros (<span class="math inline">\(N = 10000\)</span> clientes y <span class="math inline">\(d = 9\)</span> estrategias):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># Inteligencia Artificial aplicada a Negocios y Empresas</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="co"># Maximizando los ingresos de un negocio minorista en línea con el muestreo de Thompson</span></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"></a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="co"># Importar las librerías</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="im">import</span> random</a>
<a class="sourceLine" id="cb1-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="co"># Configuración de parámetros</span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10">N <span class="op">=</span> <span class="dv">10000</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11">d <span class="op">=</span> <span class="dv">9</span></a></code></pre></div>
<p>Luego, creamos la simulación, construyendo la matriz de recompensas de 10000 filas correspondientes a los clientes, y 9 columnas correspondientes a las estrategias. En cada ronda y para cada estrategia, seleccionamos un número aleatorio entre 0 y 1, y si este número aleatorio es menor que la tasa de conversión de dicha estrategia, la recompensa será 1; de lo contrario, será 0. De esa manera simulamos las tasas de conversión enumeradas anteriormente para nuestras 9 estrategias:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co"># Creación de la simulación</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">conversion_rates <span class="op">=</span> [<span class="fl">0.05</span>,<span class="fl">0.13</span>,<span class="fl">0.09</span>,<span class="fl">0.16</span>,<span class="fl">0.11</span>,<span class="fl">0.04</span>,<span class="fl">0.20</span>,<span class="fl">0.08</span>,<span class="fl">0.01</span>]</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">X <span class="op">=</span> np.array(np.zeros([N,d]))</a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(d):</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">        <span class="cf">if</span> np.random.rand() <span class="op">&lt;=</span> conversion_rates[j]:</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">            X[i,j] <span class="op">=</span> <span class="dv">1</span></a></code></pre></div>
<p>Luego, recorreremos las 10000 filas (o rondas) de esta matriz de recompensas, y en cada ronda obtendremos dos selecciones de estrategia separadas: una del algoritmo de Selección aleatoria y otra del muestreo de Thompson. Llevamos un registro de las estrategias seleccionadas por cada uno de estos dos algoritmos, y calculamos la recompensa total acumulada durante las rondas por cada uno de ellos. El muestreo de Thompson se implementa siguiendo exactamente los pasos 1, 2 y 3 proporcionados anteriormente:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># Implementación de la selección aleatoria y del muestreo de Thompson</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">strategies_selected_rs <span class="op">=</span> []</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">strategies_selected_ts <span class="op">=</span> []</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">total_reward_rs <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">total_reward_ts <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">numbers_of_rewards_1 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">numbers_of_rewards_0 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb3-8" data-line-number="8"><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N):</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">    <span class="co"># Selección aleatoria</span></a>
<a class="sourceLine" id="cb3-10" data-line-number="10">    strategy_rs <span class="op">=</span> random.randrange(d)</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">    strategies_selected_rs.append(strategy_rs)</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">    reward_rs <span class="op">=</span> X[n, strategy_rs]</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">    total_reward_rs <span class="op">=</span> total_reward_rs <span class="op">+</span> reward_rs</a>
<a class="sourceLine" id="cb3-14" data-line-number="14">    <span class="co"># Muestreo de Thompson</span></a>
<a class="sourceLine" id="cb3-15" data-line-number="15">    strategy_ts <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb3-16" data-line-number="16">    max_random <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb3-17" data-line-number="17">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, d):</a>
<a class="sourceLine" id="cb3-18" data-line-number="18">        random_beta <span class="op">=</span> random.betavariate(numbers_of_rewards_1[i] <span class="op">+</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb3-19" data-line-number="19">                                         numbers_of_rewards_0[i] <span class="op">+</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb3-20" data-line-number="20">        <span class="cf">if</span> random_beta <span class="op">&gt;</span> max_random:</a>
<a class="sourceLine" id="cb3-21" data-line-number="21">            max_random <span class="op">=</span> random_beta</a>
<a class="sourceLine" id="cb3-22" data-line-number="22">            strategy_ts <span class="op">=</span> i</a>
<a class="sourceLine" id="cb3-23" data-line-number="23">    reward_ts <span class="op">=</span> X[n, strategy_ts]</a>
<a class="sourceLine" id="cb3-24" data-line-number="24">    <span class="cf">if</span> reward_ts <span class="op">==</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb3-25" data-line-number="25">        numbers_of_rewards_1[strategy_ts] <span class="op">=</span> numbers_of_rewards_1[strategy_ts] <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb3-26" data-line-number="26">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb3-27" data-line-number="27">        numbers_of_rewards_0[strategy_ts] <span class="op">=</span> numbers_of_rewards_0[strategy_ts] <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb3-28" data-line-number="28">    strategies_selected_ts.append(strategy_ts)</a>
<a class="sourceLine" id="cb3-29" data-line-number="29">    total_reward_ts <span class="op">=</span> total_reward_ts <span class="op">+</span> reward_ts</a></code></pre></div>
<p>Luego calculamos resultado final, que es el rendimiento relativo del muestreo de Thompson con respecto a nuestro punto de referencia que es la selección aleatoria:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># Cálculo del rendimiento relativo</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">relative_return <span class="op">=</span> (total_reward_ts <span class="op">-</span> total_reward_rs) <span class="op">/</span> total_reward_rs <span class="op">*</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="bu">print</span>(<span class="st">&quot;Rendimiento Relativo: </span><span class="sc">{:.0f}</span><span class="st"> %&quot;</span>.<span class="bu">format</span>(relative_return))</a></code></pre></div>
<p>Y prepárate, porque al ejecutar este código obtenemos un retorno relativo final, de …:</p>
<pre><code>## Rendimiento Relativo: 106 %</code></pre>
<p>En otras palabras, el muestreo de Thompson casi duplicó el rendimiento de nuestro punto de referencia de la selección aleatoria.</p>
<p>Y finalmente, representemos el histograma de las estrategias seleccionadas, para verificar que efectivamente la Estrategia 7 (la de índice 6) fue la más seleccionada por el algoritmo, ya que es la que tiene la tasa de conversión más alta:¡</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co"># Representación del histograma de selecciones</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2">plt.hist(strategies_selected_ts)</a></code></pre></div>
<pre><code>## (array([   54.,   318.,   125.,   408.,     0.,   163.,    54.,  8740.,
##          102.,    36.]), array([ 0. ,  0.8,  1.6,  2.4,  3.2,  4. ,  4.8,  5.6,  6.4,  7.2,  8. ]), &lt;a list of 10 Patch objects&gt;)</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1">plt.title(<span class="st">&#39;Histograma de Selecciones&#39;</span>)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">plt.xlabel(<span class="st">&#39;Estrategia&#39;</span>)</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">plt.ylabel(<span class="st">&#39;Numero de veces que se ha seleccionado la estrategia&#39;</span>)</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">plt.show()</a></code></pre></div>
<p><img src="curso-ia-business-udemy_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Al ejecutar este código final, obtenemos el siguiente histograma:</p>
<p><img src="Images/Histogram.png" /></p>
<p>Y de hecho, es la estrategia del índice 6, es decir, la Estrategia 7, la que fue, con diferencia, la más seleccionada. El muestreo de Thompson ha sido capaz de identificarlo rápidamente. Y, de hecho, si volvemos a ejecutar el mismo código pero con solo 1000 clientes, nos damos cuenta de que el muestreo de Thompson todavía puede identificar la Estrategia 7 como la mejor, con muchas menos pruebas.</p>
<p>En consecuencia, el muestreo de Thompson seguramente ha hecho un trabajo increíble para este negocio minorista en línea. Porque no solo ha sido capaz de identificar la mejor estrategia rápidamente en un número reducido de rondas, es decir, con solamente algunos clientes, lo que nos ha ahorrado mucho en publicidad y costes operativos. Pero también, por supuesto, ha sido capaz de descubrir claramente la estrategia con la tasa de conversión más alta. Y, de hecho, si este negocio minorista en línea tiene 100 millones de clientes, y si el plan premium tiene un precio de 100 dólares al año, la implementación de esta mejor estrategia que tiene una tasa de conversión del 20% conduciría a generar un ingreso adicional de …:</p>
<p><span class="math display">\[\textrm{Ingresos extra generados} = 100000000 \times 0.2 \times 100 = \textrm{2 mil millones de }\$!!\]</span></p>
<p>En otras palabras, muestreo de Thompson maximizó clara y rápidamente los ingresos de este negocio minorista en línea, al mismo tiempo que ahorró mucho en los costos, maximizando así la rentabilidad del negocio.</p>
<p><strong>Curva del arrepentimiento.</strong></p>
<p>La curva de arrepentimiento de un modelo (con estrategia aleatoria o con el muestreo de Sampling) es la representación gráfica de la diferencia entre la mejor estrategia y el modelo desplegado, con respecto a las rondas.</p>
<p>La mejor estrategia se calcula simplemente obteniendo, en cada ronda, el máximo de las recompensas acumuladas sobre todas las diferentes estrategias. Por lo tanto, en nuestra implementación, obtendremos la mejor estrategia de la siguiente manera:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">rewards_strategies <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb9-2" data-line-number="2"><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N):</a>
<a class="sourceLine" id="cb9-3" data-line-number="3">    <span class="co"># La mejor estrategia</span></a>
<a class="sourceLine" id="cb9-4" data-line-number="4">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, d):</a>
<a class="sourceLine" id="cb9-5" data-line-number="5">        rewards_strategies[i] <span class="op">=</span> rewards_strategies[i] <span class="op">+</span> X[n, i]</a>
<a class="sourceLine" id="cb9-6" data-line-number="6">    total_reward_bs <span class="op">=</span> <span class="bu">max</span>(rewards_strategies)</a></code></pre></div>
<p>Entonces, el arrepentimiento del muestreo de Thompson se calcula simplemente como la diferencia entre la mejor estrategia y el modelo del muestreo de Thompson:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co"># Arrepentimiento del muestreo Thompson</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2">strategies_selected_ts <span class="op">=</span> []</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">total_reward_ts <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb10-4" data-line-number="4">total_reward_bs <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb10-5" data-line-number="5">numbers_of_rewards_1 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb10-6" data-line-number="6">numbers_of_rewards_0 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb10-7" data-line-number="7">rewards_strategies <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">regret <span class="op">=</span> []</a>
<a class="sourceLine" id="cb10-9" data-line-number="9"><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N):</a>
<a class="sourceLine" id="cb10-10" data-line-number="10">    <span class="co"># Muestreo de Thompson</span></a>
<a class="sourceLine" id="cb10-11" data-line-number="11">    strategy_ts <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb10-12" data-line-number="12">    max_random <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb10-13" data-line-number="13">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, d):</a>
<a class="sourceLine" id="cb10-14" data-line-number="14">        random_beta <span class="op">=</span> random.betavariate(numbers_of_rewards_1[i] <span class="op">+</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb10-15" data-line-number="15">                                         numbers_of_rewards_0[i] <span class="op">+</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb10-16" data-line-number="16">        <span class="cf">if</span> random_beta <span class="op">&gt;</span> max_random:</a>
<a class="sourceLine" id="cb10-17" data-line-number="17">            max_random <span class="op">=</span> random_beta</a>
<a class="sourceLine" id="cb10-18" data-line-number="18">            strategy_ts <span class="op">=</span> i</a>
<a class="sourceLine" id="cb10-19" data-line-number="19">    reward_ts <span class="op">=</span> X[n, strategy_ts]</a>
<a class="sourceLine" id="cb10-20" data-line-number="20">    <span class="cf">if</span> reward_ts <span class="op">==</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb10-21" data-line-number="21">        numbers_of_rewards_1[strategy_ts] <span class="op">=</span> numbers_of_rewards_1[strategy_ts] <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb10-22" data-line-number="22">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb10-23" data-line-number="23">        numbers_of_rewards_0[strategy_ts] <span class="op">=</span> numbers_of_rewards_0[strategy_ts] <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb10-24" data-line-number="24">    strategies_selected_ts.append(strategy_ts)</a>
<a class="sourceLine" id="cb10-25" data-line-number="25">    total_reward_ts <span class="op">=</span> total_reward_ts <span class="op">+</span> reward_ts</a>
<a class="sourceLine" id="cb10-26" data-line-number="26">    <span class="co"># La mejor estrategia</span></a>
<a class="sourceLine" id="cb10-27" data-line-number="27">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, d):</a>
<a class="sourceLine" id="cb10-28" data-line-number="28">        rewards_strategies[i] <span class="op">=</span> rewards_strategies[i] <span class="op">+</span> X[n, i]</a>
<a class="sourceLine" id="cb10-29" data-line-number="29">    total_reward_bs <span class="op">=</span> <span class="bu">max</span>(rewards_strategies)</a>
<a class="sourceLine" id="cb10-30" data-line-number="30">    <span class="co"># Arrepentimiento</span></a>
<a class="sourceLine" id="cb10-31" data-line-number="31">    regret.append(total_reward_bs <span class="op">-</span> total_reward_ts)</a></code></pre></div>
<p>Y lo mismo, el arrepentimiento de la estrategia aleatoria simplemente se calcula como la diferencia entre la mejor estrategia y el algoritmo de selección aleatoria:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="co"># Arrepentimiento de la estrategia aleatoria</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2">strategies_selected_rs <span class="op">=</span> []</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">total_reward_rs <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb11-4" data-line-number="4">total_reward_bs <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb11-5" data-line-number="5">numbers_of_rewards_1 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb11-6" data-line-number="6">numbers_of_rewards_0 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb11-7" data-line-number="7">rewards_strategies <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb11-8" data-line-number="8">regret <span class="op">=</span> []</a>
<a class="sourceLine" id="cb11-9" data-line-number="9"><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N):</a>
<a class="sourceLine" id="cb11-10" data-line-number="10">    <span class="co"># Estrategia aleatoria</span></a>
<a class="sourceLine" id="cb11-11" data-line-number="11">    strategy_rs <span class="op">=</span> random.randrange(d)</a>
<a class="sourceLine" id="cb11-12" data-line-number="12">    strategies_selected_rs.append(strategy_rs)</a>
<a class="sourceLine" id="cb11-13" data-line-number="13">    reward_rs <span class="op">=</span> X[n, strategy_rs]</a>
<a class="sourceLine" id="cb11-14" data-line-number="14">    total_reward_rs <span class="op">=</span> total_reward_rs <span class="op">+</span> reward_rs</a>
<a class="sourceLine" id="cb11-15" data-line-number="15">    <span class="co"># La mejor estrategia</span></a>
<a class="sourceLine" id="cb11-16" data-line-number="16">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, d):</a>
<a class="sourceLine" id="cb11-17" data-line-number="17">        rewards_strategies[i] <span class="op">=</span> rewards_strategies[i] <span class="op">+</span> X[n, i]</a>
<a class="sourceLine" id="cb11-18" data-line-number="18">    total_reward_bs <span class="op">=</span> <span class="bu">max</span>(rewards_strategies)</a>
<a class="sourceLine" id="cb11-19" data-line-number="19">    <span class="co"># Arrepentimiento</span></a>
<a class="sourceLine" id="cb11-20" data-line-number="20">    regret.append(total_reward_bs <span class="op">-</span> total_reward_rs)</a></code></pre></div>
<p>Y finalmente, por supuesto, representamos el arrepentimiento sobre las rondas con este simple código (no tenemos que especificar las coordenadas x en la función <code>plt.plot()</code> porque las rondas ya son índices desde 0 hasta N):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co"># Representación de la curva de arrepentimiento</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2">plt.plot(regret)</a>
<a class="sourceLine" id="cb12-3" data-line-number="3">plt.title(<span class="st">&#39;Curva de Arrepentimiento&#39;</span>)</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">plt.xlabel(<span class="st">&#39;Ronda&#39;</span>)</a>
<a class="sourceLine" id="cb12-5" data-line-number="5">plt.ylabel(<span class="st">&#39;Arrepentimiento&#39;</span>)</a>
<a class="sourceLine" id="cb12-6" data-line-number="6">plt.show()</a></code></pre></div>
<p>Si representamos la curva de arrepentimiento de la estrategia aleatoria, obtenemos lo siguiente:</p>
<p><img src="curso-ia-business-udemy_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Y, por supuesto, no observamos absolutamente ninguna convergencia de la estrategia aleatoria hacia la mejor estrategia.</p>
<p>Sin embargo, si ahora representamos la curva de arrepentimiento del modelo de muestreo de Thompson, obtenemos la siguiente curva hermosa:</p>
<p><img src="curso-ia-business-udemy_files/figure-html/unnamed-chunk-13-1.png" width="672" />
Y obviamente, el muestreo de Thompson está convergiendo muy bien hacia la mejor estrategia.</p>
<p>Finalmente, aquí está el código final que incluye esa Curva de arrepentimiento del muestreo de Thompson:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># Muestreo de Thompson</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2"></a>
<a class="sourceLine" id="cb13-3" data-line-number="3"><span class="co"># Importar las librerías</span></a>
<a class="sourceLine" id="cb13-4" data-line-number="4"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb13-5" data-line-number="5"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb13-6" data-line-number="6"><span class="im">import</span> random</a>
<a class="sourceLine" id="cb13-7" data-line-number="7"></a>
<a class="sourceLine" id="cb13-8" data-line-number="8"><span class="co"># Configuración de los parámetros</span></a>
<a class="sourceLine" id="cb13-9" data-line-number="9">N <span class="op">=</span> <span class="dv">10000</span></a>
<a class="sourceLine" id="cb13-10" data-line-number="10">d <span class="op">=</span> <span class="dv">9</span></a>
<a class="sourceLine" id="cb13-11" data-line-number="11"></a>
<a class="sourceLine" id="cb13-12" data-line-number="12"><span class="co"># Creación de la simulación</span></a>
<a class="sourceLine" id="cb13-13" data-line-number="13">conversion_rates <span class="op">=</span> [<span class="fl">0.05</span>,<span class="fl">0.13</span>,<span class="fl">0.09</span>,<span class="fl">0.16</span>,<span class="fl">0.11</span>,<span class="fl">0.04</span>,<span class="fl">0.20</span>,<span class="fl">0.08</span>,<span class="fl">0.01</span>]</a>
<a class="sourceLine" id="cb13-14" data-line-number="14">X <span class="op">=</span> np.array(np.zeros([N,d]))</a>
<a class="sourceLine" id="cb13-15" data-line-number="15"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</a>
<a class="sourceLine" id="cb13-16" data-line-number="16">    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(d):</a>
<a class="sourceLine" id="cb13-17" data-line-number="17">        <span class="cf">if</span> np.random.rand() <span class="op">&lt;=</span> conversion_rates[j]:</a>
<a class="sourceLine" id="cb13-18" data-line-number="18">            X[i,j] <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb13-19" data-line-number="19"></a>
<a class="sourceLine" id="cb13-20" data-line-number="20"><span class="co"># Implementación de la estrategia aleatoria y del muestreo de Thompson con la curva de arrepentimiento</span></a>
<a class="sourceLine" id="cb13-21" data-line-number="21">strategies_selected_rs <span class="op">=</span> []</a>
<a class="sourceLine" id="cb13-22" data-line-number="22">strategies_selected_ts <span class="op">=</span> []</a>
<a class="sourceLine" id="cb13-23" data-line-number="23">total_reward_rs <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb13-24" data-line-number="24">total_reward_ts <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb13-25" data-line-number="25">total_reward_bs <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb13-26" data-line-number="26">numbers_of_rewards_1 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb13-27" data-line-number="27">numbers_of_rewards_0 <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb13-28" data-line-number="28">rewards_strategies <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</a>
<a class="sourceLine" id="cb13-29" data-line-number="29">regret <span class="op">=</span> []</a>
<a class="sourceLine" id="cb13-30" data-line-number="30"><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N):</a>
<a class="sourceLine" id="cb13-31" data-line-number="31">    <span class="co"># Estrategia aleatoria</span></a>
<a class="sourceLine" id="cb13-32" data-line-number="32">    strategy_rs <span class="op">=</span> random.randrange(d)</a>
<a class="sourceLine" id="cb13-33" data-line-number="33">    strategies_selected_rs.append(strategy_rs)</a>
<a class="sourceLine" id="cb13-34" data-line-number="34">    reward_rs <span class="op">=</span> X[n, strategy_rs]</a>
<a class="sourceLine" id="cb13-35" data-line-number="35">    total_reward_rs <span class="op">=</span> total_reward_rs <span class="op">+</span> reward_rs</a>
<a class="sourceLine" id="cb13-36" data-line-number="36">    <span class="co"># Muestreo de Thompson</span></a>
<a class="sourceLine" id="cb13-37" data-line-number="37">    strategy_ts <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb13-38" data-line-number="38">    max_random <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb13-39" data-line-number="39">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, d):</a>
<a class="sourceLine" id="cb13-40" data-line-number="40">        random_beta <span class="op">=</span> random.betavariate(numbers_of_rewards_1[i] <span class="op">+</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb13-41" data-line-number="41">                                         numbers_of_rewards_0[i] <span class="op">+</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-42" data-line-number="42">        <span class="cf">if</span> random_beta <span class="op">&gt;</span> max_random:</a>
<a class="sourceLine" id="cb13-43" data-line-number="43">            max_random <span class="op">=</span> random_beta</a>
<a class="sourceLine" id="cb13-44" data-line-number="44">            strategy_ts <span class="op">=</span> i</a>
<a class="sourceLine" id="cb13-45" data-line-number="45">    reward_ts <span class="op">=</span> X[n, strategy_ts]</a>
<a class="sourceLine" id="cb13-46" data-line-number="46">    <span class="cf">if</span> reward_ts <span class="op">==</span> <span class="dv">1</span>:</a>
<a class="sourceLine" id="cb13-47" data-line-number="47">        numbers_of_rewards_1[strategy_ts] <span class="op">=</span> numbers_of_rewards_1[strategy_ts] <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb13-48" data-line-number="48">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb13-49" data-line-number="49">        numbers_of_rewards_0[strategy_ts] <span class="op">=</span> numbers_of_rewards_0[strategy_ts] <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb13-50" data-line-number="50">    strategies_selected_ts.append(strategy_ts)</a>
<a class="sourceLine" id="cb13-51" data-line-number="51">    total_reward_ts <span class="op">=</span> total_reward_ts <span class="op">+</span> reward_ts</a>
<a class="sourceLine" id="cb13-52" data-line-number="52">    <span class="co"># La mejor estrategia</span></a>
<a class="sourceLine" id="cb13-53" data-line-number="53">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, d):</a>
<a class="sourceLine" id="cb13-54" data-line-number="54">        rewards_strategies[i] <span class="op">=</span> rewards_strategies[i] <span class="op">+</span> X[n, i]</a>
<a class="sourceLine" id="cb13-55" data-line-number="55">    total_reward_bs <span class="op">=</span> <span class="bu">max</span>(rewards_strategies)</a>
<a class="sourceLine" id="cb13-56" data-line-number="56">    <span class="co"># Arrepentimiento</span></a>
<a class="sourceLine" id="cb13-57" data-line-number="57">    regret.append(total_reward_bs <span class="op">-</span> total_reward_ts)</a>
<a class="sourceLine" id="cb13-58" data-line-number="58"></a>
<a class="sourceLine" id="cb13-59" data-line-number="59"><span class="co"># Calcular el rendimiento absoluto y relativo</span></a>
<a class="sourceLine" id="cb13-60" data-line-number="60">absolute_return <span class="op">=</span> total_reward_ts <span class="op">-</span> total_reward_rs</a>
<a class="sourceLine" id="cb13-61" data-line-number="61">relative_return <span class="op">=</span> (total_reward_ts <span class="op">-</span> total_reward_rs) <span class="op">/</span> total_reward_rs <span class="op">*</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb13-62" data-line-number="62"><span class="bu">print</span>(<span class="st">&quot;Rendimiento Absoluto: </span><span class="sc">{:.0f}</span><span class="st"> $&quot;</span>.<span class="bu">format</span>(absolute_return))</a>
<a class="sourceLine" id="cb13-63" data-line-number="63"><span class="bu">print</span>(<span class="st">&quot;Rendimiento Relativo: </span><span class="sc">{:.0f}</span><span class="st"> %&quot;</span>.<span class="bu">format</span>(relative_return))</a>
<a class="sourceLine" id="cb13-64" data-line-number="64"></a>
<a class="sourceLine" id="cb13-65" data-line-number="65"><span class="co"># Representación de los histogramas de selecciones</span></a>
<a class="sourceLine" id="cb13-66" data-line-number="66">plt.hist(strategies_selected_ts)</a>
<a class="sourceLine" id="cb13-67" data-line-number="67">plt.title(<span class="st">&#39;Histograma de Selecciones&#39;</span>)</a>
<a class="sourceLine" id="cb13-68" data-line-number="68">plt.xlabel(<span class="st">&#39;Estrategia&#39;</span>)</a>
<a class="sourceLine" id="cb13-69" data-line-number="69">plt.ylabel(<span class="st">&#39;Numero de veces que la estrategia ha sido seleccionada&#39;</span>)</a>
<a class="sourceLine" id="cb13-70" data-line-number="70">plt.show()</a>
<a class="sourceLine" id="cb13-71" data-line-number="71">plt.close()</a>
<a class="sourceLine" id="cb13-72" data-line-number="72"></a>
<a class="sourceLine" id="cb13-73" data-line-number="73"><span class="co"># Representación de la curva de arrepentimiento</span></a>
<a class="sourceLine" id="cb13-74" data-line-number="74">plt.plot(regret)</a>
<a class="sourceLine" id="cb13-75" data-line-number="75">plt.title(<span class="st">&#39;Curva de Arrepentimiento&#39;</span>)</a>
<a class="sourceLine" id="cb13-76" data-line-number="76">plt.xlabel(<span class="st">&#39;Ronda&#39;</span>)</a>
<a class="sourceLine" id="cb13-77" data-line-number="77">plt.ylabel(<span class="st">&#39;Arrepentimiento&#39;</span>)</a>
<a class="sourceLine" id="cb13-78" data-line-number="78">plt.show()</a></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parte-2-minimización-de-costes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusión.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/joanby/ia4business/edit/master/3.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["curso-ia-business-udemy.pdf", "curso-ia-business-udemy.epub"],
"toc": {
"collapse": "subsection"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
