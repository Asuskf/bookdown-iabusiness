# Part 2 - Minimizing Costs

Congratulations for smashing the first case study! Let's move on to a brand new and more advanced AI.

## Case Study: Minimizing Costs in Energy Consumption of a Data Center

### Problem to solve

In 2016, \href{https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/}{DeepMind AI minimized a big part of Google's cost by reducing Google Data Centre Cooling Bill by 40\%} using their DQN AI model (Deep Q-Learning). In this case study, we will do something very similar. We will set up our own server environment, and we will build an AI that will be controlling the cooling/heating of the server so that it stays in an optimal range of temperatures while saving the maximum energy, therefore minimizing the costs. And just as DeepMind AI did, our goal will be to achieve at least 40\% energy saving.

\subsubsection{Environment to define}

Before we define the states, actions and rewards, we need to explain how the server operates. We will do that in several steps. First, we will list all the environment parameters and variables by which the server is controlled. After that we will set the essential assumption of the problem, on which our AI will rely to provide a solution. Then we will specify how we will simulate the whole process. And eventually we will explain the overall functioning of the server, and how the AI plays its role.

\

\textbf{Parameters:}

\begin{itemize}
    \item the average atmospheric temperature over a month
    \item the optimal range of temperatures of the server, which will be $[18 \degree \textrm{C}, 24 \degree \textrm{C}]$
    \item the minimum temperature of the server below which it fails to operate, which will be $-20 \degree \textrm{C}$
    \item the maximum temperature of the server above which it fails to operate, which will be $80 \degree \textrm{C}$
    \item the minimum number of users in the server, which will be 10
    \item the maximum number of users in the server, which will be 100
    \item the maximum number of users in the server that can go up or down per minute, which will be 5
    \item the minimum rate of data transmission in the server, which will be 20
    \item the maximum rate of data transmission in the server, which will be 300
    \item the maximum rate of data transmission that can go up or down per minute, which will be 10
\end{itemize}

\textbf{Variables:}

\begin{itemize}
    \item the temperature of the server at any minute
    \item the number of users in the server at any minute
    \item the rate of data transmission at any minute
    \item the energy spent by the AI onto the server (to cool it down or heat it up) at any minute
    \item the energy spent by the server's integrated cooling system that automatically brings the server's temperature back to the optimal range whenever the server's temperature goes outside this optimal range
\end{itemize}

All these parameters and variables will be part of our server environment and will influence the actions of the AI on the server.

\

Then let's give and explain below the two core assumptions of the environment. It is important to understand that these assumptions are not AI related, but just used to simplify the environment so that we can focus the maximum on the AI solution.

\

\textbf{Assumptions:}

\

We will rely on the following two essential assumptions:

\

\textbf{Assumption 1: The temperature of the server can be approximated through Multiple Linear Regression, by a linear function of the atmospheric temperature, the number of users and the rate of data transmission}:

\begin{equation*}
    \textrm{server temperature} = b_0 + b_1 \times \textrm{atmospheric temperature} + b_2 \times \textrm{number of users} + b_3 \times \textrm{data transmission rate} 
\end{equation*}

where $b_0 \in \mathbb{R}$, $b_1>0$, $b_2>0$ and $b_3>0$.

\

The raison d'Ãªtre of this assumption and the reason why $b_1>0$, $b_2>0$ and $b_3>0$ are intuitive to understand. Indeed, it makes sense that when the atmospheric temperature increases, the temperature of the server increases. Also, the more users are active in the server, the more the server has to spend energy to handle them and therefore the higher the temperature of the server will be. And finally of course, the more data is transmitted inside the server, the more the server has to spend energy to process it, and therefore the higher temperature of the server will be. And for simplicity purposes, we just suppose that these correlations are linear. However you could totally run the same simulation by assuming they are quadratic or logarithmic. Feel free to tweak around.

\

Eventually, let's assume further that after performing this Multiple Linear Regression, we obtained the following values of the coefficients: $b_0 = 0$, $b_1 = 1$, $b_2 = 1.25$ and $b_3 = 1.25$. Accordingly:

\begin{equation*}
    \textrm{server temperature} = \textrm{atmospheric temperature} + 1.25 \times \textrm{number of users} + 1.25 \times \textrm{data transmission rate}
\end{equation*}

\

\textbf{Assumption 2: The energy spent by a system (our AI or the server's integrated cooling system) that changes the server's temperature from $T_t$ to $T_{t+1}$ within 1 unit of time (here 1 minute), can be approximated again through regression by a linear function of the server's absolute temperature change:}

\begin{equation*}
    E_t = \alpha |\Delta T_t| + \beta = \alpha |T_{t+1} - T_t| + \beta
\end{equation*}

where:

\begin{equation*}
\begin{cases}
\textrm{$E_t$ is the energy spent by the system onto the server between times $t$ and $t$ + 1 minute} \\
\textrm{$\Delta T_t$ is the change of the server's temperature caused by the system between times $t$ and $t$ + 1 minute} \\
\textrm{$T_t$ is the temperature of the server at time $t$} \\
\textrm{$T_{t+1}$ is the temperature of the server at time $t$ + 1 minute} \\
\alpha > 0 \\
\beta \in \mathbb{R}
\end{cases}
\end{equation*}

Again, let's explain why it intuitively makes sense to make this assumption with $\alpha > 0$. That's simply because the more the AI heats up or cools down the server, the more it spends energy to do that heat transfer. Indeed for example, imagine the server suddenly has overheating issues and just reached $80 \degree$C, then within one unit of time (1 minute) the AI will need much more energy to bring the server's temperature back to its optimal temperature $24 \degree$C than to bring it back to $50 \degree$C for example. And again for simplicity purposes, we just suppose that these correlations are linear. Also (in case you are wondering), why do we take the absolute value? That's simply because when the AI cools down the server, $T_{t+1} < T_t$, so $\Delta T < 0$. And of course an energy is always positive so we have to take the absolute value of $\Delta T$.

\

Eventually, for further simplicity purposes we will also assume that the results of the regression are $\alpha = 1$ and $\beta = 0$, so that we get, the following final equation based on Assumption 2:

\begin{equation*}
E_t = |\Delta T_t| = |T_{t+1} - T_t| =
\begin{cases}
T_{t+1} - T_t & \textrm{if $T_{t+1} > T_t$, that is if the server is heated up} \\
T_t - T_{t+1} & \textrm{if $T_{t+1} < T_t$, that is if the server is cooled down}
\end{cases}
\end{equation*}

\

Now let's explain how we will simulate the server operating with the users and data coming in and out.

\

\textbf{Simulation:}

\

The number of users and the rate of data transmission will be randomly fluctuating to simulate an actual server. This leads to randomness in the temperature and the AI has to understand how much cooling or heating power it has to transfer to the server so as to not deteriorate the server performance and at the same time, expend the least energy by optimizing its heat transfer.

\

Now that we have the full picture, let's explain the overall functioning of the server and the AI inside this environment.

\

\textbf{Overall functioning:}

\

Inside a data center, we are dealing with a specific server that is controlled by the parameters and variables listed above. Every minute, some new users log on to the server and some current users log off, therefore updating the number of active users in the server. Same, every minute some new data is transmitted into the server, and some existing data is transmitted outside the server, therefore updating the rate of data transmission happening inside the server. Hence, based on Assumption 1 given above, the temperature of the server is updated every minute. Now please focus, because this is where you will understand the huge role the AI has to play on the server. Two possible systems can regulate the temperature of the server: the AI, or the server's integrated cooling system. The server's integrated cooling system is an unintelligent system that will automatically bring back the server's temperature to its optimal temperature. Let's explain this in more details: when the server's temperature is updated every minute, it can either stay within the range of optimal temperatures ($[18 \degree \textrm{C}, 24 \degree \textrm{C}]$), or go outside this range. If it goes outside the optimal range, like say $30 \degree$C, the server's integrated cooling system will automatically bring the temperature back to the closest bound of the optimal range, that is $24 \degree$C. However this server's integrated cooling system will do that only when the AI is not activated. If the AI is activated, then in that case the server's integrated cooling system is deactivated and it is the AI itself that updates the temperature of the server to regulate it the best way. But the AI does that after some prior predictions, not in a deterministic way as with the unintelligent server's integrated cooling system. Before there is an update of the number of users and the rate of data transmission causing to change the temperature of the server, the AI predicts if it should cool down the server, do nothing, or heat up the server. Then the temperature change happens and the AI reiterates. And since these two systems are complementary, we will evaluate them separately to compare their performance.

And that brings us to the energy. Indeed remember that one primary goal of the AI is to save some energy spent on this server. Accordingly, our AI has to spend less energy than the energy spent by the unintelligent cooling system onto the server. And since, based on Assumption 2 given above, the energy spent on the server (by any system) is proportional to the change of temperature within one unit of time:

\begin{equation*}
E_t = |\Delta T_t| = \alpha |T_{t+1} - T_t| =
\begin{cases}
T_{t+1} - T_t & \textrm{if $T_{t+1} > T_t$, that is if the server is heated up} \\
T_t - T_{t+1} & \textrm{if $T_{t+1} < T_t$, that is if the server is cooled down}
\end{cases}
\end{equation*}

\

then that means that the energy saved by the AI at each iteration $t$ (each minute) is in fact the difference in absolute changes of temperatures caused on the server between the unintelligent server's integrated cooling system and the AI from $t$ and $t+1$:

\begin{align*}
        \textrm{Energy saved by the AI between $t$ and $t+1$}
        & = |\Delta T_t^{\textrm{Server's Integrated Cooling System}}| - |\Delta T_t^{\textrm{AI}}| \\
        & = |\Delta T_t^{\textrm{noAI}}| - |\Delta T_t^{\textrm{AI}}|
\end{align*}

where:

\begin{equation*}
\begin{cases}
\textrm{$\Delta T_t^{\textrm{noAI}}$ is the change of temperature that the server's integrated cooling system would cause} \\
\textrm{without the AI onto the server during the iteration $t$, that is from $t$ to $t+1$ minute} \\
\textrm{$\Delta T_t^{\textrm{AI}}$ is the change of temperature caused by the AI onto the server during the iteration $t$,} \\
\textrm{that is from $t$ to $t+1$ minute}
\end{cases}
\end{equation*}

\

Our goal will be to save the maximum energy each minute, therefore saving the maximum total energy over 1 full year of simulation, and eventually saving the maximum costs in the cooling/heating electricity bill.

\

Are you ready?

\

Great! Now that we fully understand how our server environment works and how it is simulated, it is time to proceed with what must be absolutely done when defining an AI environment:

\

\begin{itemize}
    \item Defining the states
    \item Defining the actions
    \item Defining the rewards
\end{itemize}

\

\textbf{Defining the states.}

\

The input state $s_t$ at time $t$ is composed of the following three elements:

\begin{enumerate}
    \item The temperature of the server at time $t$.
    \item The number of users in the server at time $t$.
    \item The rate of data transmission in the server at time $t$.
\end{enumerate}

Thus the input state will be an input vector of these three elements. Our future AI will take this vector as input, and will return the action to play at each time $t$.

\

\textbf{Defining the actions.}

\

The actions are simply the temperature changes that the AI can cause inside the server, in order to heat it up or cool it down. In order to make our actions discrete, we will consider 5 possible temperature changes from $-3 \degree$C to $+3 \degree$C, so that we end up with the 5 following possible actions that the AI can play to regulate the temperature of the server:

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{c|l}
      \textbf{Action} & \textbf{What it does} \\
      \hline
      0 & The AI cools down the server by $3 \degree$C \\
      1 & The AI cools down the server by $1.5 \degree$C \\
      2 & The AI does not transfer any heat to the server (no temperature change) \\
      3 & The AI heats up the server by $1.5 \degree$C \\
      4 & The AI heats up the server by $3 \degree$C \\
    \end{tabular}
  \end{center}
\end{table}

\textbf{Defining the rewards.}

\

After reading the "Overall functioning" paragraph above you might guess what the reward is going to be. Of course, the reward at iteration $t$ is the energy spent on the server that the AI is saving with respect to the server's integrated cooling system, that is, the difference between the energy that the unintelligent cooling system would spend if the AI was deactivated and the energy that the AI spends onto the server:

\begin{equation*}
    \textrm{Reward}_t = E_t^{\textrm{noAI}} - E_t^{\textrm{AI}}
\end{equation*}

And since (Assumption 2), the energy spent is equal to the change of the temperature caused on the server (by any system, including the AI or the unintelligent cooling system):

\begin{equation*}
E_t = |\Delta T_t| = \alpha |T_{t+1} - T_t| =
\begin{cases}
T_{t+1} - T_t & \textrm{if $T_{t+1} > T_t$, that is if the server is heated up} \\
T_t - T_{t+1} & \textrm{if $T_{t+1} < T_t$, that is if the server is cooled down}
\end{cases}
\end{equation*}

then we get that the reward received at time $t$ is in fact the difference in change of temperatures caused on the server between unintelligent cooling system (that is when there is no AI) and the AI:

\begin{align*}
    \textrm{Reward}_t
    & = \textrm{Energy saved by the AI between $t$ and $t+1$} \\
    & = E_t^{\textrm{noAI}} - E_t^{\textrm{AI}} \\
    & = |\Delta T_t^{\textrm{noAI}}| - |\Delta T_t^{\textrm{AI}}|
\end{align*}

where:

\begin{equation*}
\begin{cases}
\textrm{$\Delta T_t^{\textrm{noAI}}$ is the change of temperature that the server's integrated cooling system would cause} \\
\textrm{without the AI onto the server during the iteration $t$, that is from $t$ to $t+1$ minute} \\
\textrm{$\Delta T_t^{\textrm{AI}}$ is the change of temperature caused by the AI onto the server during the iteration $t$,} \\
\textrm{that is from $t$ to $t+1$ minute}
\end{cases}
\end{equation*}

\

\textbf{Important note:} it is important to understand that the systems (our AI and the server's cooling system) will be evaluated separately, in order to compute the rewards. And since each time their actions lead to different temperatures, we will have to keep track separately of the two temperatures $T_t^{\textrm{AI}}$ and $T_t^{\textrm{noAI}}$.

\

Now to finish this section we are going to do a small simulation of 2 iterations (i.e. 2 minutes), as an example that will make everything crystal clear.

\textbf{Final Simulation Example.}

\

Let's say that we are at time $t = 4:00$ pm and that the temperature of the server is $T_t = 28 \degree$C, both with the AI and without the AI. At this exact time, the AI predicts the action 0, 1, 2, 3 or 4. Since right now the server's temperature is outside the optimal temperature range $[18 \degree \textrm{C}, 24 \degree \textrm{C}]$, the AI will probably predict actions 0, 1 or 2. Let's say that it predicts 1, which corresponds to cooling the server down by $1.5 \degree$C. Therefore, between $t = 4:00$ pm and $t+1 = 4:01$ pm, the AI makes the server's temperature go from $T_t^{\textrm{AI}} = 28 \degree \textrm{C}$ to $T_{t+1}^{\textrm{AI}} = 26.5 \degree \textrm{C}$:

\begin{align*}
    \Delta T_t^{\textrm{AI}}
    & = T_{t+1}^{\textrm{AI}} - T_t^{\textrm{AI}} \\
    & = 26.5 - 27 \\
    & = -1.5 \degree \textrm{C}
\end{align*}

Thus, based on Assumption 2, the energy spent by the AI onto the server is:

\begin{align*}
    E_t^{\textrm{AI}}
    & = |\Delta T_t^{\textrm{AI}}| \\
    & = 1.5 \ \textrm{Joules}
\end{align*}

Good, now only one info is missing to compute the reward: it is the energy that the server's integrated cooling system would have spent if the AI was deactivated between 4:00pm and 4:01pm. Remember that this unintelligent cooling system is automatically bringing the server's temperature back to the closest bound of the optimal temperature range $[18 \degree \textrm{C}, 24 \degree \textrm{C}]$. So since at $t = 4:00$ pm the temperature was $T_t = 28 \degree$C, then the closest bound of the optimal temperature range at that time was $24 \degree$C. Thus the server's integrated cooling system would have changed the temperature from $T_t = 28 \degree \textrm{C}$ to $T_{t+1} = 24 \degree \textrm{C}$, and therefore  the server's temperature change that would have occurred if there was no AI is:

\begin{align*}
    \Delta T_t^{\textrm{noAI}}
    & = T_{t+1}^{\textrm{noAI}} - T_t^{\textrm{noAI}} \\
    & = 24 - 28 \\
    & = -4 \degree C
\end{align*}

\

Thus, based on Assumption 2, the energy that the unintelligent cooling system would have spent if there was no AI is:

\begin{align*}
    E_t^{\textrm{noAI}}
    & = |\Delta T_t^{\textrm{noAI}}| \\
    & = 4 \ \textrm{Joules}
\end{align*}

\

Hence in conclusion, the reward we get after playing this action at time $t = 4:00$ pm is:

\begin{align*}
    \textrm{Reward}
    & = E_t^{\textrm{noAI}} - E_t^{\textrm{AI}} \\
    & = 4 - 1.5 \\
    & = 2.5
\end{align*}

\

Then, between $t = 4:00$ pm and $t+1 = 4:01$ pm, other things happen: some new users are logging on to the server, some existing users are logging off the server, some new data is transmitting inside the server, and some existing data is transmitting outside the server. Based on Assumption 1, these factors make the server's temperature change. Let's say they increase the server's temperature by $5 \degree$C:

\begin{equation*}
    \Delta_t \ \textrm{Intrinsic Temperature} = 5 \degree C
\end{equation*}

\

Now remember that we are evaluating two systems separately: our AI, and the server's integrated cooling system. Therefore we must compute separately the two temperatures we would get with these two systems at $t+1 = 4:01$ pm. Let's start with the AI.

\

The temperature we get at $t+1 = 4:01$ pm when the AI is activated is:

\begin{align*}
    T_{t+1}^{\textrm{AI}}
    & = T_t^{\textrm{AI}} + \Delta T_t^{\textrm{AI}} + \Delta_t \ \textrm{Intrinsic Temperature} \\
    & = 28 + (-1.5) + 5 \\
    & = 31.5 \degree C
\end{align*}

And the temperature we get at $t+1 = 4:01$ pm when the AI is not activated is:

\begin{align*}
    T_{t+1}^{\textrm{noAI}}
    & = T_t^{\textrm{noAI}} + \Delta T_t^{\textrm{noAI}} + \Delta_t \ \textrm{Intrinsic Temperature} \\
    & = 28 + (-4) + 5 \\
    & = 29 \degree C
\end{align*}

Perfect, we have our two separate temperatures, which are $T_{t+1}^{\textrm{AI}} = 29.5 \degree C$ when the AI is activated, and $T_{t+1}^{\textrm{noAI}} = 27 \degree C$ when the AI is not activated.

\

Now let's simulate what happens between $t+1 = 4:01$ pm and $t+2 = 4:02$ pm. Again, our AI will make a prediction, and since the server is heating up, let's say it predicts action 0, which corresponds to cooling down the server by $3 \degree C$, bringing it down to $T_{t+2}^{\textrm{AI}} = 28.5 \degree C$. Therefore, the energy spent by the AI between $t+1 = 4:01$ pm and $t+2 = 4:02$ pm, is:

\begin{align*}
    E_{t+1}^{\textrm{AI}}
    & = |\Delta T_{t+1}^{\textrm{AI}}| \\
    & = |28.5 - 31.5| \\
    & = 3 \ \textrm{Joules}
\end{align*}

Now regarding the server's integrated cooling system (i.e. when there is no AI), since at $t+1 = 4:01$ pm we had $T_{t+1}^{\textrm{noAI}} = 29 \degree C$, then the closest bound of the optimal range of temperatures is still $24 \degree C$, and so the energy that the server's unintelligent cooling system would spend between $t+1 = 4:01$ pm and $t+2 = 4:02$ pm, is:

\begin{align*}
    E_{t+1}^{\textrm{noAI}}
    & = |\Delta T_{t+1}^{\textrm{noAI}}| \\
    & = |24 - 29| \\
    & = 5 \ \textrm{Joules}
\end{align*}

Hence the reward obtained between $t+1 = 4:01$ pm and $t+2 = 4:02$ pm, is:

\begin{align*}
    \textrm{Reward}
    & = E_{t+1}^{\textrm{noAI}} - E_{t+1}^{\textrm{AI}} \\
    & = 5 - 3 \\
    & = 2
\end{align*}

And finally, the total reward obtained between $t = 4:00$ pm and $t+2 = 4:02$ pm, is:

\begin{align*}
    \textrm{Total Reward}
    & = (\textrm{Reward obtained between $t$ and $t+1$}) + (\textrm{Reward obtained between $t+1$ and $t+2$}) \\
    &  = 2.5 + 2 \\
    & = 4.5
\end{align*}

That was an example of the whole process happening in two minutes. In our implementation we will run the same process over 1000 epochs of 5-months period for the training, and then, once our AI is trained, we will run the same process over 1 full year of simulation for the testing. The training will be done with Deep Q-Learning, and this is where the next section comes into play.

\subsection{AI Solution}

The AI Solution that will solve the problem described above is a Deep Q-Learning model. Let's give the intuition and the maths equations behind it.

\subsubsection{Q-Learning into Deep Learning}

Deep Q-Learning consists of combining Q-Learning to an Artificial Neural Network. Inputs are encoded vectors, each one defining a state of the environment. These inputs go into an Artificial Neural Network, where the output is the action to play. More precisely, let's say the game has n possible actions, the output layer of the neural network is comprised of n output neurons, each one corresponding to the Q-values of each action played in the current state. Then the action played is the one associated with the output neuron that has the highest Q-value (argmax), or the one returned by the softmax method. In our case we will use argmax. And since Q-values are real numbers, that makes our neural network an ANN for Regression.

\

Hence, in each state $s_t$:

\begin{itemize}

\item the prediction is the Q-value $Q(s_t, a_t)$ where $a_t$ is chosen by argmax or softmax
\item the target is $r_t + \gamma \underset{a}{\max}(Q(s_{t+1}, a))$
\item the loss error between the prediction and the target is the squared of the Temporal Difference:

\begin{equation*}
\textrm{Loss} = \frac{1}{2} \left( r_t + \gamma \underset{a}{\max}(Q(s_{t+1}, a)) - Q(s_t, a_t) \right)^2 = \frac{1}{2} TD_t(s_t, a_t)^2
\end{equation*}

\end{itemize}

Then this loss error is backpropagated into the network, and the weights are updated according to how much they contributed to the error.

\subsubsection{Experience Replay}

We notice that so far we have only considered transitions from one state $s_t$ to the next state $s_{t+1}$. The problem with this is that $s_t$ is most of the time very correlated with $s_{t+1}$. Therefore the network is not learning much. This could be way improved if, instead of considering only this one previous transition, we considered the last m transitions where m is a large number. This pack of the last m transitions is what is called the Experience Replay. Then from this Experience Replay we take some random batches of transitions to make our updates.

\subsubsection{The Brain}

The brain, or more precisely the deep neural network of our AI, will be a fully connected neural network, composed of two hidden layers, the first one having 64 neurons, and the second one having 32 neurons. And as a reminder, this neural network takes as inputs the states of the environment, and returns as outputs the Q-Values for each of the 5 actions. This artificial brain will be trained with a "Mean Squared Error" loss, and an Adam optimizer.

\

Here is what this artificial brain looks like:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{Brain.png}
			\caption{The Artificial Brain: A Fully Connected Neural Network}
		\end{center}
\end{figure}

This artificial brain looks complex to create, but we will build it very easily thanks to the amazing Keras library. Here is actually a preview of the full implementation containing the part that builds this brain all by itself:

\

\begin{lstlisting}
# Building the Brain

class Brain(object):

    def __init__(self, learning_rate = 0.001, number_actions = 11):
        self.learning_rate = learning_rate
        states = Input(shape = (3,))
        x = Dense(units = 64, activation = 'sigmoid')(states)
        y = Dense(units = 32, activation = 'sigmoid')(x)
        q_values = Dense(units = number_actions, activation = 'softmax')(y)
        self.model = Model(inputs = states, outputs = q_values)
        self.model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))
\end{lstlisting}

\

As we can see gladly, it only took a couple lines of code.

\subsubsection{The whole Deep Q-Learning algorithm}

\

Let's summarize the different steps of the whole Deep Q-Learning process:

\

\textbf{Initialization}:

\

The memory of the Experience Replay is initialized to an empty list $M$.

\

We choose a maximum size of the memory. In our case study we choose a maximum size of 100 transitions.

\

We start in a first state, corresponding to a specific time within the year.

\

\textbf{At each time $t$, we repeat the following process, until the end of the epoch (5 months in our implementation)}:

\begin{enumerate}

\item We predict the Q-Values of the current state $s_t$.

\item We play the action that corresponds to the maximum of these predicted Q-Values (argmax method):

\begin{equation*}
    a_t = \underset{a}{\textrm{argmax}} Q(s_t, a)
\end{equation*}

\item We get the reward:

\begin{equation*}
    r_t = E_t^{\textrm{noAI}} - E_t^{\textrm{AI}}
\end{equation*}

\item We reach the next state $s_{t+1}$.

\item We append the transition $(s_t, a_t, r_t, s_{t+1})$ in $M$.

\item We take a random batch $B \subset M$ of transitions. For all the transitions $(s_{t_B}, a_{t_B}, r_{t_B}, s_{t_B+1})$ of the random batch $B$:

\begin{itemize}

\item We get the predictions:

\begin{equation*}
Q(s_{t_B}, a_{t_B})
\end{equation*}

\item We get the targets:

\begin{equation*}
r_{t_B} + \gamma \underset{a}{\max}(Q(s_{t_B+1}, a))
\end{equation*}

\item We compute the loss between the predictions and the targets over the whole batch $B$:

\begin{equation*}
\textrm{Loss} = \frac{1}{2} \sum_B \left( r_{t_B} + \gamma \underset{a}{\max}(Q(s_{t_B+1}, a)) - Q(s_{t_B}, a_{t_B}) \right)^2 = \frac{1}{2} \sum_B TD_{t_B}(s_{t_B}, a_{t_B})^2
\end{equation*}

\item We backpropagate this loss error back into the neural network, and through stochastic gradient descent we update the weights according to how much they contributed to the error.

\end{itemize}

\end{enumerate}

\newpage

\subsection{Implementation}

\

This implementation will be divided in 5 parts, each part having its own python file. These 5 parts constitute the general AI Framework, or AI Blueprint, that should be followed whenever we build an environment to solve any business problem with Deep Reinforcement Learning.

\

Here they are, from Step 1 to Step 5:

\

\begin{enumerate}
    \item \textbf{Step 1:} Building the Environment.
    \
    \item \textbf{Step 2:} Building the Brain.
    \
    \item \textbf{Step 3:} Implementing the Deep Reinforcement Learning algorithm (in our case it will be the DQN model).
    \
    \item \textbf{Step 4:} Training the AI.
    \
    \item \textbf{Step 5:} Testing the AI.
\end{enumerate}

\

These are the main steps (in that same order) of the general AI Framework. Let's thus implement our AI for our specific case study, following this AI Blueprint, in the following five sections corresponding to these five main steps. Besides in each step, we will distinguish the sub-steps that are still part of the general AI Framework, from the sub-steps that are specific to our case study, by writing the titles of the code sections in capital letters for all the sub-steps of the general AI Framework, and in minimal letters for all the sub-steps specific to our case study. That means that anytime you see a new code section of which the title is written in capital letters, then it is the next sub-step of the general AI Framework, which you should also follow when building an AI for your own business problem.

\

So now here we go with the beginning of the journey: Step 1 - Building the Environment.

\

This is the largest Python implementation file of this case study, and of the course. So please make sure to rest before, recharge your batteries to get a good energy level, and as soon as you are ready, let's tackle this together!

\

We begin in the next page.

\newpage

\subsubsection{Step 1: Building the Environment}

In this first step, we are going to build the environment inside a class. Why a class? Because we would like to have our environment as an object which we can create easily with any values of some parameters we choose. For example, we can create one environment object for one server that has a certain number of connected users and a certain rate of data at a specific time, and one other environment object for another server that has a different number of connected users and a different rate of data at some other time. And thanks to this advanced structure of the class, we can easily plug-and-play the environment objects we create on different servers which have their own parameters, hence regulating their temperatures with several different AIs, so that we end up minimizing the energy consumption of a whole data center, just as Google DeepMind for Google's data centers did with their DQN algorithm.

\

This class follows the below sub-steps, which are part of the general AI Framework inside Step 1 - Building the environment:

\begin{enumerate}
    \item \textbf{Step 1-1:} Introducing and initializing all the parameters and variables of the environment.
    \item \textbf{Step 1-2:} Making a method that updates the environment right after the AI plays an action.
    \item \textbf{Step 1-3:} Making a method that resets the environment.
    \item \textbf{Step 1-4:} Making a method that gives us at any time the current state, the last reward obtained, and whether the game is over.
\end{enumerate}

You will find the whole implementation of this Environment class in the next four pages. Remember the most important: all the code sections having their titles written in capital letters are the steps of the general AI Framework / Blueprint, and all the code sections having their titles written in minimal letters are specific to our case study.

\

Below is the whole implementation of our first python file. The code sections titles and the chosen variables names are clear enough to understand what is being coded, but if you need any more explanation, I encourage to watch our video tutorials where we code everything from scratch, step by step, while explaining every single line of code in terms of why, what and how. Here we go:

\

\begin{lstlisting}
# Artificial Intelligence for Business - Case Study 2
# Building the Environment

# Importing the libraries
import numpy as np

# BUILDING THE ENVIRONMENT IN A CLASS

class Environment(object):
    
    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT
    
    def __init__(self,
                optimal_temperature = (18.0, 24.0),
                initial_month = 0,
                initial_number_users = 10,
                initial_rate_data = 60):
        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0,
                                                23.0, 24.0, 22.0, 10.0, 5.0, 1.0]
        self.initial_month = initial_month
        self.atmospheric_temperature = \
                                self.monthly_atmospheric_temperatures[initial_month]
        self.optimal_temperature = optimal_temperature
        self.min_temperature = -20
        self.max_temperature = 80
        self.min_number_users = 10
        self.max_number_users = 100
        self.max_update_users = 5
        self.min_rate_data = 20
        self.max_rate_data = 300
        self.max_update_data = 10
        self.initial_number_users = initial_number_users
        self.current_number_users = initial_number_users
        self.initial_rate_data = initial_rate_data
        self.current_rate_data = initial_rate_data
        self.intrinsic_temperature = self.atmospheric_temperature
                                     + 1.25 * self.current_number_users
                                     + 1.25 * self.current_rate_data
        self.temperature_ai = self.intrinsic_temperature
        self.temperature_noai = (self.optimal_temperature[0]
                                + self.optimal_temperature[1]) / 2.0
        self.total_energy_ai = 0.0
        self.total_energy_noai = 0.0
        self.reward = 0.0
        self.game_over = 0
        self.train = 1


    # MAKING A METHOD THAT UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION
    
    def update_env(self, direction, energy_ai, month):
        
        # GETTING THE REWARD
        
        # Computing the energy spent by the server's cooling system when there is no AI
        energy_noai = 0
        if (self.temperature_noai < self.optimal_temperature[0]):
            energy_noai = self.optimal_temperature[0] - self.temperature_noai
            self.temperature_noai = self.optimal_temperature[0]
        elif (self.temperature_noai > self.optimal_temperature[1]):
            energy_noai = self.temperature_noai - self.optimal_temperature[1]
            self.temperature_noai = self.optimal_temperature[1]
        # Computing the Reward
        self.reward = energy_noai - energy_ai
        # Scaling the Reward
        self.reward = 1e-3 * self.reward
        
        # GETTING THE NEXT STATE
        
        # Updating the atmospheric temperature
        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]
        # Updating the number of users
        self.current_number_users += np.random.randint(-self.max_update_users,
                                                       self.max_update_users)
        if (self.current_number_users > self.max_number_users):
            self.current_number_users = self.max_number_users
        elif (self.current_number_users < self.min_number_users):
            self.current_number_users = self.min_number_users
        # Updating the rate of data
        self.current_rate_data += np.random.randint(-self.max_update_data,
                                                    self.max_update_data)
        if (self.current_rate_data > self.max_rate_data):
            self.current_rate_data = self.max_rate_data
        elif (self.current_rate_data < self.min_rate_data):
            self.current_rate_data = self.min_rate_data
        # Computing the Delta of Intrinsic Temperature
        past_intrinsic_temperature = self.intrinsic_temperature
        self.intrinsic_temperature = self.atmospheric_temperature
                                     + 1.25 * self.current_number_users
                                     + 1.25 * self.current_rate_data
        delta_intrinsic_temperature = self.intrinsic_temperature
                                      - past_intrinsic_temperature
        # Computing the Delta of Temperature caused by the AI
        if (direction == -1):
            delta_temperature_ai = -energy_ai
        elif (direction == 1):
            delta_temperature_ai = energy_ai
        # Updating the new Server's Temperature when there is the AI
        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai
        # Updating the new Server's Temperature when there is no AI
        self.temperature_noai += delta_intrinsic_temperature
        
        # GETTING GAME OVER
        
        if (self.temperature_ai < self.min_temperature):
            if (self.train == 1):
                self.game_over = 1
            else:
                self.total_energy_ai += self.optimal_temperature[0]
                                        - self.temperature_ai
                self.temperature_ai = self.optimal_temperature[0]
        elif (self.temperature_ai > self.max_temperature):
            if (self.train == 1):
                self.game_over = 1
            else:
                self.total_energy_ai += self.temperature_ai
                                        - self.optimal_temperature[1]
                self.temperature_ai = self.optimal_temperature[1]
        
        # UPDATING THE SCORES
        
        # Updating the Total Energy spent by the AI
        self.total_energy_ai += energy_ai
        # Updating the Total Energy spent by the alternative system when there is no AI
        self.total_energy_noai += energy_noai
        
        # SCALING THE NEXT STATE
        
        scaled_temperature_ai = (self.temperature_ai - self.min_temperature)
                                / (self.max_temperature - self.min_temperature)
        scaled_number_users = (self.current_number_users - self.min_number_users)
                              / (self.max_number_users - self.min_number_users)
        scaled_rate_data = (self.current_rate_data - self.min_rate_data)
                           / (self.max_rate_data - self.min_rate_data)
        next_state = np.matrix([scaled_temperature_ai,
                                scaled_number_users,
                                scaled_rate_data])
        
        # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER
        
        return next_state, self.reward, self.game_over

    # MAKING A METHOD THAT RESETS THE ENVIRONMENT
    
    def reset(self, new_month):
        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]
        self.initial_month = new_month
        self.current_number_users = self.initial_number_users
        self.current_rate_data = self.initial_rate_data
        self.intrinsic_temperature = self.atmospheric_temperature
                                     + 1.25 * self.current_number_users
                                     + 1.25 * self.current_rate_data
        self.temperature_ai = self.intrinsic_temperature
        self.temperature_noai = (self.optimal_temperature[0]
                                + self.optimal_temperature[1]) / 2.0
        self.total_energy_ai = 0.0
        self.total_energy_noai = 0.0
        self.reward = 0.0
        self.game_over = 0
        self.train = 1

    # MAKING A METHOD THAT GIVES US AT ANY TIME THE STATE, THE REWARD AND GAMEOVER
    
    def observe(self):
        scaled_temperature_ai = (self.temperature_ai - self.min_temperature)
                                / (self.max_temperature - self.min_temperature)
        scaled_number_users = (self.current_number_users - self.min_number_users)
                              / (self.max_number_users - self.min_number_users)
        scaled_rate_data = (self.current_rate_data - self.min_rate_data)
                           / (self.max_rate_data - self.min_rate_data)
        current_state = np.matrix([scaled_temperature_ai,
                                   scaled_number_users,
                                   scaled_rate_data])
        return current_state, self.reward, self.game_over
\end{lstlisting}

\

Congratulations for implementing Step 1 - Building the Environment. Now let's move on to Step 2 - Building the Brain.

\newpage

\subsubsection{Step 2: Building the Brain}

In this Step 2, we are going to build the artificial brain of our AI, which is nothing else than a fully connected neural network. Here it is again:

\begin{figure}[!htbp]
		\begin{center}
			\includegraphics[scale=0.18]{Brain.png}
			\caption{The Artificial Brain: A Fully Connected Neural Network}
		\end{center}
\end{figure}

Again, we will build this artificial brain inside a class, for the same reason as before which is to allow us to create several artificial brains for different servers inside a data center. Indeed, maybe some servers will need different artificial brains with different hyper-parameters than other servers. That's why thanks to this class / object Python advanced structure, we can easily switch from one brain to another to regulate the temperature of a new server that requires an AI with different neural networks parameters.

\

We build this artificial brain thanks to the amazing Keras library. From this library we use the Dense() class to create our two fully connected hidden layers, the first one having 64 hidden neurons, and the second one having 32 neurons. And we use the Dense() class again to return Q-Values, which keep in mind are the outputs of the artificial neural networks. Then later on in the training and the testing files, we will use the argmax method to select the action that has the maximum Q-Value. Then, we assemble all the components of the brain, including the inputs and the outputs, by creating it as an object of the Model() class (very useful to then save and load a model with specific weights). End eventually, we compile it with a Mean-Squared Error loss and an Adam optimizer. Thus here are the new steps of the general AI Framework:

\begin{enumerate}
    \item \textbf{Step 2-1:} Building the input layer composed of the input states.
    \item \textbf{Step 2-2:} Building the hidden layers with a chosen number of these layers and neurons inside each, fully connected to the input layer and between each other.
    \item \textbf{Step 2-3:} Building the output layer, fully connected to the last hidden layer.
    \item \textbf{Step 2-4:} Assembling the full architecture inside a model object.
    \item \textbf{Step 2-5:} Compiling the model with a Mean-Squared Error loss function and a chosen optimizer.
\end{enumerate}

Here we go with the implementation:

\

\begin{lstlisting}
# Artificial Intelligence for Business - Case Study 2
# Building the Brain

# Importing the libraries
from keras.layers import Input, Dense
from keras.models import Model
from keras.optimizers import Adam

# BUILDING THE BRAIN

class Brain(object):
    
    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD
    
    def __init__(self, learning_rate = 0.001, number_actions = 5):
        self.learning_rate = learning_rate
        
        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE
        states = Input(shape = (3,))
        
        # BUILDING THE FULLY CONNECTED HIDDEN LAYERS
        x = Dense(units = 64, activation = 'sigmoid')(states)
        y = Dense(units = 32, activation = 'sigmoid')(x)
        
        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER
        q_values = Dense(units = number_actions, activation = 'softmax')(y)
        
        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT
        self.model = Model(inputs = states, outputs = q_values)
        
        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER
        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))
\end{lstlisting}

\

\textbf{Dropout.}

\

I thought it would be valuable for you to even add one more powerful technique in your toolkit: \textbf{Dropout}.

\

Dropout is a regularization technique that prevents overfitting. It simply consists of deactivating a certain rate of random neurones during each step of forward \& back propagation. That way, not all the neurones learn the same way, thus preventing the neural network from overfitting the training data.

\

Here is how you implement Dropout:

\begin{enumerate}
    \item First, import Dropout:
    \begin{lstlisting}
    from keras.layers:from keras.layers import Input, Dense, Dropout
    \end{lstlisting}
    \
    \item Then, activate Dropout in the first hidden layer x, with a rate of 0.1, meaning that 10\% of the neurones will be randomly deactivated:
    \begin{lstlisting}
    x = Dense(units = 64, activation = 'sigmoid')(states)
    x = Dropout(rate = 0.1)(x)
    \end{lstlisting}
    \
    \item And finally, activate Dropout in the second hidden layer y, with a rate of 0.1, meaning that 10\% of the neurones will be randomly deactivated:
    \begin{lstlisting}
    y = Dense(units = 32, activation = 'sigmoid')(x)
    y = Dropout(rate = 0.1)(y)
    \end{lstlisting}
\end{enumerate}

\

Congratulations! You have implemented Dropout. It was very simple, once again thanks to Keras.

Below it the whole new brain.py implementation with Dropout:

\

\begin{lstlisting}
# Artificial Intelligence for Business - Case Study 2
# Building the Brain

# Importing the libraries
from keras.layers import Input, Dense, Dropout
from keras.models import Model
from keras.optimizers import Adam

# BUILDING THE BRAIN

class Brain(object):
    
    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD
    
    def __init__(self, learning_rate = 0.001, number_actions = 5):
        self.learning_rate = learning_rate
        
        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE
        states = Input(shape = (3,))
        
        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED
        x = Dense(units = 64, activation = 'sigmoid')(states)
        x = Dropout(rate = 0.1)(x)
        
        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED
        y = Dense(units = 32, activation = 'sigmoid')(x)
        y = Dropout(rate = 0.1)(y)
        
        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER
        q_values = Dense(units = number_actions, activation = 'softmax')(y)
        
        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT
        self.model = Model(inputs = states, outputs = q_values)
        
        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER
        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))
\end{lstlisting}

\

Now let's move on to next step of our general AI Framework: Step 3 - Implementing the DQN algorithm.

\newpage

\subsubsection{Step 3: Implementing the Deep Reinforcement Learning algorithm}

In this new Python file, we simply have to follow the Deep Q-Learning algorithm provided before. Hence, this implementation follows the following sub-steps, which are part of the general AI Framework:

\begin{enumerate}
    \item \textbf{Step 3-1:} Introducing and initializing all the parameters and variables of the DQN model.
    \item \textbf{Step 3-2:} Making a method that builds the memory in Experience Replay.
    \item \textbf{Step 3-3:} Making a method that builds and returns two batches of 10 inputs and 10 targets
\end{enumerate}

Below is the code following this new part of the AI Blueprint:

\

\begin{lstlisting}
# Artificial Intelligence for Business - Case Study 2
# Implementing Deep Q-Learning with Experience Replay

# Importing the libraries
import numpy as np

# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY

class DQN(object):
    
    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN
    def __init__(self, max_memory = 100, discount = 0.9):
        self.memory = list()
        self.max_memory = max_memory
        self.discount = discount

    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY
    def remember(self, transition, game_over):
        self.memory.append([transition, game_over])
        if len(self.memory) > self.max_memory:
            del self.memory[0]

    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS
    def get_batch(self, model, batch_size = 10):
        len_memory = len(self.memory)
        num_inputs = self.memory[0][0][0].shape[1]
        num_outputs = model.output_shape[-1]
        inputs = np.zeros((min(len_memory, batch_size), num_inputs))
        targets = np.zeros((min(len_memory, batch_size), num_outputs))
        for i, idx in enumerate(np.random.randint(0, len_memory,
                                                  size = min(len_memory, batch_size))):
            current_state, action, reward, next_state = self.memory[idx][0]
            game_over = self.memory[idx][1]
            inputs[i] = current_state
            targets[i] = model.predict(current_state)[0]
            Q_sa = np.max(model.predict(next_state)[0])
            if game_over:
                targets[i, action] = reward
            else:
                targets[i, action] = reward + self.discount * Q_sa
        return inputs, targets
\end{lstlisting}

\newpage

\subsubsection{Step 4: Training the AI}

Now that our AI has a fully functional brain, time to train it. And this is exactly what we do in this fourth Python file. The process is long, but very easy: we start by setting all the parameters, then we build the environment by creating an object of the Environment() class, then we build the brain of the AI by creating an object of the Brain() class, then we build the Deep Q-Learning model by creating an object of the DQN() class, and finally we launch the training connecting all these objects together, over 1000 epochs of 5-months period. You will notice in the training loop that we also do some exploration when playing the actions. This consists of playing some random actions from time to time. In our case study this will be done 30\% of the time, since we use an exploration parameter $\epsilon = 0.3$, and then we force to play a random action when drawing a random value between 0 and 1 that is below $\epsilon = 0.3$). The reason why we do some exploration is because it improves the Deep Reinforcement Learning process. This trick is called: "Exploration vs. Exploitation". Then, besides you will also notice that we use an early stopping technique, which will make sure to stop the training if there is performance improvement.

\

Let's highlight these new steps that still belong to our general AI Framework / Blueprint:

\begin{enumerate}
    \item \textbf{Step 4-1:} Building the environment by creating an object of the Environment class.
    \item \textbf{Step 4-2:} Building the artificial brain by creating an object of the brain class.
    \item \textbf{Step 4-3:} Building the DQN model by creating an object of the DQN class.
    \item \textbf{Step 4-4:} Choosing the training mode.
    \item \textbf{Step 4-5:} Starting the training with a for loop over 100 epochs of 5-months periods.
    \item \textbf{Step 4-6:} During each epoch we repeat the whole Deep Q-Learning process, while also doing some exploration 30\% of the time.
\end{enumerate}

And now let's implement this new part, Step 4 - Training the AI, of our general Blueprint. Below is the whole implementation of this fourth python file. Again, the code sections titles and the chosen variables names are clear enough to understand what is being coded. Here we go:

\

\begin{lstlisting}
# Artificial Intelligence for Business - Case Study 2
# Training the AI

# Installing Keras
# conda install -c conda-forge keras

# Importing the libraries and the other python files
import os
import numpy as np
import random as rn
import environment
import brain
import dqn

# Setting seeds for reproducibility
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(42)
rn.seed(12345)

# SETTING THE PARAMETERS
epsilon = .3
number_actions = 5
direction_boundary = (number_actions - 1) / 2
number_epochs = 100
max_memory = 3000
batch_size = 512
temperature_step = 1.5

# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS
env = environment.Environment(optimal_temperature = (18.0, 24.0),
                              initial_month = 0,
                              initial_number_users = 20,
                              initial_rate_data = 30)

# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS
brain = brain.Brain(learning_rate = 0.00001, number_actions = number_actions)

# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS
dqn = dqn.DQN(max_memory = max_memory, discount = 0.9)

# CHOOSING THE MODE
train = True

# TRAINING THE AI
env.train = train
model = brain.model
early_stopping = True
patience = 10
best_total_reward = -np.inf
patience_count = 0
if (env.train):
    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)
    for epoch in range(1, number_epochs):
        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP
        total_reward = 0
        loss = 0.
        new_month = np.random.randint(0, 12)
        env.reset(new_month = new_month)
        game_over = False
        current_state, _, _ = env.observe()
        timestep = 0
        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH
        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):
            # PLAYING THE NEXT ACTION BY EXPLORATION
            if np.random.rand() <= epsilon:
                action = np.random.randint(0, number_actions)
                if (action - direction_boundary < 0):
                    direction = -1
                else:
                    direction = 1
                energy_ai = abs(action - direction_boundary) * temperature_step
            
            # PLAYING THE NEXT ACTION BY INFERENCE
            else:
                q_values = model.predict(current_state)
                action = np.argmax(q_values[0])
                if (action - direction_boundary < 0):
                    direction = -1
                else:
                    direction = 1
                energy_ai = abs(action - direction_boundary) * temperature_step
            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE
            next_state, reward, game_over = env.update_env(direction,
                                                           energy_ai,
                                                           int(timestep / (30*24*60)))
            total_reward += reward
            # STORING THIS NEW TRANSITION INTO THE MEMORY
            dqn.remember([current_state, action, reward, next_state], game_over)
            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS
            inputs, targets = dqn.get_batch(model, batch_size = batch_size)
            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS
            loss += model.train_on_batch(inputs, targets)
            timestep += 1
            current_state = next_state
        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH
        print("\n")
        print("Epoch: {:03d}/{:03d}".format(epoch, number_epochs))
        print("Total Energy spent with an AI: {:.0f}".format(env.total_energy_ai))
        print("Total Energy spent with no AI: {:.0f}".format(env.total_energy_noai))
        # EARLY STOPPING
        if (early_stopping):
            if (total_reward <= best_total_reward):
                patience_count += 1
            elif (total_reward > best_total_reward):
                best_total_reward = total_reward
                patience_count = 0
            if (patience_count >= patience):
                print("Early Stopping")
                break
        # SAVING THE MODEL
        model.save("model.h5")
\end{lstlisting}

\

After executing the code, we already see some good performance of our AI during the training, spending most of the time less energy than the alternative system, i.e. the server's integrated cooling system. But that is only the training, now we need to see if we also obtain some good performance on a new 1-year simulation. That's where our next, and final Python file, comes into play. 

\newpage

\subsubsection{Step 5: Testing the AI}

Now indeed, we need to test the performance of our AI on a brand new situation. To do so, we will run a 1-year simulation, only in inference mode, meaning that there will be no training happening at any time. Our AI will only return predictions over a one full year of simulation. Then thanks to our environment object we will get in the end the total energy spent by the AI over this one full year, as well as the total energy spent by the server's integrated cooling system. Eventually we will compare these two total energy spent, by simply computing their relative difference (in \%), which will exactly give us the total energy saved by the AI. Buckle up for the final results, we will reveal them at the end of this Part 2!

\

In terms of the AI Blueprint, here for the testing implementation we almost have the same as before, except that this time, we don't have to create a brain object nor a DQN model object, and of course we must not run the Deep Q-Learning process over some training epochs. However we do have to create a new environment object, and instead of creating a brain, we will load our artificial brain with its pre-trained weights from the previous training that we executed in Step 4 - Training the AI. Hence, let's give the final sub-steps of this final part of the AI Framework / Blueprint:

\begin{enumerate}
    \item \textbf{Step 5-1:} Building a new environment by creating an object of the Environment class.
    \item \textbf{Step 5-2:} Loading the artificial brain with its pre-trained weights from the previous training.
    \item \textbf{Step 5-3:} Choosing the inference mode.
    \item \textbf{Step 5-4:} Starting the 1-year simulation.
    \item \textbf{Step 5-5:} At each iteration (each minute), our AI only plays the action that results from its prediction, and no exploration or Deep Q-Learning training is happening whatsoever.
\end{enumerate}

And now let's implement this fifth and final part, Step 5 - Testing the AI. Again, below is the whole implementation of our first python file. The code sections titles and the chosen variables names are clear enough to understand what is being coded, but if you need any more explanation, I encourage to watch our video tutorials where we code everything from scratch, step by step, while explaining every single line of code in terms of why, what and how. Here we go:

\

\begin{lstlisting}
# Artificial Intelligence for Business - Case Study 2
# Testing the AI

# Installing Keras
# conda install -c conda-forge keras

# Importing the libraries and the other python files
import os
import numpy as np
import random as rn
from keras.models import load_model
import environment

# Setting seeds for reproducibility
os.environ['PYTHONHASHSEED'] = '0'
np.random.seed(42)
rn.seed(12345)

# SETTING THE PARAMETERS
number_actions = 5
direction_boundary = (number_actions - 1) / 2
temperature_step = 1.5

# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS
env = environment.Environment(optimal_temperature = (18.0, 24.0),
                              initial_month = 0,
                              initial_number_users = 20,
                              initial_rate_data = 30)

# LOADING A PRE-TRAINED BRAIN
model = load_model("model.h5")

# CHOOSING THE MODE
train = False

# RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE
env.train = train
current_state, _, _ = env.observe()
for timestep in range(0, 12 * 30 * 24 * 60):
    q_values = model.predict(current_state)
    action = np.argmax(q_values[0])
    if (action - direction_boundary < 0):
        direction = -1
    else:
        direction = 1
    energy_ai = abs(action - direction_boundary) * temperature_step
    next_state, reward, game_over = env.update_env(direction,
                                                   energy_ai,
                                                   int(timestep / (30*24*60)))
    current_state = next_state

# PRINTING THE TRAINING RESULTS FOR EACH EPOCH
print("\n")
print("Total Energy spent with an AI: {:.0f}".format(env.total_energy_ai))
print("Total Energy spent with no AI: {:.0f}".format(env.total_energy_noai))
print("ENERGY SAVED: {:.0f} %".format((env.total_energy_noai - env.total_energy_ai)
                                     / env.total_energy_noai * 100))
\end{lstlisting}

\

And finally, we obtain in the printed results that the total energy consumption saved by the AI is...: 

\begin{equation*}
    \textrm{Total Energy saved by the AI} = 39 \ \% \ !
\end{equation*}

Exactly like what Google DeepMind achieved in 2016! Indeed, if on Google you type: "DeepMind reduces Google cooling bill", you will see that the result they achieved is 40 \%. Very close to ours!

\

Hence what we have built is surely excellent for our business client, as our AI will save them a lot of costs! Indeed, remember that thanks to our object oriented structure (working with classes and objects), we can very easily take our objects created in this implementation that we did for one server, and then plug them into other servers, so that in the end we end up saving the total energy consumption of a whole data center! That's how Google saved billions of dollars in energy related costs, thanks to their DQN model built by DeepMind AI.

\newpage

\subsubsection{Recap: The General AI Framework / Blueprint}

Let's recap and provide the whole AI Blueprint, so that you can print it out and put it on your wall.

\

\textbf{Step 1: Building the Environment}

\begin{enumerate}
    \item \textbf{Step 1-1:} Introducing and initializing all the parameters and variables of the environment.
    \item \textbf{Step 1-2:} Making a method that updates the environment right after the AI plays an action.
    \item \textbf{Step 1-3:} Making a method that resets the environment.
    \item \textbf{Step 1-4:} Making a method that gives us at any time the current state, the last reward obtained, and whether the game is over.
\end{enumerate}

\textbf{Step 2: Building the Brain}

\begin{enumerate}
    \item \textbf{Step 2-1:} Building the input layer composed of the input states.
    \item \textbf{Step 2-2:} Building the hidden layers with a chosen number of these layers and neurons inside each, fully connected to the input layer and between each other.
    \item \textbf{Step 2-3:} Building the output layer, fully connected to the last hidden layer.
    \item \textbf{Step 2-4:} Assembling the full architecture inside a model object.
    \item \textbf{Step 2-5:} Compiling the model with a Mean-Squared Error loss function and a chosen optimizer.
\end{enumerate}

\textbf{Step 3: Implementing the Deep Reinforcement Learning Algorithm}

\begin{enumerate}
    \item \textbf{Step 3-1:} Introducing and initializing all the parameters and variables of the DQN model.
    \item \textbf{Step 3-2:} Making a method that builds the memory in Experience Replay.
    \item \textbf{Step 3-3:} Making a method that builds and returns two batches of 10 inputs and 10 targets
\end{enumerate}

\textbf{Step 4: Training the AI}

\begin{enumerate}
    \item \textbf{Step 4-1:} Building the environment by creating an object of the Environment class built in Step 1.
    \item \textbf{Step 4-2:} Building the artificial brain by creating an object of the Brain class built in Step 2.
    \item \textbf{Step 4-3:} Building the DQN model by creating an object of the DQN class built in Step 3.
    \item \textbf{Step 4-4:} Choosing the training mode.
    \item \textbf{Step 4-5:} Starting the training with a for loop over a chosen number of epochs.
    \item \textbf{Step 4-6:} During each epoch we repeat the whole Deep Q-Learning process, while also doing some exploration 30\% of the time.
\end{enumerate}

\textbf{Step 5: Testing the AI}

\begin{enumerate}
    \item \textbf{Step 5-1:} Building a new environment by creating an object of the Environment class built in Step 1.
    \item \textbf{Step 5-2:} Loading the artificial brain with its pre-trained weights from the previous training.
    \item \textbf{Step 5-3:} Choosing the inference mode.
    \item \textbf{Step 5-4:} Starting the simulation.
    \item \textbf{Step 5-5:} At each iteration (each minute), our AI only plays the action that results from its prediction, and no exploration or Deep Q-Learning training is happening whatsoever.
\end{enumerate}



