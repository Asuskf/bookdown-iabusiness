<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Parte 1 Optimización de Procesos | Inteligencia Artificial aplicada a Negocios y Empresas</title>
  <meta name="description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Parte 1 Optimización de Procesos | Inteligencia Artificial aplicada a Negocios y Empresas" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" />
  <meta property="og:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />
  <meta property="og:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="github-repo" content="https://github.com/joanby/ia4business" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Parte 1 Optimización de Procesos | Inteligencia Artificial aplicada a Negocios y Empresas" />
  
  <meta name="twitter:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="twitter:image" content="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />

<meta name="author" content="Hadelin de Ponteves y Kirill Ermenko" />


<meta name="date" content="2020-03-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="Images/apple-icon-120x120.png" />
  <link rel="shortcut icon" href="Images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="index.html"/>
<link rel="next" href="part-2-minimizing-costs.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inteligencia Artificial aplicada Negocios y Empresas</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introducción</a></li>
<li class="chapter" data-level="1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html"><i class="fa fa-check"></i><b>1</b> Optimización de Procesos</a><ul>
<li class="chapter" data-level="1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#caso-práctico-optimización-de-tareas-en-un-almacén-de-comercio-electrónico"><i class="fa fa-check"></i><b>1.1</b> Caso Práctico: Optimización de tareas en un almacén de comercio electrónico</a><ul>
<li class="chapter" data-level="1.1.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#problema-a-resolver"><i class="fa fa-check"></i><b>1.1.1</b> Problema a resolver</a></li>
<li class="chapter" data-level="1.1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#entorno-a-definir"><i class="fa fa-check"></i><b>1.1.2</b> Entorno a definir</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#solución-de-inteligencia-artificial"><i class="fa fa-check"></i><b>1.2</b> Solución de Inteligencia Artificial</a><ul>
<li class="chapter" data-level="1.2.1" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#proceso-de-decisión-de-markov"><i class="fa fa-check"></i><b>1.2.1</b> Proceso de Decisión de Markov</a></li>
<li class="chapter" data-level="1.2.2" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#q-learning"><i class="fa fa-check"></i><b>1.2.2</b> Q-Learning</a></li>
<li class="chapter" data-level="1.2.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#el-algoritmo-de-q-learning-al-completo"><i class="fa fa-check"></i><b>1.2.3</b> El algoritmo de Q-Learning al completo</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimización-de-procesos.html"><a href="optimización-de-procesos.html#implementación"><i class="fa fa-check"></i><b>1.3</b> Implementación</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="part-2-minimizing-costs.html"><a href="part-2-minimizing-costs.html"><i class="fa fa-check"></i><b>2</b> Part 2 - Minimizing Costs</a><ul>
<li class="chapter" data-level="2.1" data-path="part-2-minimizing-costs.html"><a href="part-2-minimizing-costs.html#case-study-minimizing-costs-in-energy-consumption-of-a-data-center"><i class="fa fa-check"></i><b>2.1</b> Case Study: Minimizing Costs in Energy Consumption of a Data Center</a><ul>
<li class="chapter" data-level="2.1.1" data-path="part-2-minimizing-costs.html"><a href="part-2-minimizing-costs.html#problem-to-solve"><i class="fa fa-check"></i><b>2.1.1</b> Problem to solve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="part-3-maximizing-revenues.html"><a href="part-3-maximizing-revenues.html"><i class="fa fa-check"></i><b>3</b> Part 3 - Maximizing Revenues</a><ul>
<li class="chapter" data-level="3.1" data-path="part-3-maximizing-revenues.html"><a href="part-3-maximizing-revenues.html#case-study-maximizing-revenue-of-an-online-retail-business"><i class="fa fa-check"></i><b>3.1</b> Case Study: Maximizing Revenue of an Online Retail Business</a><ul>
<li class="chapter" data-level="3.1.1" data-path="part-3-maximizing-revenues.html"><a href="part-3-maximizing-revenues.html#problem-to-solve-1"><i class="fa fa-check"></i><b>3.1.1</b> Problem to solve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="4" data-path="annex-1-artificial-neural-networks.html"><a href="annex-1-artificial-neural-networks.html"><i class="fa fa-check"></i><b>4</b> Annex 1: Artificial Neural Networks</a></li>
<li class="chapter" data-level="5" data-path="annex-2-three-extra-ai-models.html"><a href="annex-2-three-extra-ai-models.html"><i class="fa fa-check"></i><b>5</b> Annex 2: Three Extra AI Models</a></li>
<li class="chapter" data-level="6" data-path="annex-3-questions-and-answers.html"><a href="annex-3-questions-and-answers.html"><i class="fa fa-check"></i><b>6</b> Annex 3: Questions and Answers</a></li>
<li class="divider"></li>
<li><a href="https://www.udemy.com/course/ia4business/?couponCode=B85F8D52148DF5AAD8F7" target="blank">Curso en Udemy</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Inteligencia Artificial aplicada a Negocios y Empresas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimización-de-procesos" class="section level1">
<h1><span class="header-section-number">Parte 1</span> Optimización de Procesos</h1>
<p>Aquí vamos con nuestro primer caso práctico y nuestro primer modelo de IA. ¡Esperamos que estés listo!</p>
<div id="caso-práctico-optimización-de-tareas-en-un-almacén-de-comercio-electrónico" class="section level2">
<h2><span class="header-section-number">1.1</span> Caso Práctico: Optimización de tareas en un almacén de comercio electrónico</h2>
<div id="problema-a-resolver" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Problema a resolver</h3>
<p>El problema a resolver será optimizar los flujos dentro del siguiente almacén:</p>
<p><img src="Warehouse_1.png" style="width:100.0%" /></p>
<p>El almacén pertenece a una empresa online minorista que vende productos a una variedad de clientes. Dentro de este almacén, los productos se almacenan en 12 ubicaciones diferentes, etiquetadas con las siguientes letras de la A a la L:</p>
<p><img src="Warehouse_2.png" style="width:100.0%" /></p>
<p>A medida que los clientes hacen los pedidos online, un robot de almacén autónomo se mueve por el almacén para recoger los productos para futuras entregas. Así es como se ve:</p>
<div class="figure">
<img src="Autonomous_Warehouse_Robot.jpg" alt="Autonomous Warehouse Robot" style="width:100.0%" />
<p class="caption">Autonomous Warehouse Robot</p>
</div>
<p>Las 12 ubicaciones están conectadas a un sistema informático, que clasifica en tiempo real las prioridades de recolección de productos para estas 12 ubicaciones. Por ejemplo, en un momento específico <span class="math inline">\(t\)</span>, devolverá la siguiente clasificación:</p>
<table>
<thead>
<tr class="header">
<th align="center"><strong>Rango de Prioridad</strong></th>
<th align="center"><strong>Ubicación</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">G</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">K</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">L</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">J</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">A</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">I</td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center">H</td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center">C</td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center">B</td>
</tr>
<tr class="even">
<td align="center">10</td>
<td align="center">D</td>
</tr>
<tr class="odd">
<td align="center">11</td>
<td align="center">F</td>
</tr>
<tr class="even">
<td align="center">12</td>
<td align="center">E</td>
</tr>
</tbody>
</table>
<p>La ubicación G tiene prioridad 1, lo que significa que es la máxima prioridad, ya que contiene un producto que debe recogerse y entregarse de inmediato. Nuestro robot de almacén autónomo debe moverse a la ubicación G por la ruta más corta, dependiendo de dónde se encuentre. Nuestro objetivo es construir una IA que regrese esa ruta más corta, donde sea que esté el robot. Pero luego, como vemos, las ubicaciones K y L están en las 3 prioridades principales. Por lo tanto, querremos implementar una opción para que nuestro Robot de almacén autónomo pase por algunas ubicaciones intermedias antes de llegar a su ubicación final de máxima prioridad.</p>
<p>La forma en que el sistema calcula las prioridades de las ubicaciones está fuera del alcance de este caso práctico. La razón de esto es que puede haber muchas formas, desde reglas o algoritmos simples, hasta cálculos deterministas y aprendizaje automático. Pero la mayoría de estas formas no serían inteligencia artificial como la conocemos hoy. En lo que realmente queremos centrarnos es en la IA central, que abarca Q-Learning, Deep Q-Learning y otras ramas de Reinforcement Learning. Entonces, solo diremos, por ejemplo, que la ubicación G es la máxima prioridad porque uno de los clientes de platino más leales de la compañía hizo un pedido urgente de un producto almacenado en la ubicación G, que por lo tanto debe entregarse lo antes posible.</p>
<p>Por lo tanto, en conclusión, nuestra misión es construir una IA que siempre tome la ruta más corta a la ubicación de máxima prioridad, sea cual sea la ubicación desde la que comienza, y tener la opción de ir a una ubicación intermedia que se encuentre entre las 3 prioridades principales.</p>
</div>
<div id="entorno-a-definir" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Entorno a definir</h3>
<p>Al construir una IA, lo primero que siempre tenemos que hacer es definir el entorno. Y definir un entorno siempre requiere los tres elementos siguientes:</p>
<ul>
<li>Definir los estados</li>
<li>Definir las acciones</li>
<li>Definir las recompensas</li>
</ul>
<p>Definamos estos tres elementos, uno por uno.</p>
<p><strong>Definir los estados.</strong></p>
<p>Comencemos con los estados. El estado de entrada es simplemente la ubicación donde está nuestro Robot de almacén autónomo en cada momento <span class="math inline">\(t\)</span>. Sin embargo, dado que construiremos nuestra IA con ecuaciones matemáticas, codificaremos los nombres de las ubicaciones (A, B, C, …) en números de índice, con respecto a la siguiente asignación:</p>
<table>
<thead>
<tr class="header">
<th align="center"><strong>Ubicación</strong></th>
<th align="center"><strong>Estado</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">A</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">B</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">C</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">D</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">E</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">F</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">G</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">H</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">I</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">J</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center">K</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td align="center">L</td>
<td align="center">11</td>
</tr>
</tbody>
</table>
<p>Hay una razón específica por la que codificamos los estados con índices del 0 al 11, en lugar de otros enteros. La razón es que trabajaremos con matrices, una matriz de recompensas y una matriz de valores Q, y cada línea y columna de estas matrices corresponderá a una ubicación específica. Por ejemplo, la primera línea de cada matriz, que tiene el índice 0, corresponde a la ubicación A. La segunda línea / columna, que tiene el índice 1, corresponde a la ubicación B. Etc. Veremos el propósito de trabajar con matrices con más detalles. un poco más tarde.</p>
<p><strong>Definir las acciones.</strong></p>
<p>Ahora definamos las posibles acciones a realizar Las acciones son simplemente los siguientes movimientos que el robot puede hacer para ir de un lugar a otro. Entonces, por ejemplo, digamos que el robot está en la ubicación J, las posibles acciones que el robot puede llevar a cabo es ir a I, F o K. Y nuevamente, ya que trabajaremos con ecuaciones matemáticas, codificaremos estas acciones con los mismos índices que para los estados. Por lo tanto, siguiendo nuestro mismo ejemplo donde el robot está en la ubicación J en un momento específico, las posibles acciones que el robot puede jugar son, de acuerdo con nuestro mapeo anterior anterior: 5, 8 y 10. De hecho, el índice 5 corresponde a F, el índice 8 corresponde a I y el índice 10 corresponde a K. Por lo tanto, eventualmente, la lista total de acciones que la IA puede llevar a cabo en general es la siguiente:</p>
<p><span class="math display">\[actions = [0,1,2,3,4,5,6,7,8,9,10,11]\]</span></p>
<p>Obviamente, al estar en una ubicación específica, hay algunas acciones que el robot no puede llevar a cabo. Tomando el mismo ejemplo anterior, si el robot está en la ubicación J, puede ejecutar las acciones 5, 8 y 10, pero no puede ejecutar las otras acciones. Nos aseguraremos de especificar eso al atribuir una recompensa 0 a las acciones que no puede llevar a cabo, y una recompensa 1 a las acciones que si puede realizar. Y eso nos lleva a las recompensas.</p>
<p><strong>Definir las recompensas.</strong></p>
<p>Lo último que tenemos que hacer ahora para construir nuestro entorno es definir un sistema de recompensas. Más específicamente, tenemos que definir una función de recompensa <span class="math inline">\(R\)</span> que toma como entradas un estado <span class="math inline">\(s\)</span> y una acción <span class="math inline">\(a\)</span>, y devuelve una recompensa numérica que la IA obtendrá al llevar a cabo la acción <span class="math inline">\(a\)</span> en el estado <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[R : (\textrm{state}, \textrm{action}) \mapsto r \in \mathbb{R}\]</span></p>
<p>Entonces, ¿cómo vamos a construir esa función para nuestro caso práctico? Aquí esto es simple. Dado que hay un número discreto y finito de estados (los índices de 0 a 11), así como un número discreto y finito de acciones (mismos índices de 0 a 11), la mejor manera de construir nuestra función de recompensa R es simplemente hacer una matriz. Nuestra función de recompensa será exactamente una matriz de 12 filas y 12 columnas, donde las filas corresponden a los estados y las columnas corresponden a las acciones. De esa forma, en nuestra función $R: (s, a) r  in  $, <span class="math inline">\(s\)</span> será el índice de la fila de la matriz, <span class="math inline">\(a\)</span> será el índice de la columna de matriz, y <span class="math inline">\(r\)</span> será la celda de los índices <span class="math inline">\((s, a)\)</span> en la matriz.</p>
<p>Por lo tanto, lo único que tenemos que hacer ahora para definir nuestra función de recompensa es simplemente llenar esta matriz con las recompensas numéricas. Y como acabamos de decir en el párrafo anterior, lo que tenemos que hacer primero es atribuir, para cada una de las 12 ubicaciones, una recompensa 0 por las acciones que el robot no puede ejecutar, y una recompensa 1 por las acciones que el robot puede llevar a cabo. Al hacer eso para cada una de las 12 ubicaciones, terminaremos con una matriz de recompensas. Vamos a construirlo paso a paso, comenzando con la primera ubicación:</p>
<p><em>Ubicación A.</em></p>
<p>Cuando se encuentra en la ubicación A, el robot solo puede ir a la ubicación B. Por lo tanto, dado que la ubicación A tiene el índice 0 (primera fila de la matriz) y la ubicación B tiene el índice 1 (segunda columna de la matriz), la primera fila de la matriz de las recompensas obtendrá un 1 en la segunda columna y un 0 en todas las otras columnas, así:</p>
<p><img src="Rewards_Matrix_1.png" /></p>
<p><em>Ubicación B.</em></p>
<p>Al estar en la ubicación B, el robot solo puede ir a tres ubicaciones diferentes: A, C y F. Dado que B tiene el índice 1 (segunda fila), y A, C, F tienen los índices respectivos 0, 2, 5 (1ra, 3ra. , y sexta columna), entonces la segunda fila de la matriz de recompensas obtendrá un 1 en las columnas 1a, 3a y 6a, y 0 en todas las otras columnas. Por lo tanto obtenemos:</p>
<p><img src="Rewards_Matrix_2.png" /></p>
<p><em>Ubicación C.</em></p>
<p>Ocurre lo mismo, la ubicación C (de índice 2) solo está conectada a B y G (de índices 1 y 6), por lo que la tercera fila de la matriz de recompensas es:</p>
<p><img src="Rewards_Matrix_3.png" /></p>
<p><em>En el resto de ubicaciones…</em></p>
<p>Al hacer lo mismo para todas las demás ubicaciones, finalmente obtenemos nuestra matriz final de recompensas:</p>
<p><img src="Rewards_Matrix_4.png" /></p>
<p>Felicidades, acabamos de definir las recompensas. Lo hicimos simplemente construyendo esta matriz de recompensas. Es importante entender que esta es la forma en que definimos el sistema de recompensas cuando hacemos Q-Learning con un número finito de entradas y acciones. En el Caso Práctico 2, veremos que procederemos de manera muy diferente.</p>
<p>Ya casi hemos terminado, lo único que tenemos que hacer es atribuir grandes recompensas a las ubicaciones de mayor prioridad. Esto lo hará el sistema informático que devuelve las prioridades de recolección de productos para cada una de las 12 ubicaciones. Por lo tanto, dado que la ubicación G es la máxima prioridad, el sistema informático actualizará la matriz de recompensas atribuyendo una alta recompensa en la celda <span class="math inline">\((G, G)\)</span>:</p>
<p><img src="Rewards_Matrix_5.png" /></p>
<p>Y así es como el sistema de recompensas funcionará con Q-Learning. Atribuimos la recompensa más alta (aquí 1000) a la ubicación de máxima prioridad G. Luego puedes ver en las clases de vídeo del curso cómo podemos atribuir una recompensa más alta a la segunda ubicación de mayor prioridad (ubicación K), para hacer que nuestro robot pase por esto ubicación intermedia de máxima prioridad, optimizando así los flujos de movimiento por el almacén.</p>
</div>
</div>
<div id="solución-de-inteligencia-artificial" class="section level2">
<h2><span class="header-section-number">1.2</span> Solución de Inteligencia Artificial</h2>
<p>The AI Solution that will solve the problem described above is a Q-Learning model. Since the latter is based on Markov Decision Processes, or MDPs, we will start by explaining what they are, and then we will move on to the intuition and maths details behind the Q-Learning model.</p>
<div id="proceso-de-decisión-de-markov" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Proceso de Decisión de Markov</h3>
<p>A Markov Decision Process is a tuple <span class="math inline">\((S, A, T, R)\)</span> where:</p>

<p><br />
</p>
<p>After defining the MDP, it is now important to remind that it relies on the following assumption: the probability of the future state <span class="math inline">\(s_{t+1}\)</span> only depends on the current state <span class="math inline">\(s_t\)</span> and action <span class="math inline">\(a_t\)</span>, and doesn’t depend on any of the previous states and actions. That is:</p>
<p><span class="math display">\[\begin{equation*}
\mathbb{P}(s_{t+1}|s_0,a_0,s_1,a_1,...,s_t,a_t) = \mathbb{P}(s_{t+1}|s_t,a_t)
\end{equation*}\]</span></p>
<p><br />
</p>
<p>Hence in other words, a Markov Decision Process has no memory.</p>
<div style="page-break-after: always;"></div>
<p>Now let’s recap what is going on in terms of MDPs. At every time <span class="math inline">\(t\)</span>:</p>

<p>So now the question is:</p>
<p><span class="math display">\[\begin{equation*}
\textbf{How does the AI know which action to play at each time $t$?}
\end{equation*}\]</span></p>
<p>To answer this question, we need to introduce the policy function. The policy function <span class="math inline">\(\pi\)</span> is exactly the function that, given a state <span class="math inline">\(s_t\)</span>, returns the action <span class="math inline">\(a_t\)</span>:</p>
<p><span class="math display">\[\begin{equation*}
\pi: s_t \in S \mapsto a_t \in A
\end{equation*}\]</span></p>
<p>Let’s denote by <span class="math inline">\(\Pi\)</span> the set of all possible policy functions. Then the choice of the best actions to play becomes an optimization problem. Indeed, it comes down to finding the optimal policy <span class="math inline">\(\pi^*\)</span> that maximizes the accumulated reward:</p>
<p><span class="math display">\[\begin{equation*}
\pi^* = \underset{\pi \in \Pi}{\textrm{argmax}} \sum_{t \ge 0} R(s_t,\pi(s_t))
\end{equation*}\]</span></p>
<p>Therefore of course the question becomes:</p>
<p><span class="math display">\[\begin{equation*}
\textbf{How to find this optimal policy $\pi^*$ ?}
\end{equation*}\]</span></p>
<p>This is where Q-Learning comes into play.</p>
</div>
<div id="q-learning" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Q-Learning</h3>
<p>Before we start getting into the details of Q-Learning, we need to explain the concept of the Q-Value.</p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>To each couple of state and action <span class="math inline">\((s,a)\)</span>, we are going to associate a numeric value <span class="math inline">\(Q(s,a)\)</span>:</p>
<p><span class="math display">\[\begin{equation*}
Q: (s \in S, a \in A) \mapsto Q(s,a) \in \mathbb{R}
\end{equation*}\]</span></p>
<p>We will say that <span class="math inline">\(Q(s,a)\)</span> is “the Q-value of the action <span class="math inline">\(a\)</span> played in the state <span class="math inline">\(s\)</span>”.</p>
<p><br />
</p>
<p>To understand the purpose of this “Q-Value”, we need to introduce the Temporal Difference.</p>
<p><br />
</p>
<p></p>
<p><br />
</p>
<p>At the beginning <span class="math inline">\(t=0\)</span>, all the Q-values are initialized to 0:</p>
<p><span class="math display">\[\begin{equation*}
\forall s \in S, a \in A, Q(s,a) = 0
\end{equation*}\]</span></p>
<p><br />
</p>
<p>Now let’s suppose we are at time <span class="math inline">\(t\)</span>, in a certain state <span class="math inline">\(s_t\)</span>. We play a random action <span class="math inline">\(a_t\)</span>, which brings us to the state <span class="math inline">\(s_{t+1}\)</span> and we get the reward <span class="math inline">\(R(s_t,a_t)\)</span>.</p>
<p><br />
</p>
<p>We can now introduce the Temporal Difference, which is at the heart of Q-Learning. The Temporal Difference at time <span class="math inline">\(t\)</span>, denoted by <span class="math inline">\(TD_t(s_t,a_t)\)</span>, is the difference between:</p>

<p>thus leading to:</p>
<p><span class="math display">\[\begin{equation*}
TD_t(s_t,a_t) = R(s_t,a_t) + \gamma \underset{a}{\max}(Q(s_{t+1},a)) - Q(s_t,a_t)
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
\textbf{OK great, but what exactly is the purpose of this Temporal Difference $TD_t(s_t,a_t)$?}
\end{equation*}\]</span></p>
<p><br />
</p>
<p>Let’s answer this question to give us some better AI intuition. <span class="math inline">\(TD_t(s_t,a_t)\)</span> is like an intrinsic reward. The AI will learn the Q-values in such a way that:</p>

<p><br />
</p>
<p>To that extent, the AI will iterate some updates of the Q-Values (through an equation called the Bellman equation) towards higher temporal differences.</p>
<p><br />
</p>
<p>Accordingly, in the final next step of the Q-Learning algorithm, we use the Temporal Difference to reinforce the couples (state, action) from time <span class="math inline">\(t-1\)</span> to time <span class="math inline">\(t\)</span>, according to the following equation:</p>
<p><br />
</p>
<p><span class="math display">\[\begin{equation*}
Q_t(s_t,a_t) = Q_{t-1}(s_t,a_t) + \alpha TD_t(s_t,a_t)
\end{equation*}\]</span></p>
<p><br />
</p>
<p>where <span class="math inline">\(\alpha \in \mathbb{R}\)</span> is the learning rate, which dictates how fast the learning of the Q-Values goes, or how big the updates of the Q-Values are. Its value is usually a real number chosen between 0 and 1, like for example 0.01, 0.05, 0.1 or 0.5. The lower is its value, the smaller will be the updates of the Q-Values and the longer will be the Q-Learning. The higher is its value, the bigger will be the updates of the Q-Values and the faster will be the Q-Learning.</p>
<p><br />
</p>
<p>This equation above is the Bellman equation. It is the pillar of Q-Learning.</p>
<p><br />
</p>
<p>With this point of view, the Q-Values measure the accumulation of surprise or frustration associated with the couple of action and state <span class="math inline">\((s_t,a_t)\)</span>. In the surprise case, the AI is reinforced, and in the frustration case, the AI is weakened. Hence we want to learn the Q-Values that will give the AI the maximum “good surprise”.</p>
<p><br />
</p>
<p>Accordingly, the decision of which action to play mostly depends on the Q-value <span class="math inline">\(Q(s_t, a_t)\)</span>. If the action <span class="math inline">\(a_t\)</span> played in the state <span class="math inline">\(s_t\)</span> is associated with a high Q-Value <span class="math inline">\(Q(s_t,a_t)\)</span>, the AI will have a higher tendency to choose <span class="math inline">\(a_t\)</span>. On the other hand if the action <span class="math inline">\(a_t\)</span> played in the state <span class="math inline">\(s_t\)</span> is associated with a small Q-value <span class="math inline">\(Q(s_t, a_t)\)</span>, the AI will have a smaller tendency to choose <span class="math inline">\(a_t\)</span>.</p>
<p><br />
</p>
<p>There are several ways of choosing the best action to play. First, when being in a certain state <span class="math inline">\(s_t\)</span>, we could simply take the action <span class="math inline">\(a_t\)</span> that maximizes the Q-Value <span class="math inline">\(Q(s_t,a_t)\)</span>:</p>
<p><span class="math display">\[\begin{equation*}
a_t = \underset{a}{\textrm{argmax}}(Q(s_t,a))
\end{equation*}\]</span></p>
<p>This solution is the Argmax method.</p>
<p><br />
</p>
<p>Another great solution, which turns out to be an even better solution for complex problems, is the Softmax method.</p>
<p><br />
</p>
<p>The Softmax method consists of considering for each state <span class="math inline">\(s\)</span> the following distribution:</p>
<p><span class="math display">\[\begin{equation*}
W_s: a \in A \mapsto \frac{\exp(Q(s,a))^{\tau}}{\sum_{a&#39;}\exp(Q(s,a&#39;))^{\tau}} \textrm{ with } \tau \ge 0
\end{equation*}\]</span></p>
<p>Then we choose which action <span class="math inline">\(a\)</span> to play by taking a random draw from that distribution:</p>
<p><span class="math display">\[\begin{equation*}
a \sim W_s(.)
\end{equation*}\]</span></p>
<p>However the problem we will solve in Case Study 1 will be simple enough to use the Argmax method, so this is what we will choose.</p>
</div>
<div id="el-algoritmo-de-q-learning-al-completo" class="section level3">
<h3><span class="header-section-number">1.2.3</span> El algoritmo de Q-Learning al completo</h3>
<p>Let’s summarize the different steps of the whole Q-Learning process:</p>
<p><br />
</p>
<p>:</p>
<p><br />
</p>
<p>For all couples of states <span class="math inline">\(s\)</span> and actions <span class="math inline">\(a\)</span>, the Q-Values are initialized to 0:</p>
<p><span class="math display">\[\begin{equation*}
\forall s \in S, a \in A, Q_0(s,a) = 0
\end{equation*}\]</span></p>
<p>We start in the initial state <span class="math inline">\(s_0\)</span>. We play a random possible action and we reach the first state <span class="math inline">\(s_1\)</span>.</p>
<p><br />
</p>
<p>, we will repeat for a certain number of times (1000 times in our code) the following:</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<div id="implementación" class="section level2">
<h2><span class="header-section-number">1.3</span> Implementación</h2>
<p>Now let’s provide and explain the whole implementation of this Q-Learning model, the solution of our warehouse flows optimization problem.</p>
<p><br />
</p>
<p>First, we start by importing the libraries that will be used in this implementation. These only include the numpy library, which offers a practical way of working with arrays and mathematical operations:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># Importar las librerías</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> numpy <span class="im">as</span> np</a></code></pre></div>
<p>Then we set the parameters of our model. These include the discount factor <span class="math inline">\(\gamma\)</span> and the learning rate <span class="math inline">\(\alpha\)</span>, which as we saw in Section 1.2, are the only parameters of the Q-Learning algorithm:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co"># Setting the parameters gamma and alpha for the Q-Learning</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">gamma <span class="op">=</span> <span class="fl">0.75</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">alpha <span class="op">=</span> <span class="fl">0.9</span></a></code></pre></div>
<p>The two previous code sections were simply the introductory sections, before really starting to build our AI model. Now the next step is to start the first part of our implementation: Part 1 - Defining the Environment. And for that of course, we begin by defining the states, with a dictionary mapping the locations names (in letters from A to L) into the states (in indexes from 0 to 11):</p>
<p><br />
</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># PART 1 - DEFINING THE ENVIRONMENT</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2"></a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="co"># Defining the states</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">location_to_state <span class="op">=</span> {<span class="st">&#39;A&#39;</span>: <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">                     <span class="st">&#39;B&#39;</span>: <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb3-6" data-line-number="6">                     <span class="st">&#39;C&#39;</span>: <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">                     <span class="st">&#39;D&#39;</span>: <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">                     <span class="st">&#39;E&#39;</span>: <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">                     <span class="st">&#39;F&#39;</span>: <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb3-10" data-line-number="10">                     <span class="st">&#39;G&#39;</span>: <span class="dv">6</span>,</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">                     <span class="st">&#39;H&#39;</span>: <span class="dv">7</span>,</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">                     <span class="st">&#39;I&#39;</span>: <span class="dv">8</span>,</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">                     <span class="st">&#39;J&#39;</span>: <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb3-14" data-line-number="14">                     <span class="st">&#39;K&#39;</span>: <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">                     <span class="st">&#39;L&#39;</span>: <span class="dv">11</span>}</a></code></pre></div>
<p>Then we define the actions, with a simple list of indexes from 0 to 11. Remember that each action index corresponds to the next state (next location) where that action leads to:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># Defining the actions</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">actions <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>]</a></code></pre></div>
<p>And eventually, we define the rewards, by creating a matrix of rewards, where the rows correspond to the current states <span class="math inline">\(s_t\)</span>, the columns correspond to the actions <span class="math inline">\(a_t\)</span> leading to the next state <span class="math inline">\(s_{t+1}\)</span>, and the cells contain the rewards <span class="math inline">\(R(s_t,a_t)\)</span>. If a cell <span class="math inline">\((s_t,a_t)\)</span> has a 1, that means that we can play the action <span class="math inline">\(a_t\)</span> from the current state <span class="math inline">\(s_t\)</span> to reach the next state <span class="math inline">\(s_{t+1}\)</span>. If a cell <span class="math inline">\((s_t,a_t)\)</span> has a 0, that means that we cannot play the action <span class="math inline">\(a_t\)</span> from the current state <span class="math inline">\(s_t\)</span> to reach any next state <span class="math inline">\(s_{t+1}\)</span>. And for now we will manually put a high reward (1000) inside the cell corresponding to location G, because it is the top priority location where the autonomous warehouse has to go to collect the products. Since location G has encoded index state 6, we put a 1000 reward on the cell of row 6 and column 6. Then later on we will improve our solution by implementing an automatic way of going to the top priority location, without having to manually update the matrix of rewards and leaving it initialized with 0s and 1s just as it should be. But in the meantime, here is below our matrix of rewards including the manual update:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co"># Defining the rewards</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-4" data-line-number="4">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-8" data-line-number="8">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1000</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-9" data-line-number="9">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a></code></pre></div>
<p>That closes this first part. Now let’s begin the second part of our implementation: Part 2 - Building the AI Solution with Q-Learning. To that extent, we are going to follow the Q-Learning algorithm exactly as it was provided in Section 1.2. Hence we first initialize all the Q-Values, by creating our matrix of Q-Values full of zeros (in which same, the rows correspond to the current states <span class="math inline">\(s_t\)</span>, the columns correspond to the actions <span class="math inline">\(a_t\)</span> leading to the next state <span class="math inline">\(s_{t+1}\)</span>, and the cells contain the Q-Values <span class="math inline">\(Q(s_t,a_t))\)</span>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co"># PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="co"># Initializing the Q-Values</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4">Q <span class="op">=</span> np.array(np.zeros([<span class="dv">12</span>,<span class="dv">12</span>]))</a></code></pre></div>
<p>Then of course we implement the Q-Learning process, with a for loop over 1000 iterations, repeating 1000 times the steps of the Q-Learning process provided at the end of Section 1.2:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co"># Implementing the Q-Learning process</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">    current_state <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb7-4" data-line-number="4">    playable_actions <span class="op">=</span> []</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>):</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">        <span class="cf">if</span> R[current_state, j] <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb7-7" data-line-number="7">            playable_actions.append(j)</a>
<a class="sourceLine" id="cb7-8" data-line-number="8">    next_state <span class="op">=</span> np.random.choice(playable_actions)</a>
<a class="sourceLine" id="cb7-9" data-line-number="9">    TD <span class="op">=</span> R[current_state, next_state] <span class="op">+</span> gamma<span class="op">*</span>Q[next_state, np.argmax(Q[next_state,])]</a>
<a class="sourceLine" id="cb7-10" data-line-number="10">         <span class="op">-</span> Q[current_state, next_state]</a>
<a class="sourceLine" id="cb7-11" data-line-number="11">    Q[current_state, next_state] <span class="op">=</span> Q[current_state, next_state] <span class="op">+</span> alpha<span class="op">*</span>TD</a></code></pre></div>
<p>Optional: at this stage of the code, our matrix of Q-Values is ready. We can have a look at it by executing the whole code we have implemented so far, and by entering the following print in the console:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="bu">print</span>(<span class="st">&quot;Q-Values:&quot;</span>)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="bu">print</span>(Q.astype(<span class="bu">int</span>))</a></code></pre></div>
<p>And we obtain the following matrix of final Q-Values:</p>
<p><img src="Q_Values_Console.png" /></p>
<p>For more visual clarity, you can even check the matrix of Q-Values directly in Variable Explorer, by double clicking on Q. Then to get the Q-Values as integers you can click on “Format” and inside enter a float formatting of “%.0f”. You will obtain this, which is a bit more clear since you can see the indexes of the rows and columns in your matrix Q:</p>
<p><img src="Q_Values_Variable_Explorer.png" /></p>
<p>Good, now that we have our matrix of Q-Values, we are ready to go into production! Hence we can move on to the third part of the implementation, Part 3 - Going into Production, inside which we will compute the optimal path from any starting location to any ending top priority location. The idea here will be to implement a “route” function, that will take as inputs the starting location where our autonomous warehouse robot is located at a specific time and the ending location where it has to go in top priority, and that will return as output the shortest route inside a list. However since we want to input the locations with there names (in letters), as opposed to their states (in indexes), we will need a dictionary that maps the locations states (in indexes) to the locations names (in letters). And that is the first thing we will do here in this third part, using a trick to inverse our previous dictionary “location_to_state”, since indeed we simply want to get the exact inverse mapping from this dictionary:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co"># PART 3 - GOING INTO PRODUCTION</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2"></a>
<a class="sourceLine" id="cb9-3" data-line-number="3"><span class="co"># Making a mapping from the states to the locations</span></a>
<a class="sourceLine" id="cb9-4" data-line-number="4">state_to_location <span class="op">=</span> {state: location <span class="cf">for</span> location, state <span class="kw">in</span> location_to_state.items()}</a></code></pre></div>
<p>This is when the most important code section comes into play. We are about to implement the final “route()” function that will take as inputs the starting and ending locations, and that will return the optimal path between these two locations. To explain exactly what this route function will do, let’s enumerate the different steps of the process, when going from location E to location G:</p>
<ol style="list-style-type: decimal">
<li>We start at our starting location E.</li>
<li>We get the state of location E, which according to our location_to_state mapping is <span class="math inline">\(s_0 = 4\)</span>.</li>
<li>On the row of index <span class="math inline">\(s_0 = 4\)</span> in our matrix of Q-Values, we find the column that has the maximum Q-Value (703).</li>
<li>This column has index 8, so we play the action of index 8 which leads us to the next state <span class="math inline">\(s_{t+1} = 8\)</span>.</li>
<li>We get the location of state 8, which according to our state_to_location mapping is location I. Hence our next location is location I, which is appended to our list containing the optimal path.</li>
<li>We repeat the same previous 5-steps from our new starting location I, until we reach our final destination, location G.</li>
</ol>
<p>Hence, since we don’t know how many locations we will have to go through between the starting and ending locations, we have to make a while loop that will repeat the 5-steps process described above, and that will stop as soon as we reach the ending top priority location:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co"># Making the final function that will return the optimal route</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="kw">def</span> route(starting_location, ending_location):</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">    route <span class="op">=</span> [starting_location]</a>
<a class="sourceLine" id="cb10-4" data-line-number="4">    next_location <span class="op">=</span> starting_location</a>
<a class="sourceLine" id="cb10-5" data-line-number="5">    <span class="cf">while</span> (next_location <span class="op">!=</span> ending_location):</a>
<a class="sourceLine" id="cb10-6" data-line-number="6">        starting_state <span class="op">=</span> location_to_state[starting_location]</a>
<a class="sourceLine" id="cb10-7" data-line-number="7">        next_state <span class="op">=</span> np.argmax(Q[starting_state,])</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">        next_location <span class="op">=</span> state_to_location[next_state]</a>
<a class="sourceLine" id="cb10-9" data-line-number="9">        route.append(next_location)</a>
<a class="sourceLine" id="cb10-10" data-line-number="10">        starting_location <span class="op">=</span> next_location</a>
<a class="sourceLine" id="cb10-11" data-line-number="11">    <span class="cf">return</span> route</a></code></pre></div>
<p>Congratulations, our tool is now ready! When we test it to go from E to G, we get indeed the two possible optimal paths after printing the final route executing the whole code several times:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="co"># Printing the final route</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="bu">print</span>(<span class="st">&#39;Route:&#39;</span>)</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">route(<span class="st">&#39;E&#39;</span>, <span class="st">&#39;G&#39;</span>)</a>
<a class="sourceLine" id="cb11-4" data-line-number="4"></a>
<a class="sourceLine" id="cb11-5" data-line-number="5">Route:</a>
<a class="sourceLine" id="cb11-6" data-line-number="6">Out[<span class="dv">1</span>]: [<span class="st">&#39;E&#39;</span>, <span class="st">&#39;I&#39;</span>, <span class="st">&#39;J&#39;</span>, <span class="st">&#39;F&#39;</span>, <span class="st">&#39;B&#39;</span>, <span class="st">&#39;C&#39;</span>, <span class="st">&#39;G&#39;</span>]</a>
<a class="sourceLine" id="cb11-7" data-line-number="7">Out[<span class="dv">2</span>]: [<span class="st">&#39;E&#39;</span>, <span class="st">&#39;I&#39;</span>, <span class="st">&#39;J&#39;</span>, <span class="st">&#39;K&#39;</span>, <span class="st">&#39;L&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;G&#39;</span>]</a></code></pre></div>
<p>Good, we have a first version of the model that is well functioning. But we can improve it in two ways. First, by automating the reward attribution to the top priority location, so that we don’t have to do it manually. And second, by adding a feature that gives us the option to go by an intermediary location before going to the top priority location. That intermediary location should be of course in the Top 3 priority locations. And as a matter of fact, in our top priority locations ranking, the second top priority location is location K. Therefore, in order to optimize even more the warehouse flows, our autonomous warehouse robot must go by location K to collect the products on its way to the top priority location G. A way to do this is to have the option to go by any intermediary location in the process of our “route()” function. And this is exactly what we will implement as a second improvement. But first, let’s implement the first improvement, that automates the reward attribution.</p>
<p>The way to do that is two folds: first we must make a copy (called R_new) of our reward matrix inside which the route() function will automatically update the reward in the cell of the ending location. Indeed, the ending location is one of the inputs of the route() function, so using our location_to_state dictionary we can very easily find that cell and update its reward to 1000. And second, we must include the whole Q-Learning algorithm (including the initialization step) inside the route function, right after we make that update of the reward in our copy of the rewards matrix. Indeed, in our previous implementation above, the Q-Learning process happens on the original version of the rewards matrix, which is now supposed to stay as it is, i.e. initialized to 1s and 0s only. Therefore we must include the Q-Learning process inside the route function, and make it happen on our copy R_new of the rewards matrix, instead of the original rewards matrix R. Hence, our full implementation becomes the following:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co"># Artificial Intelligence for Business</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2"><span class="co"># Optimizing Warehouse Flows with Q-Learning</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3"></a>
<a class="sourceLine" id="cb12-4" data-line-number="4"><span class="co"># Importing the libraries</span></a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb12-6" data-line-number="6"></a>
<a class="sourceLine" id="cb12-7" data-line-number="7"><span class="co"># Setting the parameters gamma and alpha for the Q-Learning</span></a>
<a class="sourceLine" id="cb12-8" data-line-number="8">gamma <span class="op">=</span> <span class="fl">0.75</span></a>
<a class="sourceLine" id="cb12-9" data-line-number="9">alpha <span class="op">=</span> <span class="fl">0.9</span></a>
<a class="sourceLine" id="cb12-10" data-line-number="10"></a>
<a class="sourceLine" id="cb12-11" data-line-number="11"><span class="co"># PART 1 - DEFINING THE ENVIRONMENT</span></a>
<a class="sourceLine" id="cb12-12" data-line-number="12"></a>
<a class="sourceLine" id="cb12-13" data-line-number="13"><span class="co"># Defining the states</span></a>
<a class="sourceLine" id="cb12-14" data-line-number="14">location_to_state <span class="op">=</span> {<span class="st">&#39;A&#39;</span>: <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb12-15" data-line-number="15">                     <span class="st">&#39;B&#39;</span>: <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb12-16" data-line-number="16">                     <span class="st">&#39;C&#39;</span>: <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb12-17" data-line-number="17">                     <span class="st">&#39;D&#39;</span>: <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb12-18" data-line-number="18">                     <span class="st">&#39;E&#39;</span>: <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb12-19" data-line-number="19">                     <span class="st">&#39;F&#39;</span>: <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb12-20" data-line-number="20">                     <span class="st">&#39;G&#39;</span>: <span class="dv">6</span>,</a>
<a class="sourceLine" id="cb12-21" data-line-number="21">                     <span class="st">&#39;H&#39;</span>: <span class="dv">7</span>,</a>
<a class="sourceLine" id="cb12-22" data-line-number="22">                     <span class="st">&#39;I&#39;</span>: <span class="dv">8</span>,</a>
<a class="sourceLine" id="cb12-23" data-line-number="23">                     <span class="st">&#39;J&#39;</span>: <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb12-24" data-line-number="24">                     <span class="st">&#39;K&#39;</span>: <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb12-25" data-line-number="25">                     <span class="st">&#39;L&#39;</span>: <span class="dv">11</span>}</a>
<a class="sourceLine" id="cb12-26" data-line-number="26"></a>
<a class="sourceLine" id="cb12-27" data-line-number="27"><span class="co"># Defining the actions</span></a>
<a class="sourceLine" id="cb12-28" data-line-number="28">actions <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>]</a>
<a class="sourceLine" id="cb12-29" data-line-number="29"></a>
<a class="sourceLine" id="cb12-30" data-line-number="30"><span class="co"># Defining the rewards</span></a>
<a class="sourceLine" id="cb12-31" data-line-number="31">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-32" data-line-number="32">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-33" data-line-number="33">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-34" data-line-number="34">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-35" data-line-number="35">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-36" data-line-number="36">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-37" data-line-number="37">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-38" data-line-number="38">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb12-39" data-line-number="39">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-40" data-line-number="40">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb12-41" data-line-number="41">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb12-42" data-line-number="42">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a>
<a class="sourceLine" id="cb12-43" data-line-number="43"></a>
<a class="sourceLine" id="cb12-44" data-line-number="44"><span class="co"># PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING</span></a>
<a class="sourceLine" id="cb12-45" data-line-number="45"></a>
<a class="sourceLine" id="cb12-46" data-line-number="46"><span class="co"># Making a mapping from the states to the locations</span></a>
<a class="sourceLine" id="cb12-47" data-line-number="47">state_to_location <span class="op">=</span> {state: location <span class="cf">for</span> location, state <span class="kw">in</span> location_to_state.items()}</a>
<a class="sourceLine" id="cb12-48" data-line-number="48"></a>
<a class="sourceLine" id="cb12-49" data-line-number="49"><span class="co"># Making the final function that will return the route</span></a>
<a class="sourceLine" id="cb12-50" data-line-number="50"><span class="kw">def</span> route(starting_location, ending_location):</a>
<a class="sourceLine" id="cb12-51" data-line-number="51">    R_new <span class="op">=</span> np.copy(R)</a>
<a class="sourceLine" id="cb12-52" data-line-number="52">    ending_state <span class="op">=</span> location_to_state[ending_location]</a>
<a class="sourceLine" id="cb12-53" data-line-number="53">    R_new[ending_state, ending_state] <span class="op">=</span> <span class="dv">1000</span></a>
<a class="sourceLine" id="cb12-54" data-line-number="54">    Q <span class="op">=</span> np.array(np.zeros([<span class="dv">12</span>,<span class="dv">12</span>]))</a>
<a class="sourceLine" id="cb12-55" data-line-number="55">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb12-56" data-line-number="56">        current_state <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb12-57" data-line-number="57">        playable_actions <span class="op">=</span> []</a>
<a class="sourceLine" id="cb12-58" data-line-number="58">        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>):</a>
<a class="sourceLine" id="cb12-59" data-line-number="59">            <span class="cf">if</span> R_new[current_state, j] <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb12-60" data-line-number="60">                playable_actions.append(j)</a>
<a class="sourceLine" id="cb12-61" data-line-number="61">        next_state <span class="op">=</span> np.random.choice(playable_actions)</a>
<a class="sourceLine" id="cb12-62" data-line-number="62">        TD <span class="op">=</span> R_new[current_state, next_state]</a>
<a class="sourceLine" id="cb12-63" data-line-number="63">             <span class="op">+</span> gamma <span class="op">*</span> Q[next_state, np.argmax(Q[next_state,])]</a>
<a class="sourceLine" id="cb12-64" data-line-number="64">             <span class="op">-</span> Q[current_state, next_state]</a>
<a class="sourceLine" id="cb12-65" data-line-number="65">        Q[current_state, next_state] <span class="op">=</span> Q[current_state, next_state] <span class="op">+</span> alpha <span class="op">*</span> TD</a>
<a class="sourceLine" id="cb12-66" data-line-number="66">    route <span class="op">=</span> [starting_location]</a>
<a class="sourceLine" id="cb12-67" data-line-number="67">    next_location <span class="op">=</span> starting_location</a>
<a class="sourceLine" id="cb12-68" data-line-number="68">    <span class="cf">while</span> (next_location <span class="op">!=</span> ending_location):</a>
<a class="sourceLine" id="cb12-69" data-line-number="69">        starting_state <span class="op">=</span> location_to_state[starting_location]</a>
<a class="sourceLine" id="cb12-70" data-line-number="70">        next_state <span class="op">=</span> np.argmax(Q[starting_state,])</a>
<a class="sourceLine" id="cb12-71" data-line-number="71">        next_location <span class="op">=</span> state_to_location[next_state]</a>
<a class="sourceLine" id="cb12-72" data-line-number="72">        route.append(next_location)</a>
<a class="sourceLine" id="cb12-73" data-line-number="73">        starting_location <span class="op">=</span> next_location</a>
<a class="sourceLine" id="cb12-74" data-line-number="74">    <span class="cf">return</span> route</a>
<a class="sourceLine" id="cb12-75" data-line-number="75"></a>
<a class="sourceLine" id="cb12-76" data-line-number="76"><span class="co"># PART 3 - GOING INTO PRODUCTION</span></a>
<a class="sourceLine" id="cb12-77" data-line-number="77"></a>
<a class="sourceLine" id="cb12-78" data-line-number="78"><span class="co"># Printing the final route</span></a>
<a class="sourceLine" id="cb12-79" data-line-number="79"><span class="bu">print</span>(<span class="st">&#39;Route:&#39;</span>)</a>
<a class="sourceLine" id="cb12-80" data-line-number="80">route(<span class="st">&#39;E&#39;</span>, <span class="st">&#39;G&#39;</span>)</a></code></pre></div>
<p>By executing this new code several times, we get of course the same two possible optimal paths as before.</p>
<p>Now let’s tackle the second improvement. There are three ways to add the option of going by the intermediary location K, the second top priority location:</p>
<ol style="list-style-type: decimal">
<li>We give a high reward to the action leading from location J to location K. This high reward has to be larger than 1, and below 1000. Indeed it has to be larger than 1 so that the Q-Learning process favors the action leading from J to K, as opposed to the action leading from J to F which has reward 1. And it must be below than 1000 so we have to keep the highest reward on the top priority location, to make sure we end up there. Hence for example, in our rewards matrix we can give a high reward of 500 to the cell in the row of index 9 and the column of index 10, since indeed that cell corresponds to the action leading from location J (state index 9) to location K (state index 10). That way our autonomous warehouse robot will always go by location K on its way to location G. Here is how the matrix of rewards would be in that case:</li>
</ol>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1">    <span class="co"># Defining the rewards</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2">    R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-3" data-line-number="3">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-4" data-line-number="4">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-5" data-line-number="5">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-6" data-line-number="6">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-7" data-line-number="7">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-8" data-line-number="8">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-9" data-line-number="9">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb13-10" data-line-number="10">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-11" data-line-number="11">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">500</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb13-12" data-line-number="12">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb13-13" data-line-number="13">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>We give a bad reward to the action leading from location J to location F. This bad reward just has to be below 0. Indeed by punishing this action with a bad reward the Q-Learning process will never favor that action leading from J to F. Hence for example, in our rewards matrix we can give a bad reward of -500 to the cell in the row of index 9 and the column of index 5, since indeed that cell corresponds to the action leading from location J (state index 9) to location F (state index 5). That way our autonomous warehouse robot will never go trough location F on its way to location G. Here is how the matrix of rewards would be in that case:</li>
</ol>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">    <span class="co"># Defining the rewards</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2">    R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-5" data-line-number="5">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-7" data-line-number="7">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-8" data-line-number="8">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-9" data-line-number="9">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb14-10" data-line-number="10">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-11" data-line-number="11">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">500</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb14-12" data-line-number="12">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb14-13" data-line-number="13">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>We make an additional best_route() function, taking as inputs the three starting, intermediary and ending locations, that will call our previous route() function twice, a first time from the starting location to the intermediary location, and a second time from the intermediary location to the ending location.</li>
</ol>
<p>The first two ideas are easy to implement manually, but very tricky to implement automatically. Indeed, it is easy to find automatically the index of the intermediary location where we want to go by, but very difficult to get the index of the location that leads to that intermediary location, since it depends on the starting location and the ending location. You can try to implement either the first or second idea, you will see what I mean. Accordingly, we will implement the third idea, which can be coded in just two extra lines of code:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="co"># Making the final function that returns the optimal route</span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="kw">def</span> best_route(starting_location, intermediary_location, ending_location):</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">    <span class="cf">return</span> route(starting_location, intermediary_location)</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">           <span class="op">+</span> route(intermediary_location, ending_location)[<span class="dv">1</span>:]</a></code></pre></div>
<p>Eventually, the final code including that major improvement for our warehouse flows optimization, becomes:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># Artificial Intelligence for Business</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2"><span class="co"># Optimizing Warehouse Flows with Q-Learning</span></a>
<a class="sourceLine" id="cb16-3" data-line-number="3"></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"><span class="co"># Importing the libraries</span></a>
<a class="sourceLine" id="cb16-5" data-line-number="5"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb16-6" data-line-number="6"></a>
<a class="sourceLine" id="cb16-7" data-line-number="7"><span class="co"># Setting the parameters gamma and alpha for the Q-Learning</span></a>
<a class="sourceLine" id="cb16-8" data-line-number="8">gamma <span class="op">=</span> <span class="fl">0.75</span></a>
<a class="sourceLine" id="cb16-9" data-line-number="9">alpha <span class="op">=</span> <span class="fl">0.9</span></a>
<a class="sourceLine" id="cb16-10" data-line-number="10"></a>
<a class="sourceLine" id="cb16-11" data-line-number="11"><span class="co"># PART 1 - DEFINING THE ENVIRONMENT</span></a>
<a class="sourceLine" id="cb16-12" data-line-number="12"></a>
<a class="sourceLine" id="cb16-13" data-line-number="13"><span class="co"># Defining the states</span></a>
<a class="sourceLine" id="cb16-14" data-line-number="14">location_to_state <span class="op">=</span> {<span class="st">&#39;A&#39;</span>: <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb16-15" data-line-number="15">                     <span class="st">&#39;B&#39;</span>: <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb16-16" data-line-number="16">                     <span class="st">&#39;C&#39;</span>: <span class="dv">2</span>,</a>
<a class="sourceLine" id="cb16-17" data-line-number="17">                     <span class="st">&#39;D&#39;</span>: <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb16-18" data-line-number="18">                     <span class="st">&#39;E&#39;</span>: <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb16-19" data-line-number="19">                     <span class="st">&#39;F&#39;</span>: <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb16-20" data-line-number="20">                     <span class="st">&#39;G&#39;</span>: <span class="dv">6</span>,</a>
<a class="sourceLine" id="cb16-21" data-line-number="21">                     <span class="st">&#39;H&#39;</span>: <span class="dv">7</span>,</a>
<a class="sourceLine" id="cb16-22" data-line-number="22">                     <span class="st">&#39;I&#39;</span>: <span class="dv">8</span>,</a>
<a class="sourceLine" id="cb16-23" data-line-number="23">                     <span class="st">&#39;J&#39;</span>: <span class="dv">9</span>,</a>
<a class="sourceLine" id="cb16-24" data-line-number="24">                     <span class="st">&#39;K&#39;</span>: <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb16-25" data-line-number="25">                     <span class="st">&#39;L&#39;</span>: <span class="dv">11</span>}</a>
<a class="sourceLine" id="cb16-26" data-line-number="26"></a>
<a class="sourceLine" id="cb16-27" data-line-number="27"><span class="co"># Defining the actions</span></a>
<a class="sourceLine" id="cb16-28" data-line-number="28">actions <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>]</a>
<a class="sourceLine" id="cb16-29" data-line-number="29"></a>
<a class="sourceLine" id="cb16-30" data-line-number="30"><span class="co"># Defining the rewards</span></a>
<a class="sourceLine" id="cb16-31" data-line-number="31">R <span class="op">=</span> np.array([[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-32" data-line-number="32">              [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-33" data-line-number="33">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-34" data-line-number="34">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-35" data-line-number="35">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-36" data-line-number="36">              [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-37" data-line-number="37">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-38" data-line-number="38">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb16-39" data-line-number="39">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-40" data-line-number="40">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>],</a>
<a class="sourceLine" id="cb16-41" data-line-number="41">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>],</a>
<a class="sourceLine" id="cb16-42" data-line-number="42">              [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]])</a>
<a class="sourceLine" id="cb16-43" data-line-number="43"></a>
<a class="sourceLine" id="cb16-44" data-line-number="44"><span class="co"># PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING</span></a>
<a class="sourceLine" id="cb16-45" data-line-number="45"></a>
<a class="sourceLine" id="cb16-46" data-line-number="46"><span class="co"># Making a mapping from the states to the locations</span></a>
<a class="sourceLine" id="cb16-47" data-line-number="47">state_to_location <span class="op">=</span> {state: location <span class="cf">for</span> location, state <span class="kw">in</span> location_to_state.items()}</a>
<a class="sourceLine" id="cb16-48" data-line-number="48"></a>
<a class="sourceLine" id="cb16-49" data-line-number="49"><span class="co"># Making a function that returns the shortest route from a starting to ending location</span></a>
<a class="sourceLine" id="cb16-50" data-line-number="50"><span class="kw">def</span> route(starting_location, ending_location):</a>
<a class="sourceLine" id="cb16-51" data-line-number="51">    R_new <span class="op">=</span> np.copy(R)</a>
<a class="sourceLine" id="cb16-52" data-line-number="52">    ending_state <span class="op">=</span> location_to_state[ending_location]</a>
<a class="sourceLine" id="cb16-53" data-line-number="53">    R_new[ending_state, ending_state] <span class="op">=</span> <span class="dv">1000</span></a>
<a class="sourceLine" id="cb16-54" data-line-number="54">    Q <span class="op">=</span> np.array(np.zeros([<span class="dv">12</span>,<span class="dv">12</span>]))</a>
<a class="sourceLine" id="cb16-55" data-line-number="55">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb16-56" data-line-number="56">        current_state <span class="op">=</span> np.random.randint(<span class="dv">0</span>,<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb16-57" data-line-number="57">        playable_actions <span class="op">=</span> []</a>
<a class="sourceLine" id="cb16-58" data-line-number="58">        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>):</a>
<a class="sourceLine" id="cb16-59" data-line-number="59">            <span class="cf">if</span> R_new[current_state, j] <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb16-60" data-line-number="60">                playable_actions.append(j)</a>
<a class="sourceLine" id="cb16-61" data-line-number="61">        next_state <span class="op">=</span> np.random.choice(playable_actions)</a>
<a class="sourceLine" id="cb16-62" data-line-number="62">        TD <span class="op">=</span> R_new[current_state, next_state]</a>
<a class="sourceLine" id="cb16-63" data-line-number="63">             <span class="op">+</span> gamma <span class="op">*</span> Q[next_state, np.argmax(Q[next_state,])]</a>
<a class="sourceLine" id="cb16-64" data-line-number="64">             <span class="op">-</span> Q[current_state, next_state]</a>
<a class="sourceLine" id="cb16-65" data-line-number="65">        Q[current_state, next_state] <span class="op">=</span> Q[current_state, next_state] <span class="op">+</span> alpha <span class="op">*</span> TD</a>
<a class="sourceLine" id="cb16-66" data-line-number="66">    route <span class="op">=</span> [starting_location]</a>
<a class="sourceLine" id="cb16-67" data-line-number="67">    next_location <span class="op">=</span> starting_location</a>
<a class="sourceLine" id="cb16-68" data-line-number="68">    <span class="cf">while</span> (next_location <span class="op">!=</span> ending_location):</a>
<a class="sourceLine" id="cb16-69" data-line-number="69">        starting_state <span class="op">=</span> location_to_state[starting_location]</a>
<a class="sourceLine" id="cb16-70" data-line-number="70">        next_state <span class="op">=</span> np.argmax(Q[starting_state,])</a>
<a class="sourceLine" id="cb16-71" data-line-number="71">        next_location <span class="op">=</span> state_to_location[next_state]</a>
<a class="sourceLine" id="cb16-72" data-line-number="72">        route.append(next_location)</a>
<a class="sourceLine" id="cb16-73" data-line-number="73">        starting_location <span class="op">=</span> next_location</a>
<a class="sourceLine" id="cb16-74" data-line-number="74">    <span class="cf">return</span> route</a>
<a class="sourceLine" id="cb16-75" data-line-number="75"></a>
<a class="sourceLine" id="cb16-76" data-line-number="76"><span class="co"># PART 3 - GOING INTO PRODUCTION</span></a>
<a class="sourceLine" id="cb16-77" data-line-number="77"></a>
<a class="sourceLine" id="cb16-78" data-line-number="78"><span class="co"># Making the final function that returns the optimal route</span></a>
<a class="sourceLine" id="cb16-79" data-line-number="79"><span class="kw">def</span> best_route(starting_location, intermediary_location, ending_location):</a>
<a class="sourceLine" id="cb16-80" data-line-number="80">    <span class="cf">return</span> route(starting_location, intermediary_location)</a>
<a class="sourceLine" id="cb16-81" data-line-number="81">           <span class="op">+</span> route(intermediary_location, ending_location)[<span class="dv">1</span>:]</a>
<a class="sourceLine" id="cb16-82" data-line-number="82"></a>
<a class="sourceLine" id="cb16-83" data-line-number="83"><span class="co"># Printing the final route</span></a>
<a class="sourceLine" id="cb16-84" data-line-number="84"><span class="bu">print</span>(<span class="st">&#39;Route:&#39;</span>)</a>
<a class="sourceLine" id="cb16-85" data-line-number="85">best_route(<span class="st">&#39;E&#39;</span>, <span class="st">&#39;K&#39;</span>, <span class="st">&#39;G&#39;</span>)</a></code></pre></div>
<p>By executing this whole new code as many times as we want, we will always get the same expected output:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" data-line-number="1">Best Route:</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">Out[<span class="dv">1</span>]: [<span class="st">&#39;E&#39;</span>, <span class="st">&#39;I&#39;</span>, <span class="st">&#39;J&#39;</span>, <span class="st">&#39;K&#39;</span>, <span class="st">&#39;L&#39;</span>, <span class="st">&#39;H&#39;</span>, <span class="st">&#39;G&#39;</span>]</a></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="part-2-minimizing-costs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/joanby/ia4business/edit/master/1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["curso-ia-business-udemy.pdf", "curso-ia-business-udemy.epub"],
"toc": {
"collapse": "subsection"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
