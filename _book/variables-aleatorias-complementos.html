<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Tema 4 Variables Aleatorias. Complementos | Probabilidad y variables aleatorias para ML con R y Python</title>
  <meta name="description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Tema 4 Variables Aleatorias. Complementos | Probabilidad y variables aleatorias para ML con R y Python" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://www.udemy.com/course/probabilidad-y-variables-aleatorias-para-ml-con-r-y-python/?couponCode=B85F8D52148DF5AAD8F7" />
  <meta property="og:image" content="https://www.udemy.com/course/probabilidad-y-variables-aleatorias-para-ml-con-r-y-python/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />
  <meta property="og:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="github-repo" content="https://github.com/joanby/probabilidad" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Tema 4 Variables Aleatorias. Complementos | Probabilidad y variables aleatorias para ML con R y Python" />
  
  <meta name="twitter:description" content="Asienta las bases para convertirte en el Data Scientist del futuro con todo el contenido de estadística descriptiva del curso. En particular verás los mismos contenidos que explicamos en primero de carrera a matemáticos, ingenieros, economistas, biólogos, médicos o informáticos." />
  <meta name="twitter:image" content="https://www.udemy.com/course/probabilidad-y-variables-aleatorias-para-ml-con-r-y-python/?couponCode=B85F8D52148DF5AAD8F7Images/cover.jpg" />

<meta name="author" content="Ricardo Alberich, Juan Gabriel Gomila y Arnau Mir" />


<meta name="date" content="2019-12-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="Images/apple-icon-120x120.png" />
  <link rel="shortcut icon" href="Images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="distribuciones-notables.html"/>
<link rel="next" href="vectores-aleatorios-bidimensionales.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Curso completo de Probabilidad y Variables Aleatorias con R y Python</a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><i class="fa fa-check"></i>Pre requisitos: Teoría de conjuntos y combinatoria</a><ul>
<li class="chapter" data-level="0.1" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#teoría-de-conjuntos"><i class="fa fa-check"></i><b>0.1</b> Teoría de conjuntos</a><ul>
<li class="chapter" data-level="0.1.1" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#conjuntos-básicos"><i class="fa fa-check"></i><b>0.1.1</b> Conjuntos básicos</a></li>
<li class="chapter" data-level="0.1.2" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#características-y-propiedades-básicas-de-los-conjuntos"><i class="fa fa-check"></i><b>0.1.2</b> Características y propiedades básicas de los conjuntos</a></li>
<li class="chapter" data-level="0.1.3" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#operaciones-con-conjuntos"><i class="fa fa-check"></i><b>0.1.3</b> Operaciones con conjuntos</a></li>
<li class="chapter" data-level="0.1.4" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#más-propiedades-y-definiciones"><i class="fa fa-check"></i><b>0.1.4</b> Más propiedades y definiciones</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#combinatoria"><i class="fa fa-check"></i><b>0.2</b> Combinatoria</a><ul>
<li class="chapter" data-level="0.2.1" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#número-binomial."><i class="fa fa-check"></i><b>0.2.1</b> Número binomial.</a></li>
<li class="chapter" data-level="0.2.2" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#variaciones."><i class="fa fa-check"></i><b>0.2.2</b> Variaciones.</a></li>
<li class="chapter" data-level="0.2.3" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#variaciones-con-repetición."><i class="fa fa-check"></i><b>0.2.3</b> Variaciones con repetición.</a></li>
<li class="chapter" data-level="0.2.4" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#permutaciones"><i class="fa fa-check"></i><b>0.2.4</b> Permutaciones</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html"><a href="pre-requisitos-teoría-de-conjuntos-y-combinatoria.html#para-acabar"><i class="fa fa-check"></i><b>0.3</b> Para acabar</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="probabilidad.html"><a href="probabilidad.html"><i class="fa fa-check"></i><b>1</b> Probabilidad</a><ul>
<li class="chapter" data-level="1.1" data-path="probabilidad.html"><a href="probabilidad.html#probabilidades-básicas"><i class="fa fa-check"></i><b>1.1</b> Probabilidades Básicas</a><ul>
<li class="chapter" data-level="1.1.1" data-path="probabilidad.html"><a href="probabilidad.html#operaciones-con-sucesos"><i class="fa fa-check"></i><b>1.1.1</b> Operaciones con sucesos</a></li>
<li class="chapter" data-level="1.1.2" data-path="probabilidad.html"><a href="probabilidad.html#propiedades"><i class="fa fa-check"></i><b>1.1.2</b> Propiedades</a></li>
<li class="chapter" data-level="1.1.3" data-path="probabilidad.html"><a href="probabilidad.html#definición-de-probabilidad"><i class="fa fa-check"></i><b>1.1.3</b> Definición de probabilidad</a></li>
<li class="chapter" data-level="1.1.4" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-1"><i class="fa fa-check"></i><b>1.1.4</b> Propiedades</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probabilidad.html"><a href="probabilidad.html#probabilidad-condicionada"><i class="fa fa-check"></i><b>1.2</b> Probabilidad condicionada</a><ul>
<li class="chapter" data-level="1.2.1" data-path="probabilidad.html"><a href="probabilidad.html#atención"><i class="fa fa-check"></i><b>1.2.1</b> ¡Atención!</a></li>
<li class="chapter" data-level="1.2.2" data-path="probabilidad.html"><a href="probabilidad.html#propiedades-2"><i class="fa fa-check"></i><b>1.2.2</b> Propiedades</a></li>
<li class="chapter" data-level="1.2.3" data-path="probabilidad.html"><a href="probabilidad.html#teorema-de-la-probabilidad-total"><i class="fa fa-check"></i><b>1.2.3</b> Teorema de la probabilidad total</a></li>
<li class="chapter" data-level="1.2.4" data-path="probabilidad.html"><a href="probabilidad.html#clasificación-o-diagnósticos"><i class="fa fa-check"></i><b>1.2.4</b> Clasificación o Diagnósticos</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probabilidad.html"><a href="probabilidad.html#bayes"><i class="fa fa-check"></i><b>1.3</b> Bayes</a><ul>
<li class="chapter" data-level="1.3.1" data-path="probabilidad.html"><a href="probabilidad.html#fórmula-de-bayes"><i class="fa fa-check"></i><b>1.3.1</b> Fórmula de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probabilidad.html"><a href="probabilidad.html#independencia-de-sucesos"><i class="fa fa-check"></i><b>1.4</b> Independencia de sucesos</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probabilidad.html"><a href="probabilidad.html#sucesos-independientes"><i class="fa fa-check"></i><b>1.4.1</b> Sucesos independientes</a></li>
<li class="chapter" data-level="1.4.2" data-path="probabilidad.html"><a href="probabilidad.html#sucesos-independientes-vs-disjuntos"><i class="fa fa-check"></i><b>1.4.2</b> Sucesos independientes vs disjuntos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html"><i class="fa fa-check"></i><b>2</b> Variables Aleatorias</a><ul>
<li class="chapter" data-level="2.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#introducción-a-las-variables-aleatorias"><i class="fa fa-check"></i><b>2.1</b> Introducción a las variables aleatorias</a><ul>
<li class="chapter" data-level="2.1.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#definición-de-variable-aleatoria"><i class="fa fa-check"></i><b>2.1.1</b> Definición de variable aleatoria</a></li>
<li class="chapter" data-level="2.1.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#tipos-de-variables-aleatorias"><i class="fa fa-check"></i><b>2.1.2</b> Tipos de variables aleatorias</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#variables-aleatorias-discretas"><i class="fa fa-check"></i><b>2.2</b> Variables aleatorias discretas</a><ul>
<li class="chapter" data-level="2.2.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#distribuciones-de-probabilidad-para-v.a.-discretas."><i class="fa fa-check"></i><b>2.2.1</b> Distribuciones de probabilidad para v.a. discretas.</a></li>
<li class="chapter" data-level="2.2.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#valores-esperados-o-esperanza"><i class="fa fa-check"></i><b>2.2.2</b> Valores esperados o esperanza</a></li>
<li class="chapter" data-level="2.2.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#medidas-de-la-variabilidad"><i class="fa fa-check"></i><b>2.2.3</b> Medidas de la variabilidad</a></li>
<li class="chapter" data-level="2.2.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#transformaciones-lineales."><i class="fa fa-check"></i><b>2.2.4</b> Transformaciones lineales.</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#variables-aleatorias-continuas"><i class="fa fa-check"></i><b>2.3</b> Variables aleatorias continuas</a><ul>
<li class="chapter" data-level="2.3.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#propiedades-3"><i class="fa fa-check"></i><b>2.3.1</b> Propiedades</a></li>
<li class="chapter" data-level="2.3.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#función-de-densidad"><i class="fa fa-check"></i><b>2.3.2</b> Función de densidad</a></li>
<li class="chapter" data-level="2.3.3" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#utilidad-de-la-función-de-densidad"><i class="fa fa-check"></i><b>2.3.3</b> Utilidad de la función de densidad</a></li>
<li class="chapter" data-level="2.3.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#esperanza-y-varianza-para-variables-aleatorias-continuas"><i class="fa fa-check"></i><b>2.3.4</b> Esperanza y varianza para variables aleatorias continuas</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#transformaciones-de-variables-aleatorias"><i class="fa fa-check"></i><b>2.4</b> Transformaciones de variables aleatorias</a></li>
<li class="chapter" data-level="2.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#desigualdades-de-markov-y-de-chebychev"><i class="fa fa-check"></i><b>2.5</b> Desigualdades de Markov y de Chebychev</a><ul>
<li class="chapter" data-level="2.5.1" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#desigualdad-de-markov"><i class="fa fa-check"></i><b>2.5.1</b> Desigualdad de Markov</a></li>
<li class="chapter" data-level="2.5.2" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#desigualdad-de-chebychev"><i class="fa fa-check"></i><b>2.5.2</b> Desigualdad de Chebychev</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html#cuantiles-de-variables-aleatorias"><i class="fa fa-check"></i><b>2.6</b> Cuantiles de variables aleatorias</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html"><i class="fa fa-check"></i><b>3</b> Distribuciones Notables</a><ul>
<li class="chapter" data-level="3.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-discretas"><i class="fa fa-check"></i><b>3.1</b> Distribuciones discretas</a><ul>
<li class="chapter" data-level="3.1.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-de-bernoulli"><i class="fa fa-check"></i><b>3.1.1</b> Distribución de Bernoulli</a></li>
<li class="chapter" data-level="3.1.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-binomial"><i class="fa fa-check"></i><b>3.1.2</b> Distribución binomial</a></li>
<li class="chapter" data-level="3.1.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-geométrica"><i class="fa fa-check"></i><b>3.1.3</b> Distribución geométrica</a></li>
<li class="chapter" data-level="3.1.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-binomial-negativa"><i class="fa fa-check"></i><b>3.1.4</b> Distribución binomial negativa</a></li>
<li class="chapter" data-level="3.1.5" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-de-poisson"><i class="fa fa-check"></i><b>3.1.5</b> Distribución de Poisson</a></li>
<li class="chapter" data-level="3.1.6" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-hipergeométrica"><i class="fa fa-check"></i><b>3.1.6</b> Distribución hipergeométrica</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#cuantiles-de-distribuciones-notables-discretas"><i class="fa fa-check"></i><b>3.2</b> Cuantiles de distribuciones notables discretas</a></li>
<li class="chapter" data-level="3.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-continuas"><i class="fa fa-check"></i><b>3.3</b> Distribuciones continuas</a><ul>
<li class="chapter" data-level="3.3.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-uniforme"><i class="fa fa-check"></i><b>3.3.1</b> Distribución uniforme</a></li>
<li class="chapter" data-level="3.3.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-exponencial"><i class="fa fa-check"></i><b>3.3.2</b> Distribución exponencial</a></li>
<li class="chapter" data-level="3.3.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribución-normal-o-gaussiana"><i class="fa fa-check"></i><b>3.3.3</b> Distribución normal o Gaussiana</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html"><i class="fa fa-check"></i><b>4</b> Variables Aleatorias. Complementos</a><ul>
<li class="chapter" data-level="4.1" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#momentos-de-variables-aleatorias"><i class="fa fa-check"></i><b>4.1</b> Momentos de variables aleatorias</a><ul>
<li class="chapter" data-level="4.1.1" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#momento-de-orden-n"><i class="fa fa-check"></i><b>4.1.1</b> Momento de orden <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="4.1.2" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#momento-central-de-orden-n"><i class="fa fa-check"></i><b>4.1.2</b> Momento central de orden <span class="math inline">\(n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#asimetría-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>4.2</b> Asimetría de una variable aleatoria</a></li>
<li class="chapter" data-level="4.3" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#curtosis-o-apuntamiento-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>4.3</b> Curtosis o apuntamiento de una variable aleatoria</a></li>
<li class="chapter" data-level="4.4" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#métodos-de-transformación"><i class="fa fa-check"></i><b>4.4</b> Métodos de transformación</a><ul>
<li class="chapter" data-level="4.4.1" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#función-generatriz-de-momentos"><i class="fa fa-check"></i><b>4.4.1</b> Función generatriz de momentos</a></li>
<li class="chapter" data-level="4.4.2" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#función-característica"><i class="fa fa-check"></i><b>4.4.2</b> Función característica</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#fiabilidad"><i class="fa fa-check"></i><b>4.5</b> Fiabilidad</a><ul>
<li class="chapter" data-level="4.5.1" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#tiempo-medio-de-vida"><i class="fa fa-check"></i><b>4.5.1</b> Tiempo medio de vida</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#generación-de-muestras-de-variables-aleatorias-por-ordenador"><i class="fa fa-check"></i><b>4.6</b> Generación de muestras de variables aleatorias por ordenador</a><ul>
<li class="chapter" data-level="4.6.1" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#método-de-transformación"><i class="fa fa-check"></i><b>4.6.1</b> Método de transformación</a></li>
<li class="chapter" data-level="4.6.2" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#método-de-rechazo"><i class="fa fa-check"></i><b>4.6.2</b> Método de rechazo</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#entropía"><i class="fa fa-check"></i><b>4.7</b> Entropía</a><ul>
<li class="chapter" data-level="4.7.1" data-path="variables-aleatorias-complementos.html"><a href="variables-aleatorias-complementos.html#entropía-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>4.7.1</b> Entropía de una variable aleatoria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html"><i class="fa fa-check"></i><b>5</b> Vectores aleatorios bidimensionales</a><ul>
<li class="chapter" data-level="5.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#dos-variables-aleatorias"><i class="fa fa-check"></i><b>5.1</b> Dos variables aleatorias</a><ul>
<li class="chapter" data-level="5.1.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#definición"><i class="fa fa-check"></i><b>5.1.1</b> Definición</a></li>
<li class="chapter" data-level="5.1.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#representación-del-dominio-de-una-variable-aleatoria-bidimensional"><i class="fa fa-check"></i><b>5.1.2</b> Representación del dominio de una variable aleatoria bidimensional</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#función-de-distribución-conjunta"><i class="fa fa-check"></i><b>5.2</b> Función de distribución conjunta</a><ul>
<li class="chapter" data-level="5.2.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#definición-1"><i class="fa fa-check"></i><b>5.2.1</b> Definición</a></li>
<li class="chapter" data-level="5.2.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#propiedades-4"><i class="fa fa-check"></i><b>5.2.2</b> Propiedades</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#variables-aleatorias-bidimensionales-discretas"><i class="fa fa-check"></i><b>5.3</b> Variables aleatorias bidimensionales discretas</a><ul>
<li class="chapter" data-level="5.3.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#función-de-probabilidad-conjunta"><i class="fa fa-check"></i><b>5.3.1</b> Función de probabilidad conjunta</a></li>
<li class="chapter" data-level="5.3.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#distribuciones-marginales"><i class="fa fa-check"></i><b>5.3.2</b> Distribuciones marginales</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#variables-aleatorias-bidimensionales-continuas"><i class="fa fa-check"></i><b>5.4</b> Variables aleatorias bidimensionales continuas</a><ul>
<li class="chapter" data-level="5.4.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#definición-2"><i class="fa fa-check"></i><b>5.4.1</b> Definición</a></li>
<li class="chapter" data-level="5.4.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#propiedades-de-la-función-de-densidad"><i class="fa fa-check"></i><b>5.4.2</b> Propiedades de la función de densidad</a></li>
<li class="chapter" data-level="5.4.3" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#la-distribución-gaussiana-bidimensional"><i class="fa fa-check"></i><b>5.4.3</b> La distribución gaussiana bidimensional</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#independencia-de-variables-aleatorias"><i class="fa fa-check"></i><b>5.5</b> Independencia de variables aleatorias</a><ul>
<li class="chapter" data-level="5.5.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#independencia-de-variables-aleatorias-discretas"><i class="fa fa-check"></i><b>5.5.1</b> Independencia de variables aleatorias discretas</a></li>
<li class="chapter" data-level="5.5.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#independencia-de-variables-aleatorias-continuas"><i class="fa fa-check"></i><b>5.5.2</b> Independencia de variables aleatorias continuas</a></li>
<li class="chapter" data-level="5.5.3" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#relación-de-la-independencia-y-la-función-de-distribución"><i class="fa fa-check"></i><b>5.5.3</b> Relación de la independencia y la función de distribución</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#momentos-conjuntos-y-valores-esperados-conjuntos"><i class="fa fa-check"></i><b>5.6</b> Momentos conjuntos y valores esperados conjuntos</a><ul>
<li class="chapter" data-level="5.6.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#valor-esperado-de-una-función-de-dos-variables-aleatorias"><i class="fa fa-check"></i><b>5.6.1</b> Valor esperado de una función de dos variables aleatorias</a></li>
<li class="chapter" data-level="5.6.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#valor-esperado-de-una-función-de-dos-variables-aleatorias-independientes"><i class="fa fa-check"></i><b>5.6.2</b> Valor esperado de una función de dos variables aleatorias independientes</a></li>
<li class="chapter" data-level="5.6.3" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#momentos-conjuntos"><i class="fa fa-check"></i><b>5.6.3</b> Momentos conjuntos</a></li>
<li class="chapter" data-level="5.6.4" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#momentos-conjuntos-centrados-en-las-medias"><i class="fa fa-check"></i><b>5.6.4</b> Momentos conjuntos centrados en las medias</a></li>
<li class="chapter" data-level="5.6.5" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#covariancia-entre-las-variables"><i class="fa fa-check"></i><b>5.6.5</b> Covariancia entre las variables</a></li>
<li class="chapter" data-level="5.6.6" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#coeficiente-de-correlación-entre-las-variables"><i class="fa fa-check"></i><b>5.6.6</b> Coeficiente de correlación entre las variables</a></li>
<li class="chapter" data-level="5.6.7" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#incorrelación-e-independencia"><i class="fa fa-check"></i><b>5.6.7</b> Incorrelación e independencia</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#variables-aleatorias-condicionales-y-valor-esperado-condicional"><i class="fa fa-check"></i><b>5.7</b> Variables aleatorias condicionales y valor esperado condicional</a><ul>
<li class="chapter" data-level="5.7.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#variables-aleatorias-condicionales-discretas"><i class="fa fa-check"></i><b>5.7.1</b> Variables aleatorias condicionales discretas</a></li>
<li class="chapter" data-level="5.7.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#variables-aleatorias-condicionales-continuas"><i class="fa fa-check"></i><b>5.7.2</b> Variables aleatorias condicionales continuas</a></li>
<li class="chapter" data-level="5.7.3" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#valores-esperados-condicionales"><i class="fa fa-check"></i><b>5.7.3</b> Valores esperados condicionales</a></li>
<li class="chapter" data-level="5.7.4" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#relación-con-el-problema-de-la-regresión-general"><i class="fa fa-check"></i><b>5.7.4</b> Relación con el problema de la regresión general</a></li>
<li class="chapter" data-level="5.7.5" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#valores-esperados-condicionales.-caso-general"><i class="fa fa-check"></i><b>5.7.5</b> Valores esperados condicionales. Caso general</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#variables-aleatorias-definidas-como-función-de-dos-variables-aleatorias-conjuntas"><i class="fa fa-check"></i><b>5.8</b> Variables aleatorias definidas como función de dos variables aleatorias conjuntas</a><ul>
<li class="chapter" data-level="5.8.1" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#variable-aleatoria-función-de-la-variable-aleatoria-bidimensional"><i class="fa fa-check"></i><b>5.8.1</b> Variable aleatoria función de la variable aleatoria bidimensional</a></li>
<li class="chapter" data-level="5.8.2" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#transformaciones-lineales-de-variables-aleatorias"><i class="fa fa-check"></i><b>5.8.2</b> Transformaciones lineales de variables aleatorias</a></li>
<li class="chapter" data-level="5.8.3" data-path="vectores-aleatorios-bidimensionales.html"><a href="vectores-aleatorios-bidimensionales.html#transformaciones-generales-de-variables-aleatorias"><i class="fa fa-check"></i><b>5.8.3</b> Transformaciones generales de variables aleatorias</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html"><i class="fa fa-check"></i><b>6</b> Vectores aleatorios</a><ul>
<li class="chapter" data-level="6.1" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#varias-variables-aleatorias"><i class="fa fa-check"></i><b>6.1</b> Varias variables aleatorias</a><ul>
<li class="chapter" data-level="6.1.1" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#definición-3"><i class="fa fa-check"></i><b>6.1.1</b> Definición</a></li>
<li class="chapter" data-level="6.1.2" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#representación-del-dominio-de-una-variable-aleatoria-n-dimensional"><i class="fa fa-check"></i><b>6.1.2</b> Representación del dominio de una variable aleatoria <span class="math inline">\(n\)</span>-dimensional</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#función-de-distribución-conjunta-1"><i class="fa fa-check"></i><b>6.2</b> Función de distribución conjunta</a><ul>
<li class="chapter" data-level="6.2.1" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#definición-4"><i class="fa fa-check"></i><b>6.2.1</b> Definición</a></li>
<li class="chapter" data-level="6.2.2" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#propiedades-5"><i class="fa fa-check"></i><b>6.2.2</b> Propiedades</a></li>
<li class="chapter" data-level="6.2.3" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#ejemplos-9"><i class="fa fa-check"></i><b>6.2.3</b> Ejemplos</a></li>
<li class="chapter" data-level="6.2.4" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#ejemplo-con-r"><i class="fa fa-check"></i><b>6.2.4</b> Ejemplo con <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#variables-aleatorias-n-dimensionales-discretas"><i class="fa fa-check"></i><b>6.3</b> Variables aleatorias <span class="math inline">\(n\)</span>-dimensionales discretas</a><ul>
<li class="chapter" data-level="6.3.1" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#función-de-probabilidad-conjunta-1"><i class="fa fa-check"></i><b>6.3.1</b> Función de probabilidad conjunta</a></li>
<li class="chapter" data-level="6.3.2" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#propiedades-de-la-función-de-probabilidad-conjunta-1"><i class="fa fa-check"></i><b>6.3.2</b> Propiedades de la función de probabilidad conjunta</a></li>
<li class="chapter" data-level="6.3.3" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#funciones-de-distribución-marginales"><i class="fa fa-check"></i><b>6.3.3</b> Funciones de distribución marginales</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#variables-aleatorias-n-dimensionales-continuas"><i class="fa fa-check"></i><b>6.4</b> Variables aleatorias <span class="math inline">\(n\)</span>-dimensionales continuas</a><ul>
<li class="chapter" data-level="6.4.1" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#definición-5"><i class="fa fa-check"></i><b>6.4.1</b> Definición</a></li>
<li class="chapter" data-level="6.4.2" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#propiedades-de-la-función-de-densidad-1"><i class="fa fa-check"></i><b>6.4.2</b> Propiedades de la función de densidad</a></li>
<li class="chapter" data-level="6.4.3" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#la-distribución-gaussiana-n-dimensional"><i class="fa fa-check"></i><b>6.4.3</b> La distribución gaussiana <span class="math inline">\(n\)</span>-dimensional</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#independencia-de-variables-aleatorias-1"><i class="fa fa-check"></i><b>6.5</b> Independencia de variables aleatorias</a><ul>
<li class="chapter" data-level="6.5.1" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#independencia-de-variables-aleatorias-discretas-1"><i class="fa fa-check"></i><b>6.5.1</b> Independencia de variables aleatorias discretas</a></li>
<li class="chapter" data-level="6.5.2" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#independencia-de-variables-aleatorias-continuas-1"><i class="fa fa-check"></i><b>6.5.2</b> Independencia de variables aleatorias continuas</a></li>
<li class="chapter" data-level="6.5.3" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#relación-de-la-independencia-y-la-función-de-distribución-1"><i class="fa fa-check"></i><b>6.5.3</b> Relación de la independencia y la función de distribución</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#momentos-conjuntos-y-valores-esperados-conjuntos-1"><i class="fa fa-check"></i><b>6.6</b> Momentos conjuntos y valores esperados conjuntos</a><ul>
<li class="chapter" data-level="6.6.1" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#valor-esperado-de-una-función-de-n-variables-aleatorias"><i class="fa fa-check"></i><b>6.6.1</b> Valor esperado de una función de <span class="math inline">\(n\)</span> variables aleatorias</a></li>
<li class="chapter" data-level="6.6.2" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#ejemplos-15"><i class="fa fa-check"></i><b>6.6.2</b> Ejemplos</a></li>
<li class="chapter" data-level="6.6.3" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#propiedad-del-valor-esperado-de-la-suma-de-variables"><i class="fa fa-check"></i><b>6.6.3</b> Propiedad del valor esperado de la suma de variables</a></li>
<li class="chapter" data-level="6.6.4" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#valor-esperado-de-una-función-de-n-variables-aleatorias-independientes"><i class="fa fa-check"></i><b>6.6.4</b> Valor esperado de una función de <span class="math inline">\(n\)</span> variables aleatorias independientes</a></li>
<li class="chapter" data-level="6.6.5" data-path="vectores-aleatorios.html"><a href="vectores-aleatorios.html#propiedades-de-la-covarianza-1"><i class="fa fa-check"></i><b>6.6.5</b> Propiedades de la covarianza</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><i class="fa fa-check"></i><b>7</b> Ley de los grandes números y Teorema Central del Límite</a><ul>
<li class="chapter" data-level="7.1" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#muestras-aleatorias-simples"><i class="fa fa-check"></i><b>7.1</b> Muestras aleatorias simples</a><ul>
<li class="chapter" data-level="7.1.1" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#la-distribución-de-la-media-muestral"><i class="fa fa-check"></i><b>7.1.1</b> La distribución de la media muestral</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#convergencia-de-sucesiones-de-variables-aleatorias"><i class="fa fa-check"></i><b>7.2</b> Convergencia de sucesiones de variables aleatorias</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#convergencia-casi-segura"><i class="fa fa-check"></i><b>7.2.1</b> Convergencia casi segura</a></li>
<li class="chapter" data-level="7.2.2" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#convergencia-en-probabilidad"><i class="fa fa-check"></i><b>7.2.2</b> Convergencia en probabilidad</a></li>
<li class="chapter" data-level="7.2.3" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#convergencia-en-ley-o-en-distribución"><i class="fa fa-check"></i><b>7.2.3</b> Convergencia en ley o en distribución</a></li>
<li class="chapter" data-level="7.2.4" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#relaciones-entre-las-distintas-convergencias"><i class="fa fa-check"></i><b>7.2.4</b> Relaciones entre las distintas convergencias</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#leyes-de-los-grandes-números"><i class="fa fa-check"></i><b>7.3</b> Leyes de los grandes números</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#leyes-débiles-de-los-grandes-números"><i class="fa fa-check"></i><b>7.3.1</b> Leyes débiles de los grandes números</a></li>
<li class="chapter" data-level="7.3.2" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#convergencia-de-los-momentos-muestrales"><i class="fa fa-check"></i><b>7.3.2</b> Convergencia de los momentos muestrales</a></li>
<li class="chapter" data-level="7.3.3" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#leyes-fuertes-de-los-grandes-números"><i class="fa fa-check"></i><b>7.3.3</b> Leyes fuertes de los grandes números</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#teorema-central-del-límite"><i class="fa fa-check"></i><b>7.4</b> Teorema Central del Límite</a><ul>
<li class="chapter" data-level="7.4.1" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#teorema-central-del-límite-1"><i class="fa fa-check"></i><b>7.4.1</b> Teorema Central del Límite</a></li>
<li class="chapter" data-level="7.4.2" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#teorema-central-del-límite-en-la-práctica"><i class="fa fa-check"></i><b>7.4.2</b> Teorema Central del Límite en la práctica</a></li>
<li class="chapter" data-level="7.4.3" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#teorema-de-moivre-laplace"><i class="fa fa-check"></i><b>7.4.3</b> Teorema de Moivre-Laplace</a></li>
<li class="chapter" data-level="7.4.4" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#aproximación-de-una-suma-de-variables-poisson"><i class="fa fa-check"></i><b>7.4.4</b> Aproximación de una suma de variables Poisson</a></li>
<li class="chapter" data-level="7.4.5" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#corrección-de-continuidad-de-fisher"><i class="fa fa-check"></i><b>7.4.5</b> Corrección de continuidad de Fisher</a></li>
<li class="chapter" data-level="7.4.6" data-path="ley-de-los-grandes-números-y-teorema-central-del-límite.html"><a href="ley-de-los-grandes-números-y-teorema-central-del-límite.html#simulación-del-teorema-central-del-límite"><i class="fa fa-check"></i><b>7.4.6</b> Simulación del Teorema Central del Límite</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://www.udemy.com/course/probabilidad-y-variables-aleatorias-para-ml-con-r-y-python/?couponCode=B85F8D52148DF5AAD8F7" target="blank">Curso en Udemy</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probabilidad y variables aleatorias para ML con R y Python</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variables-aleatorias.-complementos" class="section level1">
<h1><span class="header-section-number">Tema 4</span> Variables Aleatorias. Complementos</h1>
<div id="momentos-de-variables-aleatorias" class="section level2">
<h2><span class="header-section-number">4.1</span> Momentos de variables aleatorias</h2>
<div id="momento-de-orden-n" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Momento de orden <span class="math inline">\(n\)</span></h3>
<p><l class="definition"> <strong>Definición.</strong> </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria. Definimos el <strong>momento de orden <span class="math inline">\(n\)</span></strong> como
<span class="math inline">\(m_n = E\left(X^n\right)\)</span>.</p>
<p><l class="observ"> <strong>Observación.</strong></l>
El momento de orden <span class="math inline">\(1\)</span> de una variable aleatoria es su valor medio o <span class="math inline">\(E(X)\)</span>.</p>
<p>Los momentos de orden <span class="math inline">\(n\)</span> caracterizan una variable <span class="math inline">\(X\)</span>. O sea, que si conocemos todos los momentos de orden <span class="math inline">\(n\)</span>, podemos deducir cuál es la distribución de <span class="math inline">\(X\)</span>.</p>
<p>En general, el cálculo de los momentos de orden <span class="math inline">\(n\)</span> para una variable <span class="math inline">\(X\)</span> es bastante tedioso.</p>
<p><strong>Ejemplos de momentos de orden <span class="math inline">\(n\)</span></strong></p>
<div class="example">
<p><strong>Ejemplo: momento de orden <span class="math inline">\(n\)</span> de una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ p_X(1)=p.
\]</span>
Su momento de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
m_n = E\left(X^n\right)=p\cdot 1^n+(1-p)\cdot 0^n = p.
\]</span>
En este caso, todos los momentos de orden <span class="math inline">\(n\)</span> valen <span class="math inline">\(p\)</span>.</p>
</div>
<div class="example">
<p><strong>Ejemplo: momento de orden <span class="math inline">\(n\)</span> de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Consideremos ahora una variable <span class="math inline">\(X\)</span> exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(0\)</span>, en caso contrario.</p>
<p>Su momento de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
m_n = E\left(X^n\right)=\int_0^\infty \lambda \mathrm{e}^{-\lambda x} x^n\, dx =\frac{n!}{\lambda^n}.
\]</span></p>
<p>La expresión anterior se puede obtener integrando por partes <span class="math inline">\(n\)</span> veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes.</p>
<p>Fijémonos que los momentos de orden <span class="math inline">\(n\)</span> tienden a infinito a medida que <span class="math inline">\(n\)</span> crece: <span class="math inline">\(\lim\limits_{n\to\infty}m_n = \lim\limits_{n\to\infty}\frac{n!}{\lambda^n}=\infty\)</span>.</p>
</div>
<div class="example">
<p><strong>Ejemplo: momento de orden <span class="math inline">\(n\)</span> de una variable normal de parámetros <span class="math inline">\(m=0\)</span> y <span class="math inline">\(\sigma =1\)</span></strong></p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su momento de orden 1 será la esperanza de <span class="math inline">\(X\)</span>: <span class="math inline">\(m_1 = 0\)</span> i su momento de orden 2 será:
<span class="math inline">\(m_2 = E\left(X^2\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^2\, dx = 1.\)</span>
La integral anterior se resuelve usando técnicas de integrales de dos variables.
Dicho valor también se puede obtener usando que su varianza vale 1:
<span class="math inline">\(m_2 = \mathrm{Var}(X)+E(X)^2 = \sigma^2 +0^2 = 1.\)</span></p>
<p>Los momentos de orden impar <span class="math inline">\(n\)</span> serán cero ya que integramos una función impar:
<span class="math inline">\(m_n = E\left(X^n\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^n\, dx = 0.\)</span>
O sea, si consideramos <span class="math inline">\(g(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^n\)</span>, se verifica <span class="math inline">\(g(-x)=-g(x)\)</span>, para todo <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<p>Si intentamos calcular el momento de orden 4, obtenemos:
<span class="math inline">\(m_4 = E\left(X^4\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^4\, dx = 3,\)</span> usando técnicas de integración de dos variables otra vez.</p>
</div>
</div>
<div id="momento-central-de-orden-n" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Momento central de orden <span class="math inline">\(n\)</span></h3>
<p><l class="definition"> <strong>Definición.</strong> </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria. Definimos el <strong>momento central de orden <span class="math inline">\(n\)</span></strong> como
<span class="math inline">\(\mu_n = E\left((X-\mu)^n\right)\)</span>, donde <span class="math inline">\(\mu =E(X)\)</span> es la media o la esperanza de la variable aleatoria <span class="math inline">\(X\)</span>.</p>
<p><l class="observ"> <strong>Observación.</strong></l>
El momento central de orden <span class="math inline">\(1\)</span> de una variable aleatoria es siempre 0:
<span class="math display">\[
\mu_1 = E\left((X-\mu)\right)=E(X)-E(\mu)=E(X)-E(X)=0.
\]</span></p>
<p><l class="observ"> Observación.</l>
El momento central de orden <span class="math inline">\(2\)</span> de una variable aleatoria es la varianza:
<span class="math display">\[
\mu_2 = E\left((X-\mu)^2\right):= \mathrm{Var}(X).
\]</span></p>
<p>Los momentos centrales de orden <span class="math inline">\(n\)</span> caracterizan también una variable <span class="math inline">\(X\)</span>. O sea, que si conocemos todos los momentos centrales de orden <span class="math inline">\(n\)</span>, podemos deducir cuál es la distribución de <span class="math inline">\(X\)</span>.</p>
<p><l class="prop"> <strong>Proposición.</strong></l>
La relación que hay entre los momentos centrales y los momentos de una variable aleatoria es la siguiente:
<span class="math display">\[
\mu_n = \sum_{k=0}^n (-1)^{n-k} \binom{n}{k} \mu^{n-k} m_k = \sum_{k=0}^n (-1)^{k} \binom{n}{k} \mu^{k} m_{n-k},
\]</span>
donde <span class="math inline">\(\mu =E(X)\)</span> recordemos que es la esperanza de la variable aleatoria <span class="math inline">\(X\)</span>.</p>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Recordemos la definición de momento central de orden <span class="math inline">\(n\)</span> y desarrollemos su expresión aplicando el <strong>binomio de Newton</strong>:
<span class="math display">\[
\mu_n = E\left((X-\mu)^n\right) =E\left(\sum_{k=0}^n (-1)^{n-k} \binom{n}{k} X^k\mu^{n-k}\right).
\]</span>
Aplicando la propiedad de la esperanza que la esperanza de la suma es la suma de esperanzas, obtenemos la expresión dada por la proposición:
<span class="math display">\[
\mu_n =\sum_{k=0}^n (-1)^{n-k} \binom{n}{k} \mu^{n-k} E\left(X^k\right) = \sum_{k=0}^n (-1)^{n-k} \binom{n}{k} \mu^{n-k} m_k.
\]</span></p>
</div>
<p><strong>Ejemplos de momentos centrales de orden <span class="math inline">\(n\)</span></strong></p>
<div class="example">
<p><strong>Ejemplo: momento central de orden <span class="math inline">\(n\)</span> de una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ P_X(1)=p.
\]</span>
Usando que <span class="math inline">\(E(X)=p\)</span>, su momento central de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
\mu_n = E\left((X-p)^n\right)=p\cdot (1-p)^n+(1-p)\cdot (0-p)^n = p(1-p)^n + (-1)^n (1-p) p^n.
\]</span></p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Demostrar que la expresión anterior corresponde a un polinomio de grado <span class="math inline">\(n\)</span>.</p>
</div>
<div class="example">
<p><strong>Ejemplo: momento central de orden <span class="math inline">\(n\)</span> de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Consideremos ahora una variable <span class="math inline">\(X\)</span> exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span>.</p>
<p>Usando que <span class="math inline">\(E(X)=\frac{1}{\lambda}\)</span>, su momento central de orden <span class="math inline">\(n\)</span> será:
<span class="math display">\[
\mu_n = E\left(\left(X-\frac{1}{\lambda}\right)^n\right)=\int_0^\infty \lambda \mathrm{e}^{-\lambda x} \left(x-\frac{1}{\lambda}\right)^n\, dx =\frac{a_n}{\lambda^n},
\]</span>
donde <span class="math inline">\(a_n = n!\sum\limits_{k=0}^n \frac{(-1)^k}{k!}.\)</span></p>
<p>La expresión anterior fijado <span class="math inline">\(n\)</span> se puede obtener integrando por partes <span class="math inline">\(n\)</span> veces y resolviendo los límites correspondientes. Dejámos al lector los cálculos correspondientes. Sin embargo, la obtención de la fórmula general para <span class="math inline">\(n\)</span> se sale del nivel del curso.</p>
<p>Fijémonos que los momentos centrales de orden <span class="math inline">\(n\)</span> también tienden a infinito a medida que <span class="math inline">\(n\)</span> crece: <span class="math inline">\(\lim\limits_{n\to\infty}\mu_n = \lim\limits_{n\to\infty}\frac{a_n}{\lambda^n}=\infty\)</span>:
<span class="math display">\[
\lim_{n\to\infty}\mu_n =\lim_{n\to\infty} \frac{n!\sum\limits_{k=0}^n \frac{(-1)^k}{k!}}{\lambda^n}= 
\lim_{n\to\infty}\sum\limits_{k=0}^n \frac{(-1)^k}{k!}\cdot \lim_{n\to\infty} \frac{n!}{\lambda^n}= \mathrm{e}^{-1}\cdot \infty = \infty.
\]</span></p>
</div>
<div class="example">
<p><strong>Ejemplo: momento central de orden <span class="math inline">\(n\)</span> de una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su momento central de orden 2 será la varianza <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\mu_2 =\sigma^2.\)</span></p>
<p>Los momentos centrales de orden impar <span class="math inline">\(n\)</span> serán cero ya que integramos una función impar respecto <span class="math inline">\(x=\mu\)</span>:
<span class="math inline">\(\mu_n = E\left((X-\mu)^n\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\cdot (x-\mu)^n\, dx = 0.\)</span>
O sea, si consideramos <span class="math inline">\(g(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\cdot (x-\mu)^n\)</span>, se verifica <span class="math inline">\(g(\mu-x)=-g(\mu +x)\)</span>, para todo <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<p>Si intentamos calcular el momento central de orden 4, obtenemos:
<span class="math inline">\(\mu_4 = E\left((X-\mu)^4\right)=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\cdot (x-\mu)^4\, dx = 3\sigma^4.\)</span> La integral anterior puede resolverse con el cambio de variable <span class="math inline">\(t=\frac{x-\mu}{\sigma}\)</span> y usando que: <span class="math inline">\(\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\cdot x^4\, dx = 3.\)</span></p>
</div>
</div>
</div>
<div id="asimetría-de-una-variable-aleatoria" class="section level2">
<h2><span class="header-section-number">4.2</span> Asimetría de una variable aleatoria</h2>
<p>Una variable aleatoria tiene <strong>asimetría positiva</strong> si su función de densidad o de probabilidad presenta una cola a la
<strong>derecha</strong> y <strong>asimetría negativa</strong>, si su función de densidad o de probabilidad presenta cola a la <strong>izquierda</strong>.</p>
<p>Por ejemplo, en la figura siguiente, vemos la gráfica de la función de probabilidad de una variable aleatoria que presenta <strong>asimetría negativa</strong> a la izquierda y una función de densidad de una variable aleatoria que presenta <strong>asimetría positiva</strong> a la derecha:</p>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
</div>
<p><strong>¿Cómo calcular la asimetría de una variable aleatoria?</strong></p>
<p>La asimetría de una variable aleatoria <span class="math inline">\(X\)</span> se calcula a partir de sus momentos centrales de segundo y tercer orden:
<span class="math display">\[
\gamma_1 = E\left({\left(\frac{X-\mu}{\sigma}\right)}^3\right)=\frac{\mu_3}{\sigma^3},
\]</span>
donde <span class="math inline">\(\mu = E(X)\)</span> y <span class="math inline">\(\sigma^2 =\mathrm{Var}(X)\)</span>.</p>
<p>Dicho valor se denomina <strong>coeficiente de asimetría de Pearson</strong>.</p>
<p>Usando la relación ya vista entre los momentos centrales y los momentos, podemos expresar el <strong>coeficiente de asimetría</strong> en función de los momentos:
<span class="math display">\[
\gamma_1 = \frac{m_3 -3\mu\sigma^2-\mu^3}{\sigma^3}.
\]</span>
Dejamos al lector la comprobación de la expresión anterior.</p>
<p>Por tanto, una variable aleatoria <span class="math inline">\(X\)</span> tendrá simetría positiva o a la derecha si <span class="math inline">\(\gamma_1 &gt;0\)</span> y tendrá asimetría negativa o a la izquierda, si <span class="math inline">\(\gamma_1 &lt;0\)</span>.</p>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de asimetría para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Usando que <span class="math inline">\(m_n =p\)</span>, para todo <span class="math inline">\(n\)</span> y que <span class="math inline">\(\mu_2 = \sigma^2 = p-p^2\)</span>, el coeficiente de asimetría <span class="math inline">\(\gamma_1\)</span> será:
<span class="math display">\[
\gamma_1 = \frac{p-3p(p-p^2)-p^3}{\sqrt{(p-p^2)^3}} = \frac{p (1-p) (1-2p)}{{\sqrt{(p-p^2)^3}}}.
\]</span>
Por tanto, la variable de Bernoulli de parámetro <span class="math inline">\(p\)</span> tendrá simetria negativa si <span class="math inline">\(p&gt;\frac{1}{2}\)</span> y positiva, si <span class="math inline">\(p&lt;\frac{1}{2}\)</span>:</p>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
</div>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de asimetría para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span>.
Usando que <span class="math inline">\(\sigma^2=\frac{1}{\lambda^2}\)</span> y <span class="math inline">\(\mu_3 =\frac{a_3}{\lambda^3}=\frac{2}{\lambda^3}\)</span>, su coeficiente de asimetría de Pearson será:
<span class="math inline">\(\gamma_1 = \frac{\frac{2}{\lambda^3}}{\frac{1}{\lambda^3}}=2.\)</span></p>
<p>Entonces presenta asimetría positiva o a la derecha tal como se observa en su función de densidad:</p>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-105-1.png" width="672" /></p>
</div>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de asimetría para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Tal como se ha indicado anteriormente, los momentos centrales de orden impar son nulos.</p>
<p>Por tanto, en este caso <span class="math inline">\(\mu_3=0\)</span> y, por tanto, <span class="math inline">\(\gamma_1=0\)</span>.</p>
<p>Deducimos que la distribución normal es totalmente simétrica.</p>
<p>De hecho, usando que su función de densidad es <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>, se puede comprobar que <span class="math inline">\(f_X(\mu-x)=f_X(\mu +x)\)</span>, o sea, tiene el eje de simetría <span class="math inline">\(x=\mu\)</span>:</p>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-106-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="curtosis-o-apuntamiento-de-una-variable-aleatoria" class="section level2">
<h2><span class="header-section-number">4.3</span> Curtosis o apuntamiento de una variable aleatoria</h2>
<p>La curtosis de una variable aleatoria <span class="math inline">\(X\)</span> es una medida de cómo son las colas de su función de densidad.</p>
<p>Dicho en otras palabras, queremos medir de alguna manera la <em>tendencia</em> que tiene la variable aleatoria a tener valores atípicos o <em>outliers</em>.</p>
<p>La manera estándard de medir la curtosis de una variable aleatoria <span class="math inline">\(X\)</span> es a partir de su <strong>momento central de cuarto orden</strong>:
<span class="math display">\[
\gamma_2 = E\left(\left(\frac{X-\mu}{\sigma}\right)^4\right) = \frac{\mu_4}{\sigma^4},
\]</span>
donde recordemos que <span class="math inline">\(\mu=E(X)\)</span> y <span class="math inline">\(\sigma^2 =\mathrm{Var}(X)\)</span>.</p>
<p>A la expresión anterior se le denomina <strong>medida de curtosis de Pearson</strong>.</p>
<ul>
<li><p>Diremos que una variable aleatoria no tiene exceso de curtosis o <strong>mesocúrtica</strong> si <span class="math inline">\(\gamma_2 \approx 3\)</span>.</p></li>
<li><p>Diremos que una variable aleatoria tiene exceso positivo de curtosis o <strong>leptocúrtica</strong> si <span class="math inline">\(\gamma_2 &gt;3\)</span>.</p></li>
<li><p>Diremos que una variable aleatoria tiene exceso negativo de curtosis o <strong>platicúrtica</strong> si <span class="math inline">\(\gamma_2 &lt;3\)</span>.</p></li>
</ul>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de curtosis para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria de parámetro <span class="math inline">\(p\)</span>.</p>
<p>El momento central de cuarto orden de <span class="math inline">\(X\)</span> será:
<span class="math display">\[
\mu_4 = p (1-p)^4 +(1-p)p^4 = p (1-p) (3 p^2-3p+1).
\]</span>
La medida de curtosis de Pearson será:
<span class="math display">\[
\gamma_2 = \frac{p (1-p) (3 p^2-3p+1)}{p^2 (1-p)^2} = \frac{3 p^2-3p+1}{p(1-p)}.
\]</span>
Se puede comprobar (ejercicio para el lector) que si <span class="math inline">\(p\in \left(\frac{3-\sqrt{3}}{6},\frac{3+\sqrt{3}}{6}\right)\approx (0.211,0.789)\)</span>, <span class="math inline">\(\gamma_2 &lt;3\)</span> y, por tanto <span class="math inline">\(X\)</span> será platicúrtica y en caso contrario, si <span class="math inline">\(p\in \left(0,\frac{3-\sqrt{3}}{6}\right)\cup \left(\frac{3+\sqrt{3}}{6},1\right)\)</span>, <span class="math inline">\(\gamma_2 &gt;3\)</span> y, por tanto, <span class="math inline">\(X\)</span> será leptocúrtica.</p>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de curtosis para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span>.
Usando que <span class="math inline">\(\sigma^2=\frac{1}{\lambda^2}\)</span> y <span class="math inline">\(\mu_4 =\frac{a_4}{\lambda^3}=\frac{9}{\lambda^4}\)</span>, su coeficiente de asimetría de Pearson será:
<span class="math inline">\(\gamma_2 = \frac{\frac{9}{\lambda^4}}{\frac{1}{\lambda^4}}=9.\)</span></p>
<p>Como <span class="math inline">\(\gamma_2 &gt;3\)</span>, se trataría de una distribución leptocúrtica.</p>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo del coeficiente de curtosis para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Tal como se ha indicado anteriormente, el momento central de orden 4 vale: <span class="math inline">\(\mu_4 = 3\sigma^4\)</span>.</p>
<p>Su coeficiente de curtosis será:
<span class="math display">\[
\gamma_2 =\frac{\mu_4}{\sigma^4}=\frac{3\sigma^4}{\sigma^4}=3.
\]</span>
Deducimos, por tanto, que toda distribución normal es mesocúrtica o no tiene exceso (ni positivo ni negativo) de curtosis.</p>
</div>
</div>
<div id="métodos-de-transformación" class="section level2">
<h2><span class="header-section-number">4.4</span> Métodos de transformación</h2>
<p>Hemos visto anteriormente que el cálculo de los <strong>momentos</strong> o los <strong>momentos centrados</strong> de una variable aleatoria <span class="math inline">\(X\)</span> puede ser muy complicado y muy tedioso.</p>
<p>Por dicho motivo, vamos a introducir un conjunto de funciones que nos permitirán calcular los <strong>momentos</strong> de la variable <span class="math inline">\(X\)</span> de forma relativamente sencilla.</p>
<div id="función-generatriz-de-momentos" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Función generatriz de momentos</h3>
<p><l class="definition"><strong>Definición de función generatriz de momentos:</strong> </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria <span class="math inline">\(X\)</span> con función de probabilidad <span class="math inline">\(P_X\)</span> en el caso discreto o función
de densidad <span class="math inline">\(f_X\)</span> en el caso continuo.</p>
<p>Sea <span class="math inline">\(t\in\mathbb{R}\)</span> un valor real cualquiera.</p>
<p>Definimos la función generatriz de momentos <span class="math inline">\(m_X(t)\)</span> en el valor <span class="math inline">\(t\)</span> como: <span class="math inline">\(m_X(t)=E\left(\mathrm{e}^{tX}\right).\)</span></p>
<div class="example">
<p><strong>Ejemplo: cálculo de la función generatriz de momentos para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ p_X(1)=p.
\]</span>
Su función generatriz de momentos será:
<span class="math display">\[
m_X (t)=E\left(\mathrm{e}^{tX}\right) =p\mathrm{e}^{t\cdot 1}+(1-p)\mathrm{e}^{t\cdot 0}=p\mathrm{e}^t+(1-p)=1+p\left(\mathrm{e}^t -1 \right).
\]</span></p>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo de la función generatriz de momentos para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>. Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(0\)</span>, en caso contrario.</p>
<p>Su función generatriz de momentos será:
<span class="math display">\[
m_X (t)=E\left(\mathrm{e}^{tX}\right)=\int_0^\infty \mathrm{e}^{t x}\lambda \mathrm{e}^{-\lambda x}\, dx = \lambda \int_0^\infty\mathrm{e}^{(t-\lambda)x}\, dx = \lambda\left[\frac{\mathrm{e}^{(t-\lambda)x}}{t-\lambda}\right]_{x=0}^{x=\infty} = \frac{\lambda}{\lambda -t},\ \mbox{si } t&lt;\lambda. 
\]</span>
En este caso vemos que el dominio de la función generatriz de momentos <span class="math inline">\(m_X\)</span> es <span class="math inline">\((-\infty,\lambda)\)</span>, ya que si <span class="math inline">\(t\geq \lambda\)</span>, la integral anterior no es convergente.</p>
<p>Fijémonos por lo que vendrá más adelante que, como <span class="math inline">\(\lambda &gt;0\)</span>, el valor <span class="math inline">\(0\)</span> pertenece al dominio de <span class="math inline">\(m_X\)</span>.</p>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo de la función generatriz de momentos para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su función generatriz de momentos será:</p>
<p><span class="math display">\[
\begin{array}{rl}
m_X (t) &amp; =E\left(\mathrm{e}^{tX}\right)=\displaystyle\int_{-\infty}^\infty \mathrm{e}^{tx}\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{tx-\frac{(x-\mu)^2}{2\sigma^2}}\, dx \\[1ex]  &amp; =  \displaystyle\frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}\left((x-(\sigma^2 t+\mu))^2-2\sigma^2 t \mu-\sigma^4t^2\right)}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{e}^{\frac{1}{2}(2 t \mu +\sigma^2 t^2)}\int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 t+\mu))^2}\, dx\\[1ex] &amp;  = \displaystyle\mathrm{e}^{\frac{1}{2}(2 t \mu +\sigma^2 t^2)} \left( \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 t+\mu))^2}\, dx\right) =  \mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}}.
\end{array}
\]</span>
La integral del último paréntesis se resuelve haciento el cambio de variable <span class="math inline">\(u=x-\sigma^2 t\)</span> y usando que la integral de la función de densidad de <span class="math inline">\(X\)</span> sobre todo <span class="math inline">\(\mathbb{R}\)</span> vale 1.</p>
</div>
<p><strong>Relación entre la función generatriz de momentos y los momentos</strong></p>
<p>La razón del nombre que lleva la <strong>función generatriz de momentos</strong> es que podemos obtener todos los momentos de la variable a partir de ella:</p>
<p><l class="prop"> <strong>Proposición.</strong> </l>
Sean <span class="math inline">\(X\)</span> una variable aleatoria con <strong>función generatriz de momentos</strong> <span class="math inline">\(m_X(t)\)</span>. Entonces, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> se puede obtener de la forma siguiente:
<span class="math display">\[
m_n =E\left(X^n\right)=\frac{d}{d t^n}m_X(t)|_{t=0} =m_X^{(n)}(0).
\]</span>
O sea, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> es la derivada <span class="math inline">\(n\)</span>-ésima de la función generatriz de momentos evaluada en <span class="math inline">\(t=0\)</span>.</p>
<div class="dem">
<p><strong>Demostración</strong></p>
<p>Recordemos la definición de la función generatriz de momentos: <span class="math inline">\(m_X(t)=E\left(\mathrm{e}^{tX}\right).\)</span></p>
<p>La idea de la demostración es probar por inducción que <span class="math inline">\(m_X^{(n)}(t) =E\left(\mathrm{e}^{tX}\cdot X^n\right)\)</span>.</p>
<p>Veámoslo para <span class="math inline">\(n=1\)</span>: <span class="math inline">\(m_X&#39;(t)=E\left(\mathrm{e}^{tX}\cdot X\right)\)</span>.</p>
<p>Seguidamente, apliquemos inducción sobre <span class="math inline">\(n\)</span>. Supongamos que <span class="math inline">\(m_X^{(n)}(t) =E\left(\mathrm{e}^{tX}\cdot X^n\right)\)</span> y veamos que <span class="math inline">\(m_X^{(n+1)}(t) =E\left(\mathrm{e}^{tX}\cdot X^{n+1}\right)\)</span>:
<span class="math inline">\(m_X^{(n+1)}(t) =\frac{d}{dt}(m_X^{(n)}(t)) =\frac{d}{dt}E\left(\mathrm{e}^{tX}\cdot X^n\right) = E\left(\mathrm{e}^{tX}\cdot X^{n+1}\right),\)</span>
tal como queríamos demostrar.</p>
<p>Ahora si aplicamos la expresión demostrada <span class="math inline">\(m_X^{(n)}(t) =E\left(\mathrm{e}^{tX}\cdot X^n\right)\)</span> a <span class="math inline">\(t=0\)</span>, obtenemos:
<span class="math inline">\(m_X^{(n)}(0) =E\left(X^n\right)=m_n,\)</span>
tal como dice la proposición.</p>
</div>
<div class="example">
<p><strong>Ejemplo: aplicación de la proposición en el caso en que <span class="math inline">\(X\)</span> es una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>En este caso, recordemos que: <span class="math inline">\(m_X (t)=1+p\left(\mathrm{e}^t -1 \right).\)</span></p>
<p>Se puede comprobar que <span class="math inline">\(m_X^{(n)}(t)=p\mathrm{e}^t\)</span>. Por tanto:
<span class="math display">\[
m_n = m_X^{(n)}(0)=p,
\]</span>
tal como habíamos calculado anteriormente.</p>
</div>
<div class="example">
<p><strong>Ejemplo: aplicación de la proposición en el caso en que <span class="math inline">\(X\)</span> es una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>En este caso, recordemos que: <span class="math inline">\(m_X (t)=\frac{\lambda}{\lambda -t},\)</span> para <span class="math inline">\(t&lt;\lambda\)</span> pero como <span class="math inline">\(\lambda &gt;0\)</span>, <span class="math inline">\(t=0\)</span> cumple la expresión anterior.</p>
<p>Dejamos como ejercicio para el lector comprobar que: <span class="math inline">\(m_X^{(n)}(t)=\frac{\lambda n!}{(\lambda-t)^{n+1}}\)</span>.</p>
<p>Por tanto:
<span class="math display">\[
m_n = m_X^{(n)}(0) = \frac{\lambda n!}{\lambda^{n+1}}=\frac{n!}{\lambda^n},
\]</span>
expresión que ya habíamos obtenido anteriormente.</p>
</div>
<div class="example">
<p><strong>Ejemplo: aplicación de la proposición en el caso en que <span class="math inline">\(X\)</span> es una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>En este caso, recordemos que: <span class="math inline">\(m_X (t)=\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}}.\)</span></p>
<p>Aplicando la fórmula de los momentos para <span class="math inline">\(n=1\)</span> obtenemos:
<span class="math inline">\(m&#39;(t)=\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}} \left(\mu+t\sigma^2\right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale:
<span class="math inline">\(m&#39;(0)=\mu=E(X)\)</span>, tal como ya sabemos.</p>
<p>Si la aplicamos para <span class="math inline">\(n=2\)</span>, obtenemos:
<span class="math inline">\(m&#39;&#39;(t)=\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}} \left((\mu+t\sigma^2)^2+ \sigma^2 \right) =\mathrm{e}^{ t \mu +\frac{\sigma^2 t^2}{2}} \left(t^2\sigma^4+\mu^2+\sigma^2+ 2t\mu\sigma^2 \right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale:
<span class="math inline">\(m&#39;&#39;(0)=\mu^2+\sigma^2=E\left(X^2\right)\)</span>, tal como ya sabemos.</p>
<p>Para <span class="math inline">\(n=3\)</span>m obtenemos:
<span class="math inline">\(m&#39;&#39;&#39;(t)=e^{\mu t+\frac{\sigma ^2 t^2}{2}}\left(\mu +\sigma ^2 t\right) \left(\left(\mu +\sigma ^2 t\right)^2+3 \sigma ^2\right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale: <span class="math inline">\(m&#39;&#39;&#39;(0)=3\sigma^2\mu = E\left(X^3\right)\)</span>, valor que correspondería al momento de tercer orden de <span class="math inline">\(X\)</span>.</p>
<p>Por último, para <span class="math inline">\(n=4\)</span>, obtenemos:
<span class="math inline">\(m^{(iv)}(t)=e^{\mu t+\frac{\sigma ^2 t^2}{2}}  \left(6 \sigma ^2 \left(\mu  +\sigma ^2 t\right)^2+\left(\mu  +\sigma ^2 t\right)^4+3 \sigma  ^4\right)\)</span>, que en <span class="math inline">\(t=0\)</span> vale: <span class="math inline">\(m^{(iv)}(0)=6\sigma^2\mu^2+\mu^4+3\sigma^4=E\left(X^4\right)\)</span>, valor que correspondería al momento de cuarto orden de <span class="math inline">\(X\)</span>.</p>
</div>
</div>
<div id="función-característica" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Función característica</h3>
<p><l class="definition"><strong>Definición de función característica:</strong> </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria <span class="math inline">\(X\)</span> con función de probabilidad <span class="math inline">\(P_X\)</span> en el caso discreto o función
de densidad <span class="math inline">\(f_X\)</span> en el caso continuo.</p>
<p>Sea <span class="math inline">\(w\in\mathbb{R}\)</span> un valor real cualquiera.</p>
<p>Definimos la función característica <span class="math inline">\(\phi_X(w)\)</span> en el valor <span class="math inline">\(w\)</span> como: <span class="math inline">\(\phi_X(w)=E\left(\mathrm{e}^{\mathrm{i} w X}\right),\)</span> donde <span class="math inline">\(\mathrm{i}\)</span> es el número complejo <span class="math inline">\(\mathrm{i}=\sqrt{-1}\)</span>.</p>
<p><l class="observ">Observación: </l>
Si <span class="math inline">\(X\)</span> es una variable continua, la <strong>función característica</strong> <span class="math inline">\(\phi_X(w)\)</span> puede interpretarse como la <strong>transformada de Fourier</strong> de la <strong>función de densidad</strong> de <span class="math inline">\(X\)</span>:
<span class="math inline">\(\phi(w)=\int_{-\infty}^\infty f_X(x)\mathrm{e}^{\mathrm{i}w x}\, dx.\)</span></p>
<p>Por tanto, usando la fórmula de la <strong>antitransformada de Fourier</strong>, podemos escribir la <strong>función de densidad</strong> <span class="math inline">\(f_X(x)\)</span> como función de la <strong>función característica</strong> de <span class="math inline">\(X\)</span>, <span class="math inline">\(\phi(w)\)</span>:
<span class="math inline">\(f_X(x)=\frac{1}{2\pi}\int_{-\infty}^\infty \phi_X(w)\mathrm{e}^{-\mathrm{i}w x}\, dw.\)</span></p>
<p><l class="observ">Observación: </l>
En el caso discreto, o sea, Si <span class="math inline">\(X\)</span> es una variable discreta, la <strong>función característica</strong> <span class="math inline">\(\phi_X(w)\)</span> se escribe como función de la <strong>función de probabilidad</strong> <span class="math inline">\(P_X(x_k)\)</span> con <strong>Dominio</strong> <span class="math inline">\(D_X=\{x_k,\ k\}\)</span> como:
<span class="math inline">\(\phi(w)=\sum_{k} P_X(x_k)\mathrm{e}^{\mathrm{i}w x_k}.\)</span></p>
<p>En los casos en que los <span class="math inline">\(x_k\)</span> sean enteros, <span class="math inline">\(x_k=k\)</span>, que son la mayoría, la ecuación anterior es la <strong>tranformada de Fourier de la secuencia</strong> <span class="math inline">\(P_X(k)\)</span>. Dicha función es una <em>función periódica</em> en <span class="math inline">\(w\)</span> de periodo <span class="math inline">\(2\pi\)</span> ya que <span class="math inline">\(\mathrm{e}^{\mathrm{i}(w+2\pi)k}=\mathrm{e}^{\mathrm{i}wk}.\)</span></p>
<p>Por tanto, usando la fórmula de <strong>inversión</strong>, podemos escribir la <strong>función de probabilidad</strong> <span class="math inline">\(P_X(k)\)</span> como función de la función característica de <span class="math inline">\(X\)</span>, <span class="math inline">\(\phi(w)\)</span>:
<span class="math inline">\(P_X(k)=\frac{1}{2\pi}\int_{0}^{2\pi} \phi_X(w)\mathrm{e}^{-\mathrm{i}w k}\, dw.\)</span></p>
<div class="example">
<p><strong>Ejemplo: cálculo de la función característica para una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria de Bernoulli de parámetro <span class="math inline">\(p\)</span>. Recordemos que su función de probabilidad es:
<span class="math display">\[
P_X(0)=q=1-p,\ p_X(1)=p.
\]</span>
Su función característica será:
<span class="math display">\[
\phi_X (w)=E\left(\mathrm{e}^{\mathrm{i}wX}\right) =p\mathrm{e}^{\mathrm{i}w\cdot 1}+(1-p)\mathrm{e}^{\mathrm{i}w\cdot 0}=p\mathrm{e}^{\mathrm{i}w}+(1-p)=1+p\left(\mathrm{e}^{\mathrm{i}w} -1 \right).
\]</span>
Comprobemos la fórmula de la inversión:
<span class="math display">\[
\begin{array}{rl}
P_X(1) &amp; = \frac{1}{2\pi}\int_0^{2\pi} \left(1+p\left(\mathrm{e}^{\mathrm{i}w} -1 \right)\right) e^{-\mathrm{i}w\cdot 1}\, dw =\frac{1}{2\pi}\left(\int_0^{2\pi} (1-p)e^{-\mathrm{i}w}\, dw + \int_0^{2\pi} p\, dw\right) \\ &amp; = \frac{1}{2\pi}\left( (1-p) \left[\frac{\mathrm{e}^{-\mathrm{i}w}}{-\mathrm{i}}\right]_0^{2\pi} +2\pi p\right)=\frac{1}{2\pi}\left((1-p)\cdot 0 +2\pi p\right)=p, \\
P_X(0) &amp; = \frac{1}{2\pi}\int_0^{2\pi} \left(1+p\left(\mathrm{e}^{\mathrm{i}w} -1 \right)\right) e^{-\mathrm{i}w\cdot 0}\, dw =\frac{1}{2\pi}\left(\int_0^{2\pi} (1-p) \, dw + \int_0^{2\pi} p \mathrm{e}^{\mathrm{i}w}\, dw\right) \\ &amp; = \frac{1}{2\pi}\left( (1-p) \cdot 2\pi  +p \left[\frac{\mathrm{e}^{\mathrm{i}w}}{\mathrm{i}}\right]_0^{2\pi}\right)=\frac{1}{2\pi}\left((1-p)\cdot 2\pi + p\cdot 0\right)=1-p.
\end{array}
\]</span></p>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo de la función característica para una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>. Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x},\)</span> para <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(0\)</span>, en caso contrario.</p>
<p>Su función característica será:
<span class="math display">\[
\phi_X (w)=E\left(\mathrm{e}^{\mathrm{i}wX}\right)=\int_0^\infty \mathrm{e}^{\mathrm{i}w x}\lambda \mathrm{e}^{-\lambda x}\, dx = \lambda \int_0^\infty\mathrm{e}^{(\mathrm{i}w-\lambda)x}\, dx = \lambda\left[\frac{\mathrm{e}^{(\mathrm{i}w-\lambda)x}}{\mathrm{i}w-\lambda}\right]_{x=0}^{x=\infty} = \frac{\lambda}{\lambda -\mathrm{i} w}. 
\]</span>
La expresión anterior es válida para todo <span class="math inline">\(w\in\mathbb{R}\)</span> ya que su valor sería:
<span class="math inline">\(\phi_X (w)=\frac{\lambda}{\lambda -\mathrm{i} w}\cdot \frac{\lambda +\mathrm{i} w}{\lambda +\mathrm{i} w}=\frac{\lambda^2+\mathrm{i}\lambda w}{\lambda^2+w^2}=\frac{\lambda^2}{\lambda^2+w^2}+\mathrm{i}\frac{\lambda w}{\lambda^2+w^2}.\)</span>
En la última expresión hemos separado la parte real de la imaginaria.</p>
<p>Calculemos la función de densidad a partir de la función característica:
<span class="math display">\[
f_X(x)=\frac{1}{2\pi}\int_{-\infty}^\infty \frac{\lambda}{\lambda -\mathrm{i} w}\mathrm{e}^{-\mathrm{i}wx}\, dw = a\mathrm{e}^{-a x},
\]</span>
si <span class="math inline">\(x&gt;0\)</span> y <span class="math inline">\(0\)</span> en caso contrario. El cálculo de la integral anterior debe realizarse usando el <em>Teorema de los Residuos</em>, <a href="https://en.wikipedia.org/wiki/Residue_theorem">Residue theorem</a> y se sale de los objetivos de este curso.</p>
</div>
<div class="example">
<p><strong>Ejemplo: cálculo de la función característica para una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},\)</span> para <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>Su función característica será:</p>
<p><span class="math display">\[
\begin{array}{rl}
\phi_X (w) &amp; =\displaystyle E\left(\mathrm{e}^{\mathrm{i}w X}\right)=\int_{-\infty}^\infty \mathrm{e}^{\mathrm{i}w x}\frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{\mathrm{i}wx-\frac{(x-\mu)^2}{2\sigma^2}}\, dx \\[1ex]  &amp; =\displaystyle  \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}\left((x-(\sigma^2 \mathrm{i}w+\mu))^2-2\sigma^2 \mathrm{i}w \mu+\sigma^4 w^2\right)}\, dx = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{e}^{\frac{1}{2}(2 \mathrm{i}w \mu -\sigma^2 w^2)}\int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 \mathrm{i}w+\mu))^2}\, dx\\[1ex] &amp;  = \displaystyle\mathrm{e}^{\frac{1}{2}(2 \mathrm{i}w \mu -\sigma^2 w^2)} \left( \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^\infty \mathrm{e}^{-\frac{1}{2\sigma^2}(x-(\sigma^2 \mathrm{i}w+\mu))^2}\, dx\right) =  \mathrm{e}^{ \mathrm{i}w \mu -\frac{\sigma^2 w^2}{2}}.
\end{array}
\]</span>
La integral del último paréntesis se resuelve haciento el cambio de variable <span class="math inline">\(u=x-\sigma^2 \mathrm{i}w\)</span> y usando que la integral de la función de densidad de <span class="math inline">\(X\)</span> sobre todo <span class="math inline">\(\mathbb{R}\)</span> vale 1.</p>
<p>Calculemos la función de densidad a partir de la función característica:
<span class="math display">\[
\begin{array}{rl}
f_X(x) &amp; =\displaystyle\frac{1}{2\pi}\int_{-\infty}^\infty \mathrm{e}^{ \mathrm{i}w \mu -\frac{\sigma^2 w^2}{2}}\mathrm{e}^{-\mathrm{i} w x}\, dw = \frac{1}{2\pi}\int_{-\infty}^\infty \mathrm{e}^{\left(\frac{\mathrm{i}w\sigma}{\sqrt{2}}+\frac{\mu-x}{\sigma\sqrt{2}}\right)^2-\frac{(\mu-x)^2}{2\sigma^2}}\, dw =\frac{1}{2\pi}\mathrm{e}^{-\frac{(\mu-x)^2}{2\sigma^2}}\int_{-\infty}^\infty \mathrm{e}^{\left(\frac{\mathrm{i}w\sigma}{\sqrt{2}}+\frac{\mu-x}{\sigma\sqrt{2}}\right)^2}\, dw \\[1ex] &amp; =\displaystyle \frac{1}{2\pi}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\int_{-\infty}^\infty \mathrm{e}^{-\left(\frac{w\sigma}{\sqrt{2}}+\frac{\mu-x}{\mathrm{i}\sigma\sqrt{2}}\right)^2}\, dw \stackrel{\mbox{cambio de variable } u=\frac{w\sigma}{\sqrt{2}}+\frac{\mu-x}{\mathrm{i}\sigma\sqrt{2}}}{=} \frac{1}{2\pi}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}}\int_{-\infty}^\infty \frac{\sqrt{2}}{\sigma}\mathrm{e}^{-u^2}\, du \\[1ex] &amp; \displaystyle\stackrel{\int_{-\infty}^\infty \mathrm{e}^{-u^2}\, du =\sqrt{\pi}}{=} \frac{1}{\sqrt{2}\pi\sigma} \mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}} \sqrt{\pi} = \frac{1}{\sqrt{2\pi}\sigma}\mathrm{e}^{-\frac{(x-\mu)^2}{2\sigma^2}},
\end{array}
\]</span>
función que coincide con la densidad de la distribución <span class="math inline">\(N(\mu,\sigma)\)</span>.</p>
</div>
<p><strong>Relación entre la función característica y los momentos</strong></p>
<p>La relación entre la <strong>función característica</strong> y los <strong>momentos</strong> es la siguiente:</p>
<p><l class="prop"> <strong>Proposición.</strong> </l>
Sean <span class="math inline">\(X\)</span> una variable aleatoria con <strong>función característica</strong> <span class="math inline">\(\phi_X(w)\)</span>. Entonces, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> se puede obtener de la forma siguiente:
<span class="math display">\[
m_n =E\left(X^n\right)=\frac{1}{\mathrm{i}^n}\frac{d}{d w^n}\phi_X(w)|_{w=0} =\frac{1}{\mathrm{i}^n}\phi_X^{(n)}(0).
\]</span>
O sea, el momento de orden <span class="math inline">\(n\)</span> de <span class="math inline">\(X\)</span> es la derivada <span class="math inline">\(n\)</span>-ésima de la función característica evaluada en <span class="math inline">\(w=0\)</span> dividido por <span class="math inline">\(\mathrm{i}^n\)</span>.</p>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>La demostración se realiza de forma similar a la demostración de la proposición que relaciona la función generatriz de momentos y los momentos.</p>
<p>Se deja como ejercicio al lector.</p>
</div>
<div class="exercise">
<p><strong>Ejercicio</strong></p>
<p>Realizar los mismos ejemplos que los realizados para la función generatriz de momentos. O sea:</p>
<ul>
<li><p>Si <span class="math inline">\(X\)</span> es una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>, demostrar usando la función característica que para todo <span class="math inline">\(n\)</span>, <span class="math inline">\(m_n = E\left(X^n\right)=p\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> es una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span>, demostrar usando la función característica que para todo <span class="math inline">\(n\)</span>, <span class="math inline">\(m_n = E\left(X^n\right)=\frac{n!}{\lambda^n}\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> es una variable normal de parámetros <span class="math inline">\(\mu\)</span> y <span class="math inline">\(\sigma\)</span>, demostrar usando la función característica que <span class="math inline">\(E(X)=\mu\)</span>, <span class="math inline">\(E\left(X^2\right)=\mu^2+\sigma^2\)</span>, <span class="math inline">\(E\left(X^3\right)=3\sigma^2\mu\)</span> y <span class="math inline">\(E\left(X^4\right)=6\sigma^2\mu^2+\mu^4+3\sigma^4\)</span>.</p></li>
</ul>
</div>
</div>
</div>
<div id="fiabilidad" class="section level2">
<h2><span class="header-section-number">4.5</span> Fiabilidad</h2>
<p>Sea <span class="math inline">\(T\geq 0\)</span> una variable aleatoria que nos da, por ejemplo, el tiempo de vida de cierto componente o dispositivo.</p>
<p>Vamos a definir medidas para estudiar la fiabilidad de este tipo de variables aleatorias.</p>
<p><l class="definition"><strong>Definición:</strong></l>
Sea <span class="math inline">\(T\geq 0\)</span> una variable aleatoria. La <strong>fiabilidad</strong> de <span class="math inline">\(T\)</span> en el tiempo <span class="math inline">\(t\)</span> se define como la probabilidad que el sistema, componente o dispositivo funcione en el tiempo <span class="math inline">\(t\)</span>: <span class="math inline">\(R(t)=P(T&gt;t)\)</span>.</p>
<p><l class="observ"><strong>Observación:</strong></l>
Dada una variable <span class="math inline">\(T\geq 0\)</span>, la relación existente entre la <strong>fiabilidad</strong> <span class="math inline">\(R\)</span> y la <strong>función de distribución</strong> <span class="math inline">\(F_T\)</span> es la siguiente:
<span class="math display">\[
R(t)=P(T&gt;t)=1-P(T\leq t)=1-F_T (t)
\]</span></p>
<div id="tiempo-medio-de-vida" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Tiempo medio de vida</h3>
<p><l class="observ">Observación:</l>
Dada una variable <span class="math inline">\(T\geq 0\)</span> continua, el <strong>tiempo medio de vida</strong> de la variable <span class="math inline">\(T\)</span> sería <span class="math inline">\(E(T)\)</span>. Entonces, este <strong>tiempo medio de vida</strong> se puede calcular como:
<span class="math inline">\(E(T)=\int_0^\infty R(t)\, dt.\)</span></p>
<p>Veámoslo. Para ello basta ver que <span class="math inline">\(E(T)=\int_0^\infty (1-F_T(t))\, dt\)</span>, donde <span class="math inline">\(F_T(t)\)</span> es la función de distribución de la variable <span class="math inline">\(T\)</span>:
<span class="math display">\[
\begin{array}{rl}
E(T) &amp; =\displaystyle\int_{t=0}^{t=\infty} 1-F_T(t)\, dt=\int_{t=0}^{t=\infty}\int_{u=t}^{u=\infty} f_T(u)\,du\,dt \\[1ex] &amp; =\displaystyle\int_{u=0}^{u=\infty} f_T(u)\int_{t=0}^{t=u} \, dt\, du =\int_{u=0}^{u=\infty} f_T(u)\cdot u\, du = E(T),
\end{array}
\]</span>
donde <span class="math inline">\(f_T(u)\)</span> seria la función de densidad de la variable <span class="math inline">\(T\)</span> en el valor <span class="math inline">\(u\)</span>.</p>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>Sea <span class="math inline">\(T\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
La fiabilidad de <span class="math inline">\(T\)</span> sería: <span class="math inline">\(R(t)=P(T&gt;t)=1-F_T(t)=\mathrm{e}^{-\lambda t}\)</span>:
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
</div>
</div>
</div>
</div>
<div id="generación-de-muestras-de-variables-aleatorias-por-ordenador" class="section level2">
<h2><span class="header-section-number">4.6</span> Generación de muestras de variables aleatorias por ordenador</h2>
<p>La simulación por <strong>computadora</strong> de cualquier fenómeno aleatorio implica la <strong>generación de variables aleatorias</strong> con distribuciones prefijadas de antemano.</p>
<p>Por ejemplo, la simulación de un sistema de colas implica generar el tiempo entre las llegadas de los clientes, así como los tiempos de servicio de cada cliente.</p>
<p>Fijémonos que fijar la variable aleatoria <span class="math inline">\(X\)</span> es equivalente a fijar la <strong>función de distribución <span class="math inline">\(F_X(x)\)</span></strong> o la <strong>función de densidad <span class="math inline">\(f_X(x)\)</span></strong> en el caso continuo o la <strong>función de probabilidad <span class="math inline">\(P_X(x)\)</span></strong> en el caso discreto.</p>
<p>Todos los métodos que vamos a describir presuponen que podemos generar <strong>números aleatorios</strong> que se distribuyen <strong>uniformemente</strong> entre 0 y 1. En <code>R</code> se puede hacer usando la función <code>runif(n)</code>, donde <code>n</code> es la cantidad de números aleatorios entre 0 y 1 a generar.</p>
<div id="método-de-transformación" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Método de transformación</h3>
<p>El <strong>método de transformación</strong> se basa en el resultado siguiente:</p>
<p><l class="prop"><strong>Proposición.</strong> </l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria con función de distribución <span class="math inline">\(F_X(x)\)</span>. Supongamos que <span class="math inline">\(F_X(x)\)</span> es estrictamente creciente o que existe <span class="math inline">\(F_X^{-1}(y)\)</span>, para todo <span class="math inline">\(y\in [0,1]\)</span>. Sea <span class="math inline">\(Y\)</span> la variable aleatoria definida como: <span class="math inline">\(Y=F_X(X)\)</span>. Entonces la distribución de <span class="math inline">\(Y\)</span> es uniforme en el intervalo <span class="math inline">\([0,1]\)</span>.</p>
<div class="dem">
<p><strong>Demostración:</strong></p>
<p>Claramente, por propia definición de <span class="math inline">\(Y\)</span>, tenemos que el dominio de <span class="math inline">\(Y\)</span> es <span class="math inline">\([0,1]\)</span> ya que el conjunto recorrido de la función de distribución de cualquier variable es el intervalo <span class="math inline">\([0,1]\)</span>.</p>
<p>Para ver que la distribución de <span class="math inline">\(Y\)</span> es <span class="math inline">\(U[0,1]\)</span> basta comprobar que <span class="math inline">\(F_Y(y)=y\)</span>, para todo <span class="math inline">\(y\in [0,1]\)</span>:
<span class="math display">\[
\begin{array}{rl}
F_Y(y) &amp; =P(Y\leq y)=P(F_X(X)\leq y)\stackrel{\mbox{usando que $F_X$ es estrictamente creciente}}{=} P(X\leq F_X^{-1}(y)) \\ &amp; =F_X(F_X^{-1}(y))=y.
\end{array}
\]</span></p>
</div>
<p>Usando la proposición anterior, dada una variable <span class="math inline">\(X\)</span>, como la distribución de la variable aleatoria <span class="math inline">\(Y=F_X(X)\)</span> es <span class="math inline">\(U[0,1]\)</span>, si hacemos <span class="math inline">\(X=F_X^{-1}(Y)\)</span>, tendremos que si sabemos generar una muestra de <span class="math inline">\(Y\)</span>, aplicándole a la muestra la función <span class="math inline">\(F_X^{-1}\)</span> tendremos generada una muestra de <span class="math inline">\(X\)</span>.</p>
<div class="example">
<p><strong>Ejemplo: generar una muestra de una variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Recordemos que si <span class="math inline">\(X\)</span> es exponencial de parámetro <span class="math inline">\(\lambda\)</span>, su función de distribución es: <span class="math inline">\(F_X(x)=1-\mathrm{e}^{-\lambda x}\)</span>.</p>
<p>Hallemos a continuación <span class="math inline">\(F_X^{-1}\)</span>:
<span class="math display">\[
y=1-\mathrm{e}^{-\lambda x},\ \Leftrightarrow 1-y=\mathrm{e}^{-\lambda x},\ \Leftrightarrow \ln(1-y)=-\lambda x,\ \Leftrightarrow x=-\frac{1}{\lambda}\ln(1-y).
\]</span>
Por tanto, <span class="math inline">\(F_X^{-1}(y)=-\frac{1}{\lambda}\ln(1-y)\)</span>.</p>
<p>Generemos una muestra con <code>R</code> de 25 valores de una variable exponencial de parámetro <span class="math inline">\(\lambda=2\)</span> usando el método anterior:</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb358-1" title="1">n=<span class="dv">25</span></a>
<a class="sourceLine" id="cb358-2" title="2">lambda=<span class="dv">2</span></a>
<a class="sourceLine" id="cb358-3" title="3">muestra.y =<span class="st"> </span><span class="kw">runif</span>(n)</a>
<a class="sourceLine" id="cb358-4" title="4">muestra.x =<span class="st"> </span><span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>lambda)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>muestra.y)</a>
<a class="sourceLine" id="cb358-5" title="5">muestra.x</a></code></pre></div>
<pre><code>##  [1] 0.35877893 0.25280668 0.11355513 0.71885498 1.19013394 0.49444154
##  [7] 0.23443983 0.07049329 1.48215515 0.09206527 0.14783308 2.35604428
## [13] 0.41728297 0.43470135 0.02910931 0.36337987 0.08532398 0.27981457
## [19] 0.40904272 0.02482599 0.25061234 1.67562739 0.07212948 0.27227039
## [25] 0.23932819</code></pre>
<p>Vamos a testear si nuestro método funciona.</p>
<p>Para ello generaremos una muestra de 500 valores usando el método de transformación y dibujaremos su <strong>histograma de frecuencias relativas</strong>.</p>
<p>Seguidamente dibujaremos la <strong>función de densidad de la variable exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong> y compararemos los resultados:</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb360-1" title="1">n=<span class="dv">500</span></a>
<a class="sourceLine" id="cb360-2" title="2">lambda=<span class="dv">2</span></a>
<a class="sourceLine" id="cb360-3" title="3">muestra.y =<span class="st"> </span><span class="kw">runif</span>(n)</a>
<a class="sourceLine" id="cb360-4" title="4">muestra.x =<span class="st"> </span><span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>lambda)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>muestra.y)</a>
<a class="sourceLine" id="cb360-5" title="5"><span class="kw">hist</span>(muestra.x,<span class="dt">freq=</span><span class="ot">FALSE</span>,<span class="dt">main=</span><span class="st">&quot;Histograma de la muestra&quot;</span>)</a>
<a class="sourceLine" id="cb360-6" title="6">x2=<span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">0</span>,<span class="dt">to=</span><span class="fl">2.5</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb360-7" title="7"><span class="kw">lines</span>(x2,<span class="kw">dexp</span>(x2,lambda),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-110-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="método-de-rechazo" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Método de rechazo</h3>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria continua tal que su función de densidad verifica:</p>
<ul>
<li>Existen valores <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> tal que <span class="math inline">\(f_X(x)= 0\)</span> si <span class="math inline">\(x\not\in [a,b]\)</span>.</li>
<li>Existen valores <span class="math inline">\(c\)</span> y <span class="math inline">\(d\)</span> tal que <span class="math inline">\(f_X(x)\in [c,d]\)</span>, si <span class="math inline">\(x\in [a,b]\)</span>.</li>
</ul>
<p>En resumen, los puntos <span class="math inline">\((x,f(x))\)</span> pertenecen al rectángulo <span class="math inline">\([a,b]\times [c,d]\)</span> y en caso contrario <span class="math inline">\(f_X(x)=0\)</span>.</p>
<p>En el gráfico siguiente, <span class="math inline">\(a=0\)</span>, <span class="math inline">\(b=2\)</span>, <span class="math inline">\(c=0\)</span> y <span class="math inline">\(d=1\)</span>.</p>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-111-1.png" width="672" /></p>
</div>
<p>Para generar una <strong>muestra aleatoria</strong> de la variable <span class="math inline">\(X\)</span>, hacemos lo siguiente:</p>
<ol style="list-style-type: decimal">
<li><p>generamos un valor aleatorio <span class="math inline">\(x\)</span> entre <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span>.</p></li>
<li><p>generamos un valor aleatorio <span class="math inline">\(y\)</span> entre <span class="math inline">\(c\)</span> y <span class="math inline">\(d\)</span>.</p></li>
<li><p>si <span class="math inline">\(y\leq f_X(x)\)</span>, aceptamos <span class="math inline">\(x\)</span> como valor de la muestra. En caso contrario, volvemos a empezar en 1.</p></li>
</ol>
<div class="example">
<p><strong>Ejemplo</strong></p>
<p>El gráfico de la figura anterior corresponde a la función de densidad siguiente:
<span class="math display">\[
f_X(x)=\begin{cases}
x, &amp; \mbox{ si }0\leq x\leq 1,\\
2-x, &amp; \mbox{ si }1\leq x\leq 2,\\
0, &amp; \mbox{en caso contrario.}
\end{cases}
\]</span></p>
<p>Vamos a generar una muestra de <span class="math inline">\(25\)</span> valores usando el <strong>método del rechazo</strong>:</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" title="1">a=<span class="dv">0</span>; b=<span class="dv">2</span>; c=<span class="dv">0</span>; d=<span class="dv">1</span>; n=<span class="dv">25</span>; i=<span class="dv">1</span>;</a>
<a class="sourceLine" id="cb361-2" title="2">f =<span class="st"> </span><span class="cf">function</span>(x){<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&lt;=</span><span class="dv">1</span>,x,<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">1</span><span class="op">&amp;</span>x<span class="op">&lt;=</span><span class="dv">2</span>,<span class="dv">2</span><span class="op">-</span>x,<span class="dv">0</span>))}</a>
<a class="sourceLine" id="cb361-3" title="3">muestra=<span class="kw">c</span>()</a>
<a class="sourceLine" id="cb361-4" title="4"><span class="cf">while</span>(i <span class="op">&lt;=</span>n){</a>
<a class="sourceLine" id="cb361-5" title="5">  x=<span class="kw">runif</span>(<span class="dv">1</span>,a,b)</a>
<a class="sourceLine" id="cb361-6" title="6">  y=<span class="kw">runif</span>(<span class="dv">1</span>,c,d)</a>
<a class="sourceLine" id="cb361-7" title="7">  <span class="cf">if</span>(y <span class="op">&lt;=</span><span class="st"> </span><span class="kw">f</span>(x)){muestra=<span class="kw">c</span>(muestra,x); i=i<span class="op">+</span><span class="dv">1</span>}</a>
<a class="sourceLine" id="cb361-8" title="8">}</a>
<a class="sourceLine" id="cb361-9" title="9">muestra</a></code></pre></div>
<pre><code>##  [1] 0.8934245 1.0241151 0.7485943 1.1318743 1.1174486 0.8956145 1.6270601
##  [8] 0.6737312 0.9584882 0.5762514 0.7390811 0.4276671 1.5210227 0.8506008
## [15] 0.4126257 0.9897587 1.2166050 1.6098125 0.8727734 0.8756381 1.4853939
## [22] 1.1539542 1.2353779 0.5157756 1.2873756</code></pre>
<p>Como hicimos con el ejemplo del <strong>método de transformación</strong>, vamos a generar una muestra de 500 valores de la variable <span class="math inline">\(X\)</span>, vamos a dibujar el <strong>histograma de frecuencias relativas</strong> junto con la función de densidad para ver si ésta se aproxima a dicho histograma:</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" title="1">a=<span class="dv">0</span>; b=<span class="dv">2</span>; c=<span class="dv">0</span>; d=<span class="dv">1</span>; n=<span class="dv">500</span>; i=<span class="dv">1</span>;</a>
<a class="sourceLine" id="cb363-2" title="2">f =<span class="st"> </span><span class="cf">function</span>(x){<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>x<span class="op">&lt;=</span><span class="dv">1</span>,x,<span class="kw">ifelse</span>(x<span class="op">&gt;=</span><span class="dv">1</span><span class="op">&amp;</span>x<span class="op">&lt;=</span><span class="dv">2</span>,<span class="dv">2</span><span class="op">-</span>x,<span class="dv">0</span>))}</a>
<a class="sourceLine" id="cb363-3" title="3">muestra=<span class="kw">c</span>()</a>
<a class="sourceLine" id="cb363-4" title="4"><span class="cf">while</span>(i <span class="op">&lt;=</span>n){</a>
<a class="sourceLine" id="cb363-5" title="5">  x=<span class="kw">runif</span>(<span class="dv">1</span>,a,b)</a>
<a class="sourceLine" id="cb363-6" title="6">  y=<span class="kw">runif</span>(<span class="dv">1</span>,c,d)</a>
<a class="sourceLine" id="cb363-7" title="7">  <span class="cf">if</span>(y <span class="op">&lt;=</span><span class="st"> </span><span class="kw">f</span>(x)){muestra=<span class="kw">c</span>(muestra,x); i=i<span class="op">+</span><span class="dv">1</span>}</a>
<a class="sourceLine" id="cb363-8" title="8">}</a>
<a class="sourceLine" id="cb363-9" title="9"><span class="kw">hist</span>(muestra,<span class="dt">freq=</span><span class="ot">FALSE</span>,<span class="dt">main=</span><span class="st">&quot;Histograma de la muestra&quot;</span>)</a>
<a class="sourceLine" id="cb363-10" title="10">x2=<span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">0</span>,<span class="dt">to=</span><span class="dv">2</span>,<span class="dt">by=</span><span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb363-11" title="11"><span class="kw">lines</span>(x2,<span class="kw">f</span>(x2),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-114-1.png" width="672" /></p>
</div>
</div>
</div>
</div>
<div id="entropía" class="section level2">
<h2><span class="header-section-number">4.7</span> Entropía</h2>
<p>La <strong>entropía</strong> es una medida de la <strong>incertidumbre</strong> en un experimento aleatorio.</p>
<p>Veremos cómo la <strong>entropía</strong> cuantifica la <strong>incertidumbre</strong> por la cantidad de <strong>información</strong> requerida para especificar el resultado de un experimento aleatorio.</p>
<div id="entropía-de-una-variable-aleatoria" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Entropía de una variable aleatoria</h3>
<p>Supongamos que tenemos una variable aleatoria <span class="math inline">\(X\)</span> discreta con valores enteros: <span class="math inline">\(D_X=\{1,2,\ldots,N\}\)</span>.</p>
<p>Sea <span class="math inline">\(k\in D_X\)</span> un valor de la variable. Estamos interesados en cuantificar la <strong>incertidumbre</strong> del suceso <span class="math inline">\(A_k =\{X=k\}\)</span>.</p>
<p>O sea, cuánta <strong>menos incertidumbre</strong> tenga <span class="math inline">\(A_k\)</span>, más <strong>alta será su probabilidad</strong>, y cuánta <strong>más incertidumbre</strong>, <strong>menos probabilidad</strong> de aparecer <span class="math inline">\(A_k\)</span>.</p>
<p>Una medida que cumple las condiciones anteriores es la siguiente: <span class="math inline">\(I(A_k)=I(\{X=k\})=\ln\left(\frac{1}{P(X=k)}\right)=-\ln\left(P(X=k)\right).\)</span></p>
<p>Por ejemplo, si <span class="math inline">\(P(A_k)=1\)</span>, o sea, <span class="math inline">\(A_k\)</span> aparece “<strong>seguro</strong>”, entonces tiene incertidumbre <strong>nula</strong>, <span class="math inline">\(I(A_k)=0\)</span>, y si <span class="math inline">\(P(A_k)=0\)</span>, o sea, <span class="math inline">\(A_k\)</span> no aparece “<strong>nunca</strong>”, tiene incertidumbre <strong>máxima</strong>, <span class="math inline">\(I(A_k)=\infty\)</span>.</p>
<p>La motivación anterior hace que definamos la <strong>entropía</strong> de una variable aleatoria de la forma siguiente:</p>
<p><l class="definition"><strong>Definición:</strong></l>
Sea <span class="math inline">\(X\)</span> una variable aleatoria con función de densidad <span class="math inline">\(f_X(x)\)</span> en el caso continuo o función de probabilidad <span class="math inline">\(P_X(x)\)</span> en el caso discreto. Definimos <strong>entropía de X</strong> como:
<span class="math inline">\(H_X = \displaystyle E\left(-\ln(f_X)\right)=\int_{-\infty}^\infty -\ln(f_X(x)) f_X(x)\, dx,\)</span> en el caso continuo y,
<span class="math inline">\(H_X = \displaystyle E\left(-\ln(P_X)\right)=\sum_{x_k\in D_X} -\ln(P_X(x_k)) P_X(x_k),\)</span> en el caso discreto.</p>
<div class="example">
<p><strong>Ejemplo: entropía de una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable de Bernoulli de parámetro <span class="math inline">\(p\)</span>.</p>
<p>Recordemos que su función de probabilidad <span class="math inline">\(P_X\)</span> es: <span class="math inline">\(P_X(0)=1-p=q,\)</span> <span class="math inline">\(P_X(1)=p\)</span>.</p>
<p>La entropía de <span class="math inline">\(X\)</span> será:
<span class="math display">\[
H_X = E\left(-\ln(P_X)\right) = -(1-p)\cdot \ln(1-p)-p\cdot \ln p.
\]</span>
El gráfico de la entropía se puede observar en el gráfico siguiente donde <span class="math inline">\(X\)</span> tiene entropía máxima cuando <span class="math inline">\(p=\frac{1}{2}\)</span> que sería cuando <span class="math inline">\(X\)</span> tiene incertidumbre máxima al tratar de adivinar el resultado de <span class="math inline">\(X\)</span> y <span class="math inline">\(X\)</span> tiene entropía mínima cuando <span class="math inline">\(p=0\)</span> o <span class="math inline">\(p=1\)</span> ya que en estos casos el resultado de <span class="math inline">\(X\)</span> sería siempre <span class="math inline">\(0\)</span> o <span class="math inline">\(1\)</span>, respectivamente.</p>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-115-1.png" width="672" /></p>
</div>
</div>
<div class="example">
<p><strong>Ejemplo: Entropía de una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span></strong></p>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria exponencial de parámetro <span class="math inline">\(\lambda\)</span>.</p>
<p>Recordemos que su función de densidad es: <span class="math inline">\(f_X(x)=\lambda \mathrm{e}^{-\lambda x}\)</span>, si <span class="math inline">\(x\geq 0\)</span> y <span class="math inline">\(f_X(x)=0\)</span>, en caso contrario.</p>
<p>Su entropía será:
<span class="math display">\[
\begin{array}{rl}
H_X &amp; = \displaystyle E\left(-\ln(f_X)\right)=-\int_0^\infty \ln\left(\lambda\mathrm{e}^{-\lambda x}\right)\lambda\mathrm{e}^{-\lambda x}\, dx = -\lambda \int_0^\infty (\ln(\lambda) -\lambda x)\mathrm{e}^{-\lambda x}\, dx \\[1ex] &amp; =\displaystyle -\ln (\lambda)\int_0^\infty \lambda\mathrm{e}^{-\lambda x}\, dx+\lambda \int_0^\infty \lambda x \mathrm{e}^{-\lambda x}\, dx =-\ln(\lambda)\int_0^\infty f_X(x)\, dx +\lambda E(X)\\[1ex] &amp; =\displaystyle -\ln(\lambda)+\lambda \frac{1}{\lambda} =1-\ln(\lambda).
\end{array}
\]</span>
El gráfico de la entropía se puede observar en el gráfico siguiente donde <span class="math inline">\(X\)</span> tiene entropía máxima cuando <span class="math inline">\(\lambda=0\)</span> que sería cuando <span class="math inline">\(X\)</span> tiene incertidumbre máxima al tratar de adivinar el resultado de <span class="math inline">\(X\)</span> al tener media <span class="math inline">\(E(X)=\frac{1}{\lambda}=\infty\)</span> y <span class="math inline">\(X\)</span> tiene entropía mínima cuando <span class="math inline">\(\lambda\)</span> tiende a <span class="math inline">\(\infty\)</span> ya que su media <span class="math inline">\(E(X)=\frac{1}{\lambda}\)</span> tendería a 0.</p>
<div class="center">
<p><img src="curso-probabilidad-udemy_files/figure-html/unnamed-chunk-116-1.png" width="672" /></p>
</div>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distribuciones-notables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="vectores-aleatorios-bidimensionales.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/joanby/probabilidad/edit/master/4.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["curso-probabilidad-udemy.pdf", "curso-probabilidad-udemy.epub"],
"toc": {
"collapse": "subsection"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
